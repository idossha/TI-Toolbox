{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TI-Toolbox API Reference","text":"<p>This is the auto-generated Python API for the Temporal Interference Toolbox (<code>tit</code>).</p>"},{"location":"#modules","title":"Modules","text":"<ul> <li>Core</li> <li>Simulation</li> <li>Analyzer</li> <li>Optimization</li> <li>Statistics</li> </ul>"},{"location":"#build-locally","title":"Build locally","text":"<p>From the repository root:</p> <pre><code>python -m pip install -r docs/api_mkdocs/requirements.txt\nmkdocs build -f docs/api_mkdocs/mkdocs.yml -d docs/api --clean\n</code></pre>"},{"location":"reference/analyzer/","title":"Analyzer (<code>tit.analyzer</code>)","text":""},{"location":"reference/analyzer/#mesh-analyzer-titanalyzermesh_analyzer","title":"Mesh analyzer (<code>tit.analyzer.mesh_analyzer</code>)","text":"<p>MeshAnalyzer: A tool for analyzing mesh-based neuroimaging data</p> <p>This module provides functionality for analyzing field data in specific regions of interest, including spherical ROIs and cortical regions defined by an atlas. It works with SimNIBS mesh files and uses the cortical middle-layer surface mesh (via msh2cortex) for cortical/surface analyses.</p> <p>Inputs:     - SimNIBS mesh files (.msh) containing field data     - Subject directory containing m2m files for atlas mapping     - ROI specifications (coordinates, regions)</p> <p>Outputs:     - Statistical measures (mean, min, max)     - ROI masks     - Surface meshes for visualization     - Visualization files (optional)</p> <p>Example Usage:     ```python     # Initialize analyzer     analyzer = MeshAnalyzer(         field_mesh_path=\"/path/to/field.msh\",         field_name=\"normE\",         subject_dir=\"/path/to/m2m_subject\",         output_dir=\"/path/to/output\"     )</p> <pre><code># Analyze a spherical ROI\nsphere_results = analyzer.analyze_sphere(\n    center_coordinates=[0, 0, 0],\n    radius=10\n)\n\n# Analyze a cortical region\ncortex_results = analyzer.analyze_cortex(\n    atlas_type=\"DK40\",\n    target_region=\"superiorfrontal\",\n    visualize=True\n)\n\n# Analyze whole head\nwhole_head_results = analyzer.analyze_whole_head(\n    atlas_type=\"DK40\",\n    visualize=True\n)\n```\n</code></pre> <p>Dependencies:     - numpy     - simnibs     - subprocess (for msh2cortex operations)</p>"},{"location":"reference/analyzer/#tit.analyzer.mesh_analyzer.MeshAnalyzer","title":"MeshAnalyzer","text":"<pre><code>MeshAnalyzer(field_mesh_path: str, field_name: str, subject_dir: str, output_dir: str, logger=None)\n</code></pre> <p>A class for analyzing mesh-based data from 3D models.</p> <p>This class provides methods for analyzing field data in specific regions of interest,  including spherical ROIs and cortical regions defined by an atlas. It works with  SimNIBS mesh files and uses cortical middle-layer surface meshes for analysis.</p> <p>Attributes:      field_mesh_path (str): Path to the mesh file containing field data      field_name (str): Name of the field to analyze in the mesh      subject_dir (str): Directory containing subject data (m2m folder)      output_dir (str): Directory where analysis results will be saved      visualizer (MeshVisualizer): Instance of visualizer for generating plots      _temp_dir (tempfile.TemporaryDirectory): Temporary directory for surface mesh      _surface_mesh_path (str): Path to the generated surface mesh</p> <p>Initialize the MeshAnalyzer class.</p> <p>Args:     field_mesh_path (str): Path to the mesh file containing the field data     field_name (str): Name of the field to analyze     subject_dir (str): Directory containing subject data (m2m folder)     output_dir (str): Directory where analysis results will be saved     logger: Optional logger instance to use. If None, creates its own.</p> Source code in <code>tit/analyzer/mesh_analyzer.py</code> <pre><code>def __init__(self, field_mesh_path: str, field_name: str, subject_dir: str, output_dir: str, logger=None):\n    \"\"\"\n    Initialize the MeshAnalyzer class.\n\n    Args:\n        field_mesh_path (str): Path to the mesh file containing the field data\n        field_name (str): Name of the field to analyze\n        subject_dir (str): Directory containing subject data (m2m folder)\n        output_dir (str): Directory where analysis results will be saved\n        logger: Optional logger instance to use. If None, creates its own.\n    \"\"\"\n    self.field_mesh_path = field_mesh_path\n    self.field_name = field_name\n    self.subject_dir = subject_dir\n    self.output_dir = output_dir\n\n    # Validate early (before logger/file handlers) so missing input paths fail\n    # with the intended error rather than a secondary logging error.\n    if not os.path.exists(field_mesh_path):\n        raise FileNotFoundError(f\"Field mesh file not found: {field_mesh_path}\")\n\n    # Set up logger - use provided logger or create a new one\n    if logger is not None:\n        # Create a child logger to distinguish mesh analyzer logs\n        self.logger = logger.getChild('mesh_analyzer')\n    else:\n        # Create our own logger if none provided\n        time_stamp = time.strftime('%Y%m%d_%H%M%S')\n\n        # Extract subject ID from subject_dir (e.g., m2m_subject -&gt; subject)\n        subject_id = os.path.basename(self.subject_dir).split('_')[1] if '_' in os.path.basename(self.subject_dir) else os.path.basename(self.subject_dir)\n\n        # Create derivatives/ti-toolbox/logs/sub-* directory structure\n        pm = get_path_manager()\n        log_dir = pm.path(\"ti_logs\", subject_id=subject_id)\n        os.makedirs(log_dir, exist_ok=True)\n\n        # Create log file in the new directory\n        log_file = os.path.join(log_dir, f'mesh_analyzer_{time_stamp}.log')\n        self.logger = logging_util.get_logger('mesh_analyzer', log_file, overwrite=True)\n\n    # Initialize visualizer with logger\n    self.visualizer = MeshVisualizer(output_dir, self.logger)\n\n    # Initialize temporary directory and surface mesh path\n    self._temp_dir = None\n    self._surface_mesh_path = None\n\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        self.logger.debug(f\"Creating output directory: {output_dir}\")\n        os.makedirs(output_dir)\n\n    self.logger.debug(f\"Mesh analyzer initialized successfully\")\n    self.logger.debug(f\"Field mesh path: {field_mesh_path}\")\n    self.logger.debug(f\"Field name: {field_name}\")\n    self.logger.debug(f\"Subject directory: {subject_dir}\")\n    self.logger.debug(f\"Output directory: {output_dir}\")\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.mesh_analyzer.MeshAnalyzer.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> <p>Cleanup temporary directory when the analyzer is destroyed.</p> Source code in <code>tit/analyzer/mesh_analyzer.py</code> <pre><code>def __del__(self):\n    \"\"\"Cleanup temporary directory when the analyzer is destroyed.\"\"\"\n    if self._temp_dir is not None:\n        try:\n            self.logger.info(\"Cleaning up temporary directory...\")\n            self._temp_dir.cleanup()\n        except (OSError, PermissionError):\n            # Best-effort cleanup - directory may already be deleted or inaccessible\n            pass\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.mesh_analyzer.MeshAnalyzer.analyze_cortex","title":"analyze_cortex","text":"<pre><code>analyze_cortex(atlas_type, target_region, visualize=False)\n</code></pre> <p>Analyze a cortical region defined by an atlas.</p> <p>Args:     atlas_type: Type of atlas to use (e.g., 'DK40', 'HCP_MMP1')     target_region: Name of the region to analyze     visualize: Whether to generate visualization files</p> <p>Returns:     Dictionary containing analysis results</p> Source code in <code>tit/analyzer/mesh_analyzer.py</code> <pre><code>def analyze_cortex(self, atlas_type, target_region, visualize=False):\n    \"\"\"\n    Analyze a cortical region defined by an atlas.\n\n    Args:\n        atlas_type: Type of atlas to use (e.g., 'DK40', 'HCP_MMP1')\n        target_region: Name of the region to analyze\n        visualize: Whether to generate visualization files\n\n    Returns:\n        Dictionary containing analysis results\n    \"\"\"\n    self.logger.info(f\"Starting cortical analysis for region '{target_region}' using {atlas_type} atlas\")\n\n    try:\n        surface_mesh_path = self._generate_surface_mesh()\n        self.logger.info(\"Loading surface mesh...\")\n        gm_surf = simnibs.read_msh(surface_mesh_path)\n\n        # Load the atlas\n        self.logger.info(f\"Loading atlas {atlas_type}...\")\n        atlas = simnibs.subject_atlas(atlas_type, self.subject_dir)\n\n        # Verify region exists in atlas\n        if target_region not in atlas:\n            available_regions = sorted(atlas.keys())\n            raise ValueError(f\"Region '{target_region}' not found in {atlas_type} atlas. Available regions: {available_regions}\")\n\n        # Get ROI mask for this region\n        roi_mask = atlas[target_region]\n\n        # Check if we have any nodes in the ROI\n        roi_nodes_count = np.sum(roi_mask)\n        if roi_nodes_count == 0:\n            self.logger.warning(f\"No nodes found in the specified region '{target_region}'\")\n            results = {\n                'mean_value': None,\n                'max_value': None,\n                'min_value': None,\n                'focality': None,\n                'normal_mean_value': None,\n                'normal_max_value': None,\n                'normal_min_value': None,\n                'normal_focality': None\n            }\n\n            # Save results to CSV even if empty\n            self.visualizer.save_results_to_csv(results, 'cortical', target_region, 'node')\n\n            return results\n\n        # Get the field values within the ROI\n        field_values = gm_surf.field[self.field_name].value\n        field_values_in_roi = field_values[roi_mask]\n\n        # Filter for positive values in ROI (matching voxel analyzer behavior)\n        positive_mask = field_values_in_roi &gt; 0\n        field_values_positive = field_values_in_roi[positive_mask]\n\n        # Check if we have any positive values in the ROI\n        positive_count = len(field_values_positive)\n        if positive_count == 0:\n            self.logger.warning(f\"Warning: Region {target_region} has no positive values\")\n            results = {\n                'mean_value': None,\n                'max_value': None,\n                'min_value': None,\n                'focality': None,\n                'normal_mean_value': None,\n                'normal_max_value': None,\n                'normal_min_value': None,\n                'normal_focality': None\n            }\n\n            # Save results to CSV even if empty\n            self.visualizer.save_results_to_csv(results, 'cortical', target_region, 'node')\n\n            return results\n\n        min_value = np.min(field_values_positive)\n        node_areas = gm_surf.nodes_areas()\n        if hasattr(node_areas, 'value'):\n            node_areas = node_areas.value\n        node_areas = np.asarray(node_areas)\n        positive_node_areas = node_areas[roi_mask][positive_mask]\n        whole_brain_positive_mask = field_values &gt; 0\n        whole_brain_node_areas = node_areas[whole_brain_positive_mask]\n\n        metrics = calculate_roi_metrics(\n            field_values_positive,\n            positive_node_areas,\n            ti_field_gm=field_values[whole_brain_positive_mask],\n            gm_volumes=whole_brain_node_areas,\n        )\n        mean_value = metrics[\"TImean_ROI\"]\n        max_value = metrics[\"TImax_ROI\"]\n        focality = metrics.get(\"Focality\")\n        whole_brain_average = metrics.get(\"TImean_GM\")\n\n        # Log the whole brain average for debugging\n        if whole_brain_average is not None:\n            self.logger.info(f\"Whole brain average (denominator for focality): {whole_brain_average:.6f}\")\n\n        # Prepare the results with TI_max values\n        results = {\n            'mean_value': mean_value,\n            'max_value': max_value,\n            'min_value': min_value,\n            'focality': focality\n        }\n\n        # Extract TI_normal values from normal mesh\n        self.logger.info(\"Extracting TI_normal values from normal mesh...\")\n        normal_field_values_positive, normal_statistics = self._extract_normal_field_values(roi_mask)\n\n        # Add TI_normal statistics to results if available\n        if normal_statistics is not None:\n            results.update(normal_statistics)\n            self.logger.debug(\"TI_normal values successfully added to results\")\n        else:\n            self.logger.warning(\"TI_normal values not available - results will only contain TI_max values\")\n\n        # Generate visualization if requested\n        if visualize:\n            self.logger.info(\"Generating visualizations...\")\n            # Generate focality histogram\n            try:\n                self.logger.info(f\"Generating focality histogram for region: {target_region}\")\n                # Get node areas for volume weighting\n                node_areas = gm_surf.nodes_areas()\n                if hasattr(node_areas, 'value'):\n                    node_areas = node_areas.value\n                node_areas = np.asarray(node_areas)\n                positive_node_areas = node_areas[roi_mask][positive_mask]\n\n                self.visualizer.generate_focality_histogram(\n                    whole_head_field_data=field_values,\n                    roi_field_data=field_values_positive,\n                    whole_head_element_sizes=node_areas,\n                    roi_element_sizes=positive_node_areas,\n                    region_name=target_region,\n                    roi_field_value=mean_value,\n                    data_type='node'\n                )\n            except Exception as e:\n                self.logger.warning(f\"Could not generate focality histogram for {target_region}: {str(e)}\")\n\n            # Create region mesh visualization\n            # This creates a mesh file with the ROI highlighted\n            region_mesh_file = self.visualizer.visualize_cortex_roi(\n                gm_surf=gm_surf,\n                roi_mask=roi_mask,\n                target_region=target_region,\n                field_values=field_values,\n                max_value=max_value,\n                output_dir=self.output_dir,\n                surface_mesh_path=self._surface_mesh_path\n            )\n\n        # Calculate and save extra focality information for entire grey matter\n        self.logger.info(\"Calculating focality metrics for entire grey matter...\")\n        focality_info = self._calculate_focality_metrics(\n            field_values,  # Use entire surface data, not just ROI\n            node_areas,    # Use all node areas, not just ROI\n            target_region\n        )\n\n        # Save results to CSV\n        self.visualizer.save_results_to_csv(results, 'cortical', target_region, 'node')\n\n        # Save extra info CSV with focality data\n        if focality_info:\n            self.visualizer.save_extra_info_to_csv(focality_info, 'cortical', target_region, 'node')\n\n        return results\n\n    except Exception as e:\n        self.logger.error(f\"Error in cortical analysis: {str(e)}\")\n        raise\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.mesh_analyzer.MeshAnalyzer.analyze_sphere","title":"analyze_sphere","text":"<pre><code>analyze_sphere(center_coordinates, radius, visualize=False, save_results=True)\n</code></pre> <p>Analyze a spherical region of interest from a mesh.</p> <p>Args:     center_coordinates: List of [x, y, z] coordinates for the center of the sphere     radius: Radius of the sphere in mm     visualize: Whether to generate visualization files     save_results: Whether to save individual CSV files (default: True, set False for batch processing)</p> <p>Returns:     Dictionary containing analysis results or None if no nodes found</p> Source code in <code>tit/analyzer/mesh_analyzer.py</code> <pre><code>def analyze_sphere(self, center_coordinates, radius, visualize=False, save_results=True):\n    \"\"\"\n    Analyze a spherical region of interest from a mesh.\n\n    Args:\n        center_coordinates: List of [x, y, z] coordinates for the center of the sphere\n        radius: Radius of the sphere in mm\n        visualize: Whether to generate visualization files\n        save_results: Whether to save individual CSV files (default: True, set False for batch processing)\n\n    Returns:\n        Dictionary containing analysis results or None if no nodes found\n    \"\"\"\n    self.logger.info(f\"Starting spherical ROI analysis (radius={radius}mm) at coordinates {center_coordinates}\")\n\n    try:\n        surface_mesh_path = self._generate_surface_mesh()\n        gm_surf = simnibs.read_msh(surface_mesh_path)\n\n        # Check if field exists\n        if self.field_name not in gm_surf.field:\n            available_fields = list(gm_surf.field.keys())\n            raise ValueError(f\"Field '{self.field_name}' not found in surface mesh. Available fields: {available_fields}\")\n\n        # Get field values from surface mesh\n        field_values = gm_surf.field[self.field_name].value\n\n        # Create spherical ROI on surface mesh (use canonical helper)\n        self.logger.info(f\"Creating spherical ROI at {center_coordinates} with radius {radius}mm...\")\n        node_coords = gm_surf.nodes.node_coord\n        roi_indices = ROICoordinateHelper.find_voxels_in_sphere(node_coords, center_coordinates, radius)\n        roi_mask = np.zeros(len(node_coords), dtype=bool)\n        roi_mask[roi_indices] = True\n\n        # Check if we have any nodes in the ROI\n        roi_nodes_count = np.sum(roi_mask)\n        if roi_nodes_count == 0:\n            self.logger.error(f\"Analysis Failed: No nodes found in ROI at [{center_coordinates[0]}, {center_coordinates[1]}, {center_coordinates[2]}], r={radius}mm\")\n            self.logger.error(\"ROI is not capturing any grey matter surface nodes\")\n            self.logger.error(\"Suggestion: Adjust coordinates/radius or verify using freeview\")\n            return None\n\n        self.logger.info(f\"Found {roi_nodes_count} nodes in the ROI\")\n        self.logger.info(\"Calculating statistics...\")\n\n        # Get the field values within the ROI\n        field_values_in_roi = field_values[roi_mask]\n\n        # Filter for positive values in ROI (matching voxel analyzer behavior)\n        positive_mask = field_values_in_roi &gt; 0\n        field_values_positive = field_values_in_roi[positive_mask]\n\n        # Check if we have any positive values in the ROI\n        positive_count = len(field_values_positive)\n        if positive_count == 0:\n            self.logger.warning(f\"Warning: No positive values found in spherical ROI\")\n            return None\n\n        self.logger.info(f\"Found {positive_count} nodes with positive values in the ROI\")\n        self.logger.info(\"Calculating statistics...\")\n\n        # Calculate statistics using canonical ROI metric helper\n        min_value = np.min(field_values_positive)\n        node_areas = gm_surf.nodes_areas()\n        if hasattr(node_areas, 'value'):\n            node_areas = node_areas.value\n        node_areas = np.asarray(node_areas)\n        positive_node_areas = node_areas[roi_mask][positive_mask]\n\n        whole_brain_positive_mask = field_values &gt; 0\n        whole_brain_node_areas = node_areas[whole_brain_positive_mask]\n        metrics = calculate_roi_metrics(\n            field_values_positive,\n            positive_node_areas,\n            ti_field_gm=field_values[whole_brain_positive_mask],\n            gm_volumes=whole_brain_node_areas,\n        )\n        mean_value = metrics[\"TImean_ROI\"]\n        max_value = metrics[\"TImax_ROI\"]\n        focality = metrics.get(\"Focality\")\n        whole_brain_average = metrics.get(\"TImean_GM\")\n\n        # Log the whole brain average for debugging\n        if whole_brain_average is not None:\n            self.logger.info(f\"Whole brain average (denominator for focality): {whole_brain_average:.6f}\")\n\n        # Create results dictionary with TI_max values\n        results = {\n            'mean_value': mean_value,\n            'max_value': max_value,\n            'min_value': min_value,\n            'nodes_in_roi': roi_nodes_count,\n            'focality': focality\n        }\n\n        # Extract TI_normal values from normal mesh\n        self.logger.info(\"Extracting TI_normal values from normal mesh...\")\n        normal_field_values_positive, normal_statistics = self._extract_normal_field_values(roi_mask)\n\n        # Add TI_normal statistics to results if available\n        if normal_statistics is not None:\n            results.update(normal_statistics)\n            self.logger.debug(\"TI_normal values successfully added to results\")\n        else:\n            self.logger.warning(\"TI_normal values not available - results will only contain TI_max values\")\n\n        # Generate visualizations if requested\n        if visualize:\n            if self.visualizer is not None:\n                self.logger.info(\"Generating visualizations...\")\n\n                # Generate focality histogram\n                try:\n                    self.logger.info(\"Generating focality histogram for spherical ROI...\")\n                    self.visualizer.generate_focality_histogram(\n                        whole_head_field_data=field_values,\n                        roi_field_data=field_values_positive,\n                        whole_head_element_sizes=node_areas,\n                        roi_element_sizes=positive_node_areas,\n                        region_name=f\"sphere_x{center_coordinates[0]:.2f}_y{center_coordinates[1]:.2f}_z{center_coordinates[2]:.2f}_r{radius}\",\n                        roi_field_value=mean_value,\n                        data_type='node'\n                    )\n                except Exception as e:\n                    self.logger.warning(f\"Could not generate focality histogram for spherical ROI: {str(e)}\")\n\n                # Create spherical ROI overlay visualization\n                # This creates a mesh file with the ROI highlighted\n                self.logger.info(\"Creating spherical ROI overlay visualization...\")\n                viz_file = self.visualizer.visualize_spherical_roi(\n                    gm_surf=gm_surf,\n                    roi_mask=roi_mask,\n                    center_coords=center_coordinates,\n                    radius=radius,\n                    field_values=field_values,\n                    max_value=max_value,\n                    output_dir=self.output_dir,\n                    surface_mesh_path=surface_mesh_path\n                )\n                results['visualization_file'] = viz_file\n            else:\n                self.logger.warning(\"Visualization requested but MeshVisualizer not available\")\n\n        # Calculate and save extra focality information for entire grey matter\n        self.logger.info(\"Calculating focality metrics for entire grey matter...\")\n        focality_info = self._calculate_focality_metrics(\n            field_values,  # Use entire surface data, not just ROI\n            node_areas,    # Use all node areas, not just ROI\n            f\"sphere_x{center_coordinates[0]:.2f}_y{center_coordinates[1]:.2f}_z{center_coordinates[2]:.2f}_r{radius}\"\n        )\n\n        # Save results to CSV (only if save_results=True)\n        if save_results and self.visualizer is not None:\n            region_name = f\"sphere_x{center_coordinates[0]:.2f}_y{center_coordinates[1]:.2f}_z{center_coordinates[2]:.2f}_r{radius}\"\n            self.visualizer.save_results_to_csv(results, 'spherical', region_name, 'node')\n\n            # Save extra info CSV with focality data\n            if focality_info:\n                self.visualizer.save_extra_info_to_csv(focality_info, 'spherical', region_name, 'node')\n        elif save_results:\n            self.logger.warning(\"Cannot save results to CSV - MeshVisualizer not available\")\n\n        return results\n\n    except Exception as e:\n        self.logger.error(f\"Error in spherical analysis: {str(e)}\")\n        raise\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.mesh_analyzer.MeshAnalyzer.analyze_whole_head","title":"analyze_whole_head","text":"<pre><code>analyze_whole_head(atlas_type='HCP_MMP1', visualize=False)\n</code></pre> <p>Analyze all regions in the specified atlas.</p> Source code in <code>tit/analyzer/mesh_analyzer.py</code> <pre><code>def analyze_whole_head(self, atlas_type='HCP_MMP1', visualize=False):\n    \"\"\"\n    Analyze all regions in the specified atlas.\n    \"\"\"\n    start_time = time.time()\n    self.logger.info(f\"Starting whole head analysis using {atlas_type} atlas\")\n\n    try:\n        surface_mesh_path = self._generate_surface_mesh()\n        self.logger.info(\"Loading surface mesh...\")\n        gm_surf = simnibs.read_msh(surface_mesh_path)\n\n        # Load the atlas\n        self.logger.info(f\"Loading atlas {atlas_type}...\")\n        atlas = simnibs.subject_atlas(atlas_type, self.subject_dir)\n\n        # Dictionary to store results for each region\n        results = {}\n\n        # Analyze each region in the atlas\n        for region_name in atlas.keys():\n            try:\n                self.logger.info(f\"Processing region: {region_name}\")\n\n                # Create a directory for this region in the main output directory\n                region_dir = os.path.join(self.output_dir, region_name)\n                os.makedirs(region_dir, exist_ok=True)\n\n                # Get ROI mask for this region\n                try:\n                    roi_mask = atlas[region_name]\n                    # Check if roi_mask is actually a mask array\n                    if callable(roi_mask):\n                        self.logger.error(f\"Region {region_name}: atlas[region_name] returned a callable object, not a mask\")\n                        raise TypeError(f\"Region {region_name}: Expected mask array, got callable object\")\n                    elif not hasattr(roi_mask, '__len__'):\n                        self.logger.error(f\"Region {region_name}: atlas[region_name] returned non-array object: {type(roi_mask)}\")\n                        raise TypeError(f\"Region {region_name}: Expected mask array, got {type(roi_mask)}\")\n                except Exception as e:\n                    self.logger.error(f\"Failed to get ROI mask for region {region_name}: {str(e)}\")\n                    raise\n\n                # Check if we have any nodes in the ROI\n                roi_nodes_count = np.sum(roi_mask)\n                if roi_nodes_count == 0:\n                    self.logger.warning(f\"Warning: No nodes found in the specified region '{region_name}'\")\n                    region_results = {\n                        'mean_value': None,\n                        'max_value': None,\n                        'min_value': None,\n                        'focality': None,\n                        'nodes_in_roi': 0,\n                        'normal_mean_value': None,\n                        'normal_max_value': None,\n                        'normal_min_value': None,\n                        'normal_focality': None\n                    }\n\n                    # Store in the overall results\n                    results[region_name] = region_results\n\n                    continue\n\n                self.logger.info(f\"Getting field values within the ROI...\")\n                # Get the field values within the ROI\n                try:\n                    field_values = gm_surf.field[self.field_name].value\n                    if callable(field_values):\n                        self.logger.error(f\"Region {region_name}: field values returned a callable object\")\n                        raise TypeError(f\"Region {region_name}: Expected field values array, got callable object\")\n                    field_values_in_roi = field_values[roi_mask]\n                except Exception as e:\n                    self.logger.error(f\"Failed to get field values for region {region_name}: {str(e)}\")\n                    self.logger.error(f\"Field values type: {type(field_values) if 'field_values' in locals() else 'undefined'}\")\n                    self.logger.error(f\"ROI mask type: {type(roi_mask)}\")\n                    raise\n\n                # Filter for positive values in ROI (matching voxel analyzer behavior)\n                positive_mask = field_values_in_roi &gt; 0\n                field_values_positive = field_values_in_roi[positive_mask]\n\n                # Check if we have any positive values in the ROI\n                positive_count = len(field_values_positive)\n                if positive_count == 0:\n                    self.logger.warning(f\"Warning: Region {region_name} has no positive values\")\n                    region_results = {\n                        'mean_value': None,\n                        'max_value': None,\n                        'min_value': None,\n                        'focality': None,\n                        'nodes_in_roi': 0,\n                        'normal_mean_value': None,\n                        'normal_max_value': None,\n                        'normal_min_value': None,\n                        'normal_focality': None\n                    }\n\n                    # Store in the overall results\n                    results[region_name] = region_results\n\n                    continue\n\n                # Calculate statistics on positive values only\n                min_value = np.min(field_values_positive)\n                max_value = np.max(field_values_positive)\n\n                # Calculate mean value using node areas for proper averaging (only positive values)\n                try:\n                    node_areas = gm_surf.nodes_areas()\n                    if hasattr(node_areas, 'value'):\n                        node_areas = node_areas.value\n                    node_areas = np.asarray(node_areas)\n                    if callable(node_areas):\n                        self.logger.error(f\"Region {region_name}: nodes_areas() returned a callable object\")\n                        raise TypeError(f\"Region {region_name}: Expected node areas array, got callable object\")\n                    positive_node_areas = node_areas[roi_mask][positive_mask]\n                except Exception as e:\n                    self.logger.error(f\"Failed to get node areas for region {region_name}: {str(e)}\")\n                    self.logger.error(f\"Node areas type: {type(node_areas) if 'node_areas' in locals() else 'undefined'}\")\n                    raise\n                # Calculate mean + focality using canonical ROI metric helper\n                whole_brain_positive_mask = field_values &gt; 0\n                whole_brain_node_areas = node_areas[whole_brain_positive_mask]\n                metrics = calculate_roi_metrics(\n                    field_values_positive,\n                    positive_node_areas,\n                    ti_field_gm=field_values[whole_brain_positive_mask],\n                    gm_volumes=whole_brain_node_areas,\n                )\n                mean_value = metrics[\"TImean_ROI\"]\n                focality = metrics.get(\"Focality\")\n                whole_brain_average = metrics.get(\"TImean_GM\")\n\n                # Log the whole brain average for debugging\n                if whole_brain_average is not None:\n                    self.logger.info(f\"Whole brain average (denominator for focality): {whole_brain_average:.6f}\")\n\n                # Create result dictionary for this region with TI_max values\n                region_results = {\n                    'mean_value': mean_value,\n                    'max_value': max_value,\n                    'min_value': min_value,\n                    'focality': focality,\n                    'nodes_in_roi': positive_count\n                }\n\n                # Extract TI_normal values from normal mesh for this region\n                self.logger.debug(f\"Extracting TI_normal values for region: {region_name}\")\n                normal_field_values_positive, normal_statistics = self._extract_normal_field_values(roi_mask)\n\n                # Add TI_normal statistics to region results if available\n                if normal_statistics is not None:\n                    region_results.update(normal_statistics)\n                    self.logger.debug(f\"TI_normal values successfully added for region: {region_name}\")\n                else:\n                    self.logger.warning(f\"TI_normal values not available for region: {region_name}\")\n\n                # Store in the overall results\n                results[region_name] = region_results\n\n                # Generate visualizations if requested\n                if visualize:\n                    self.logger.info(f\"Generating 3D visualization for region: {region_name}\")\n                    # Generate 3D visualization and save directly to the region directory\n                    self.visualizer.visualize_cortex_roi(\n                        gm_surf=gm_surf,\n                        roi_mask=roi_mask,\n                        target_region=region_name,\n                        field_values=field_values,\n                        max_value=max_value,\n                        output_dir=region_dir,\n                        surface_mesh_path=self._surface_mesh_path\n                    )\n\n                    # Generate focality histogram for this region\n                    if len(field_values_positive) &gt; 0:\n                        try:\n                            self.logger.info(f\"Generating focality histogram for region: {region_name}\")\n                            # Get node areas for volume weighting\n                            node_areas = gm_surf.nodes_areas()\n                            if hasattr(node_areas, 'value'):\n                                node_areas = node_areas.value\n                            node_areas = np.asarray(node_areas)\n                            positive_node_areas = node_areas[roi_mask][positive_mask]\n\n                            # Create a custom visualizer just for this region with the region directory as output\n                            # Pass the main logger to avoid creating derivatives folder in region directory\n                            region_visualizer = MeshVisualizer(region_dir, self.logger)\n\n                            region_visualizer.generate_focality_histogram(\n                                whole_head_field_data=field_values,\n                                roi_field_data=field_values_positive,\n                                whole_head_element_sizes=node_areas,\n                                roi_element_sizes=positive_node_areas,\n                                region_name=region_name,\n                                roi_field_value=mean_value,\n                                data_type='node'\n                            )\n                        except Exception as e:\n                            self.logger.warning(f\"Could not generate focality histogram for {region_name}: {str(e)}\")\n\n            except Exception as e:\n                self.logger.warning(f\"Warning: Failed to analyze region {region_name}: {str(e)}\")\n                results[region_name] = {\n                    'mean_value': None,\n                    'max_value': None,\n                    'min_value': None,\n                    'focality': None,\n                    'nodes_in_roi': 0,\n                    'normal_mean_value': None,\n                    'normal_max_value': None,\n                    'normal_min_value': None,\n                    'normal_focality': None\n                }\n\n        # Generate global visualization plots are disabled - only histograms are generated per region\n\n        # Always generate and save summary CSV after all regions are processed\n        self.logger.info(\"Saving whole-head analysis summary to CSV...\")\n        self.visualizer.save_whole_head_results_to_csv(results, atlas_type, 'node')\n\n        return results\n\n    finally:\n        # Clean up\n        try:\n            del gm_surf\n            del atlas\n        except NameError:\n            # Variables may not be defined if cleanup failed earlier\n            pass\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.mesh_analyzer.MeshAnalyzer.get_grey_matter_statistics","title":"get_grey_matter_statistics","text":"<pre><code>get_grey_matter_statistics()\n</code></pre> <p>Calculate grey matter field statistics from the GM surface (central layer).</p> <p>For mesh analysis, tries to use the same GM surface as other analyses for consistency. Falls back to original mesh if surface generation fails to ensure robustness.</p> <p>Returns:     dict: Dictionary containing grey matter statistics (mean, max, min)</p> Source code in <code>tit/analyzer/mesh_analyzer.py</code> <pre><code>def get_grey_matter_statistics(self):\n    \"\"\"\n    Calculate grey matter field statistics from the GM surface (central layer).\n\n    For mesh analysis, tries to use the same GM surface as other analyses for consistency.\n    Falls back to original mesh if surface generation fails to ensure robustness.\n\n    Returns:\n        dict: Dictionary containing grey matter statistics (mean, max, min)\n    \"\"\"\n    self.logger.info(\"Calculating grey matter field statistics from GM surface (central layer)...\")\n\n    try:\n        # Try to generate and load GM surface mesh (same as other mesh analyses)\n        surface_mesh_path = self._generate_surface_mesh()\n        gm_surf = simnibs.read_msh(surface_mesh_path)\n\n        # Check if field exists\n        if self.field_name not in gm_surf.field:\n            available_fields = list(gm_surf.field.keys())\n            raise ValueError(f\"Field '{self.field_name}' not found in GM surface. Available fields: {available_fields}\")\n\n        # Get field values from GM surface (same source as other mesh analyses)\n        field_values = gm_surf.field[self.field_name].value\n\n        # Filter for positive values only (matching ROI analysis behavior)\n        positive_mask = field_values &gt; 0\n        field_values_positive = field_values[positive_mask]\n\n        # Check if we have any positive values\n        if len(field_values_positive) == 0:\n            self.logger.warning(\"No positive values found in GM surface\")\n            return {'grey_mean': 0.0, 'grey_max': 0.0, 'grey_min': 0.0}\n\n        # Calculate statistics using area-weighted averaging (consistent with other mesh analyses)\n        node_areas = gm_surf.nodes_areas()\n        if hasattr(node_areas, 'value'):\n            node_areas = node_areas.value\n        node_areas = np.asarray(node_areas)\n        positive_node_areas = node_areas[positive_mask]\n        grey_mean = np.average(field_values_positive, weights=positive_node_areas)\n        grey_max = np.max(field_values_positive)\n        grey_min = np.min(field_values_positive)\n\n        self.logger.info(f\"Grey matter statistics from GM surface for field '{self.field_name}' (positive values only): \"\n                       f\"mean={grey_mean:.6f}, max={grey_max:.6f}, min={grey_min:.6f}\")\n        self.logger.info(f\"Total nodes with positive values: {len(field_values_positive)}\")\n\n        return {\n            'grey_mean': float(grey_mean),\n            'grey_max': float(grey_max),\n            'grey_min': float(grey_min)\n        }\n\n    except Exception as e:\n        self.logger.warning(f\"GM surface generation failed: {str(e)}\")\n        self.logger.info(\"Falling back to original mesh for grey matter statistics...\")\n\n        # Fallback: Use original mesh if surface generation fails\n        try:\n            field_mesh = simnibs.read_msh(self.field_mesh_path)\n\n            if self.field_name not in field_mesh.field:\n                available_fields = list(field_mesh.field.keys())\n                raise ValueError(f\"Field '{self.field_name}' not found in original mesh. Available fields: {available_fields}\")\n\n            field_values = field_mesh.field[self.field_name].value\n            positive_mask = field_values &gt; 0\n            field_values_positive = field_values[positive_mask]\n\n            if len(field_values_positive) == 0:\n                self.logger.warning(\"No positive values found in original mesh\")\n                return {'grey_mean': 0.0, 'grey_max': 0.0, 'grey_min': 0.0}\n\n            # Use simple averaging for volume mesh fallback\n            grey_mean = np.mean(field_values_positive)\n            grey_max = np.max(field_values_positive)\n            grey_min = np.min(field_values_positive)\n\n            self.logger.info(f\"Grey matter statistics from original mesh (fallback) for field '{self.field_name}': \"\n                           f\"mean={grey_mean:.6f}, max={grey_max:.6f}, min={grey_min:.6f}\")\n\n            return {\n                'grey_mean': float(grey_mean),\n                'grey_max': float(grey_max),\n                'grey_min': float(grey_min)\n            }\n\n        except Exception as fallback_error:\n            self.logger.error(f\"Fallback to original mesh also failed: {str(fallback_error)}\")\n            return {'grey_mean': 0.0, 'grey_max': 0.0, 'grey_min': 0.0}\n</code></pre>"},{"location":"reference/analyzer/#voxel-analyzer-titanalyzervoxel_analyzer","title":"Voxel analyzer (<code>tit.analyzer.voxel_analyzer</code>)","text":"<p>VoxelAnalyzer: A tool for analyzing voxel-based neuroimaging data</p> <p>This module provides functionality for analyzing voxel-based data from medical imaging, particularly focusing on field analysis in specific regions of interest (ROIs).</p> <p>Inputs:     - NIfTI files containing field data     - Atlas files (NIfTI/MGZ) for cortical parcellation     - ROI specifications (coordinates, regions)</p> <p>Outputs:     - Statistical measures (mean, min, max)     - ROI masks     - Visualization files (optional)</p> <p>Example Usage:     ```python     # Initialize analyzer     analyzer = VoxelAnalyzer(         field_nifti=\"/path/to/field.nii.gz\",         subject_dir=\"/path/to/subject\",         output_dir=\"/path/to/output\"     )</p> <pre><code># Analyze a spherical ROI\nsphere_results = analyzer.analyze_sphere(\n    center_coordinates=[0, 0, 0],\n    radius=10,\n    visualize=True\n)\n\n# Analyze a cortical region\ncortex_results = analyzer.analyze_cortex(\n    atlas_file=\"/path/to/atlas.mgz\",\n    target_region=\"Left-Hippocampus\"\n)\n\n# Analyze whole head\nwhole_head_results = analyzer.analyze_whole_head(\n    atlas_file=\"/path/to/atlas.mgz\",\n    visualize=True\n)\n```\n</code></pre> <p>Dependencies:     - numpy     - nibabel     - subprocess (for FreeSurfer operations)</p>"},{"location":"reference/analyzer/#tit.analyzer.voxel_analyzer.VoxelAnalyzer","title":"VoxelAnalyzer","text":"<pre><code>VoxelAnalyzer(field_nifti: str, subject_dir: str, output_dir: str, logger=None, quiet=False)\n</code></pre> <p>A class for analyzing voxel-based data from medical imaging.</p> <p>This class provides methods for analyzing field data in specific regions of interest, including spherical ROIs and cortical regions defined by an atlas.</p> <p>Attributes:     field_nifti (str): Path to the NIfTI file containing field data     subject_dir (str): Directory containing subject data     output_dir (str): Directory where analysis results will be saved     visualizer (VoxelVisualizer): Instance of visualizer for generating plots</p> <p>Initialize the VoxelAnalyzer with paths to required data.</p> <p>Args:     field_nifti (str): Path to the NIfTI file containing field data     subject_dir (str): Directory containing subject data     output_dir (str): Directory where analysis results will be saved     logger: Optional logger instance to use. If None, creates its own.     quiet (bool): If True, suppress verbose logging messages</p> <p>Raises:     FileNotFoundError: If field_nifti file does not exist</p> Source code in <code>tit/analyzer/voxel_analyzer.py</code> <pre><code>def __init__(self, field_nifti: str, subject_dir: str, output_dir: str, logger=None, quiet=False):\n    \"\"\"\n    Initialize the VoxelAnalyzer with paths to required data.\n\n    Args:\n        field_nifti (str): Path to the NIfTI file containing field data\n        subject_dir (str): Directory containing subject data\n        output_dir (str): Directory where analysis results will be saved\n        logger: Optional logger instance to use. If None, creates its own.\n        quiet (bool): If True, suppress verbose logging messages\n\n    Raises:\n        FileNotFoundError: If field_nifti file does not exist\n    \"\"\"\n    # Check for required dependencies\n    if nib is None:\n        raise ImportError(\"nibabel is required for voxel analysis but is not installed\")\n\n    self.field_nifti = field_nifti\n    self.subject_dir = subject_dir\n    self.output_dir = output_dir\n    self.quiet = quiet\n\n    # Validate early (before logger/file handlers) so missing input paths fail\n    # with the intended error rather than a secondary logging error.\n    if not os.path.exists(field_nifti):\n        raise FileNotFoundError(f\"Field file not found: {field_nifti}\")\n\n    # Set up logger - use provided logger or create a new one\n    if logger is not None:\n        # Create a child logger to distinguish voxel analyzer logs\n        self.logger = logger.getChild('voxel_analyzer')\n    else:\n        # Create our own logger if none provided\n        time_stamp = time.strftime('%Y%m%d_%H%M%S')\n\n        # Extract subject ID from subject_dir (e.g., m2m_subject -&gt; subject)\n        subject_id = os.path.basename(self.subject_dir).split('_')[1] if '_' in os.path.basename(self.subject_dir) else os.path.basename(self.subject_dir)\n\n        # Create derivatives/ti-toolbox/logs/sub-* directory structure\n        pm = get_path_manager()\n        log_dir = pm.path(\"ti_logs\", subject_id=subject_id)\n        os.makedirs(log_dir, exist_ok=True)\n\n        # Create log file in the new directory\n        log_file = os.path.join(log_dir, f'voxel_analyzer_{time_stamp}.log')\n        self.logger = logging_util.get_logger('voxel_analyzer', log_file, overwrite=True)\n\n    # Initialize visualizer with logger\n    self.visualizer = VoxelVisualizer(output_dir, self.logger)\n\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        self.logger.debug(f\"Creating output directory: {output_dir}\")\n        os.makedirs(output_dir)\n\n    self.logger.debug(f\"Voxel analyzer initialized successfully\")\n    self.logger.debug(f\"Field NIfTI path: {field_nifti}\")\n    self.logger.debug(f\"Subject directory: {subject_dir}\")\n    self.logger.debug(f\"Output directory: {output_dir}\")\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.voxel_analyzer.VoxelAnalyzer.analyze_cortex","title":"analyze_cortex","text":"<pre><code>analyze_cortex(atlas_file, target_region, region_info=None, atlas_data=None, field_data=None, visualize=False)\n</code></pre> <p>Analyze a field scan within a specific cortical region defined in an atlas.</p> Source code in <code>tit/analyzer/voxel_analyzer.py</code> <pre><code>def analyze_cortex(self, atlas_file, target_region, region_info=None, atlas_data=None, field_data=None, visualize=False):\n    \"\"\"\n    Analyze a field scan within a specific cortical region defined in an atlas.\n    \"\"\"\n    # Extract atlas type from filename\n    atlas_type = self._extract_atlas_type(atlas_file)\n\n    # Load the atlas and field data if not provided\n    if atlas_data is None:\n        self.logger.debug(f\"Loading atlas from {atlas_file}...\")\n        atlas_tuple = self.load_brain_image(atlas_file)\n        atlas_img, atlas_arr = atlas_tuple\n    else:\n        # Unpack the tuple\n        atlas_img, atlas_arr = atlas_data\n\n    if field_data is None:\n        self.logger.debug(f\"Loading field from {self.field_nifti}...\")\n        field_tuple = self.load_brain_image(self.field_nifti)\n        field_img, field_arr = field_tuple\n    else:\n        # Unpack the tuple\n        field_img, field_arr = field_data\n\n    # Handle 4D field data (extract first volume if multiple volumes)\n    if len(field_arr.shape) == 4:\n        if not self.quiet:\n            self.logger.info(f\"Detected 4D field data with shape {field_arr.shape}\")\n        field_shape_3d = field_arr.shape[:3]\n        # If time dimension is 1, we can simply reshape to 3D\n        if field_arr.shape[3] == 1:\n            if not self.quiet:\n                self.logger.info(\"Reshaping 4D field data to 3D\")\n            field_arr = field_arr[:,:,:,0]\n        else:\n            self.logger.warning(f\"4D field has {field_arr.shape[3]} volumes. Using only the first volume.\")\n            field_arr = field_arr[:,:,:,0]\n    else:\n        field_shape_3d = field_arr.shape\n\n    # Compare spatial dimensions for atlas and field\n    if atlas_arr.shape != field_shape_3d:\n        if not self.quiet:\n            self.logger.info(\"Atlas and field dimensions don't match, attempting to resample...\")\n        self.logger.debug(f\"Atlas shape: {atlas_arr.shape}\")\n        self.logger.debug(f\"Field shape: {field_arr.shape}\")\n\n        # Resample the atlas to match the field data, passing atlas_file\n        atlas_img, atlas_arr = self.resample_to_match(\n            atlas_img,\n            field_shape_3d,  # Use only the spatial dimensions\n            field_img.affine,\n            source_path=atlas_file  # Pass the atlas file path\n        )\n\n        # Verify the resampling worked\n        if atlas_arr.shape != field_shape_3d:\n            raise ValueError(f\"Failed to resample atlas to match field dimensions: {atlas_arr.shape} vs {field_shape_3d}\")\n    else:\n        if not self.quiet:\n            self.logger.info(\"Atlas and field dimensions already match - skipping resampling\")\n\n    # Load region information if not provided\n    if region_info is None:\n        region_info = self.get_atlas_regions(atlas_file)\n\n    # Determine region ID based on target_region\n    if not self.quiet:\n        self.logger.info(f\"Finding region information for {target_region}...\")\n    region_id, region_name = self.find_region(target_region, region_info)\n    if not self.quiet:\n        self.logger.info(f\"Processing region: {region_name} (ID: {region_id})\")\n\n    # Create mask for this region\n    region_mask = (atlas_arr == region_id)\n\n    # Check if the mask contains any voxels\n    mask_count = np.sum(region_mask)\n    if mask_count == 0:\n        self.logger.warning(f\"Warning: Region {region_name} (ID: {region_id}) contains 0 voxels in the atlas\")\n        results = {\n            'mean_value': None,\n            'max_value': None,\n            'min_value': None,\n            'focality': None\n        }\n\n        # Save results to CSV even if empty\n        self.visualizer.save_results_to_csv(results, 'cortical', region_name, 'voxel')\n\n        return results\n\n    # Filter for voxels with positive values\n    value_mask = (field_arr &gt; 0)\n    combined_mask = region_mask &amp; value_mask\n\n    # Extract field values after filtering\n    field_values = field_arr[combined_mask]\n\n    # Check if any voxels remain after filtering\n    filtered_count = len(field_values)\n    if filtered_count == 0:\n        self.logger.warning(f\"Warning: Region {region_name} (ID: {region_id}) has no voxels with positive values\")\n        results = {\n            'mean_value': None,\n            'max_value': None,\n            'min_value': None,\n            'focality': None\n        }\n\n        # Save results to CSV even if empty\n        self.visualizer.save_results_to_csv(results, 'cortical', region_name, 'voxel')\n\n        return results\n\n    if not self.quiet:\n        self.logger.info(\"Calculating statistics...\")\n    max_value = np.max(field_values)\n    min_value = np.min(field_values)\n\n    whole_brain_positive_mask = field_arr &gt; 0\n    gm_values = field_arr[whole_brain_positive_mask]\n    metrics = calculate_roi_metrics(\n        field_values,\n        np.ones(len(field_values), dtype=float),\n        ti_field_gm=gm_values,\n        gm_volumes=np.ones(len(gm_values), dtype=float),\n    )\n    mean_value = metrics[\"TImean_ROI\"]\n    focality = metrics.get(\"Focality\")\n    whole_brain_average = metrics.get(\"TImean_GM\")\n\n    # Log the whole brain average for debugging\n    if not self.quiet:\n        self.logger.info(f\"Whole brain average (denominator for focality): {whole_brain_average:.6f}\")\n\n    # Prepare results dictionary\n    results = {\n        'mean_value': mean_value,\n        'max_value': max_value,\n        'min_value': min_value,\n        'focality': focality,\n        'voxels_in_roi': filtered_count\n    }\n\n    # Generate visualization if requested\n    if visualize:\n        if not self.quiet:\n            self.logger.info(\"Generating visualizations...\")\n\n        # Generate focality histogram\n        try:\n            if not self.quiet:\n                self.logger.info(f\"Generating focality histogram for region: {region_name}\")\n            # Get voxel dimensions from the field image\n            voxel_dims = field_img.header.get_zooms()[:3]\n\n            # Filter out zero values from whole head data for histogram\n            whole_head_positive_mask = field_arr &gt; 0\n            whole_head_filtered = field_arr[whole_head_positive_mask]\n\n            self.visualizer.generate_focality_histogram(\n                whole_head_field_data=whole_head_filtered,\n                roi_field_data=field_values,\n                region_name=region_name,\n                roi_field_value=mean_value,\n                data_type='voxel',\n                voxel_dims=voxel_dims\n            )\n        except Exception as e:\n            self.logger.warning(f\"Could not generate focality histogram for {region_name}: {str(e)}\")\n\n        # Create visualization NIfTI file\n        viz_file = self.visualizer.create_cortex_nifti(\n            atlas_img=atlas_img,\n            atlas_arr=atlas_arr,\n            field_arr=field_arr,\n            region_id=region_id,\n            region_name=region_name\n        )\n\n    # Calculate and save extra focality information for entire field\n    if not self.quiet:\n        self.logger.info(\"Calculating focality metrics for entire field...\")\n\n    # Get voxel dimensions for volume calculations\n    voxel_dims = field_img.header.get_zooms()[:3]\n    voxel_volume = np.prod(voxel_dims)  # Volume of one voxel in mm\u00b3\n\n    # Use entire field data (positive values only) for focality calculation\n    entire_field_positive = field_arr[field_arr &gt; 0]\n\n    focality_info = self._calculate_focality_metrics(\n        entire_field_positive,  # Use entire field, not just ROI\n        voxel_volume, \n        region_name\n    )\n\n    # Save results to CSV\n    self.visualizer.save_results_to_csv(results, 'cortical', region_name, 'voxel')\n\n    # Save extra info CSV with focality data\n    if focality_info:\n        self.visualizer.save_extra_info_to_csv(focality_info, 'cortical', region_name, 'voxel')\n\n    # Return analysis results\n    return results\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.voxel_analyzer.VoxelAnalyzer.analyze_sphere","title":"analyze_sphere","text":"<pre><code>analyze_sphere(center_coordinates, radius, visualize=False)\n</code></pre> <p>Analyze a spherical region of interest from voxel data.</p> <p>Returns:     Dictionary containing analysis results or None if no valid voxels found</p> Source code in <code>tit/analyzer/voxel_analyzer.py</code> <pre><code>    def analyze_sphere(self, center_coordinates, radius, visualize=False):\n        \"\"\"\n        Analyze a spherical region of interest from voxel data.\n\n        Returns:\n            Dictionary containing analysis results or None if no valid voxels found\n        \"\"\"\n        if not self.quiet:\n            self.logger.info(f\"Starting spherical ROI analysis (radius={radius}mm) at coordinates {center_coordinates}\")\n\n        # Load the NIfTI data\n        if not self.quiet:\n            self.logger.info(\"Loading field data...\")\n        img = nib.load(self.field_nifti)\n        field_data = img.get_fdata()\n\n        # Handle 4D field data (extract first volume if multiple volumes)\n        if len(field_data.shape) == 4:\n            if field_data.shape[3] == 1:\n                field_data = field_data[:,:,:,0]\n            else:\n                field_data = field_data[:,:,:,0]\n\n        # Get voxel dimensions (for proper distance calculation)\n        voxel_size = np.array(img.header.get_zooms()[:3])\n\n        # Get affine transformation matrix\n        affine = img.affine\n\n        # Convert world coordinates to voxel coordinates if needed\n        if not self.quiet:\n            self.logger.info(\"Converting coordinates and creating ROI mask...\")\n        inv_affine = np.linalg.inv(affine)\n        voxel_coords = np.dot(inv_affine, np.append(center_coordinates, 1))[:3]\n\n        # Create coordinate grids for the entire volume\n        x_size, y_size, z_size = field_data.shape\n        x, y, z = np.ogrid[:x_size, :y_size, :z_size]\n\n        # Calculate distance from center voxel (using voxel dimensions to account for anisotropy)\n        dist_from_center = np.sqrt(\n            ((x - voxel_coords[0])**2 * voxel_size[0]**2) +\n            ((y - voxel_coords[1])**2 * voxel_size[1]**2) +\n            ((z - voxel_coords[2])**2 * voxel_size[2]**2)\n        )\n\n        # Create the spherical mask\n        roi_mask = dist_from_center &lt;= radius\n\n        # Create mask for non-zero values\n        nonzero_mask = field_data &gt; 0\n\n        # Combine masks to get only non-zero values within ROI\n        combined_mask = roi_mask &amp; nonzero_mask\n\n        # Count voxels in ROI\n        roi_voxels_count = np.sum(combined_mask)\n        total_roi_voxels = np.sum(roi_mask)\n\n        # Check if we have any voxels in the ROI\n        if roi_voxels_count == 0:\n            # Determine the tissue type from the field NIfTI name\n            field_name = os.path.basename(self.field_nifti).lower()\n            tissue_type = \"grey matter\" if \"grey\" in field_name else \"white matter\" if \"white\" in field_name else \"brain tissue\"\n\n            warning_msg = f\"\"\"\n\\033[93m\u26a0\ufe0f  WARNING: Analysis Failed \u26a0\ufe0f\n\u2022 No valid voxels found in ROI at [{center_coordinates[0]}, {center_coordinates[1]}, {center_coordinates[2]}], r={radius}mm\n\u2022 {\"ROI not intersecting \" + tissue_type if total_roi_voxels == 0 else \"ROI contains only zero/invalid values\"}\n\u2022 {\"Adjust coordinates/radius\" if total_roi_voxels == 0 else \"Check field data\"} or verify using freeview\\033[0m\"\"\"\n            self.logger.warning(warning_msg)\n\n            return None\n\n        if not self.quiet:\n            self.logger.info(\"Calculating statistics...\")\n        # Get the field values within the ROI\n        roi_values = field_data[combined_mask]\n\n        min_value = np.min(roi_values)\n        whole_brain_positive_mask = field_data &gt; 0\n        gm_values = field_data[whole_brain_positive_mask]\n        metrics = calculate_roi_metrics(\n            roi_values,\n            np.ones(len(roi_values), dtype=float),\n            ti_field_gm=gm_values,\n            gm_volumes=np.ones(len(gm_values), dtype=float),\n        )\n        mean_value = metrics[\"TImean_ROI\"]\n        max_value = metrics[\"TImax_ROI\"]\n        focality = metrics.get(\"Focality\")\n        whole_brain_average = metrics.get(\"TImean_GM\")\n\n        # Log the whole brain average for debugging\n        if not self.quiet:\n            self.logger.info(f\"Whole brain average (denominator for focality): {whole_brain_average:.6f}\")\n\n        # Create results dictionary\n        results = {\n            'mean_value': mean_value,\n            'max_value': max_value,\n            'min_value': min_value,\n            'focality': focality,\n            'voxels_in_roi': roi_voxels_count\n        }\n\n        # Generate visualization if requested\n        if visualize:\n            region_name = f\"sphere_x{center_coordinates[0]:.2f}_y{center_coordinates[1]:.2f}_z{center_coordinates[2]:.2f}_r{radius}\"\n\n            # Create visualization overlay (showing field values only within the sphere)\n            vis_arr = np.zeros_like(field_data)\n            vis_arr[combined_mask] = field_data[combined_mask]\n\n            # Save as NIfTI directly to the output directory\n            vis_img = nib.Nifti1Image(vis_arr, affine)\n            output_filename = os.path.join(self.output_dir, f\"sphere_overlay_{region_name}.nii.gz\")\n            nib.save(vis_img, output_filename)\n            if not self.quiet:\n                self.logger.info(f\"Created visualization overlay: {output_filename}\")\n\n            # Visualization requested but only histogram generation is supported\n\n            # Generate focality histogram if visualizer is available\n            if self.visualizer is not None:\n                try:\n                    if not self.quiet:\n                        self.logger.info(\"Generating focality histogram for spherical ROI...\")\n                    # Get voxel dimensions from the loaded image\n                    voxel_dims = img.header.get_zooms()[:3]\n\n                    # Filter out zero values from whole head data for histogram\n                    whole_head_positive_mask = field_data &gt; 0\n                    whole_head_filtered = field_data[whole_head_positive_mask]\n\n                    self.visualizer.generate_focality_histogram(\n                        whole_head_field_data=whole_head_filtered,\n                        roi_field_data=roi_values,\n                        region_name=region_name,\n                        roi_field_value=mean_value,\n                        data_type='voxel',\n                        voxel_dims=voxel_dims\n                    )\n                except Exception as e:\n                    self.logger.warning(f\"Could not generate focality histogram for spherical ROI: {str(e)}\")\n\n        # Calculate and save extra focality information for entire volume\n        if not self.quiet:\n            self.logger.info(\"Calculating focality metrics for entire volume...\")\n        focality_info = self._calculate_focality_metrics(\n            field_data,  # Use entire volume data\n            np.prod(voxel_size),  # Voxel volume\n            f\"sphere_x{center_coordinates[0]:.2f}_y{center_coordinates[1]:.2f}_z{center_coordinates[2]:.2f}_r{radius}\"\n        )\n\n        # Save results to CSV if visualizer is available\n        if self.visualizer is not None:\n            region_name = f\"sphere_x{center_coordinates[0]:.2f}_y{center_coordinates[1]:.2f}_z{center_coordinates[2]:.2f}_r{radius}\"\n            self.visualizer.save_results_to_csv(results, 'spherical', region_name, 'voxel')\n\n            # Save extra info CSV with focality data\n            if focality_info:\n                self.visualizer.save_extra_info_to_csv(focality_info, 'spherical', region_name, 'voxel')\n        else:\n            self.logger.warning(\"Cannot save results to CSV - VoxelVisualizer not available\")\n\n        return results\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.voxel_analyzer.VoxelAnalyzer.analyze_whole_head","title":"analyze_whole_head","text":"<pre><code>analyze_whole_head(atlas_file, visualize=False)\n</code></pre> <p>Analyze all regions in the specified atlas.</p> Source code in <code>tit/analyzer/voxel_analyzer.py</code> <pre><code>def analyze_whole_head(self, atlas_file, visualize=False):\n    \"\"\"\n    Analyze all regions in the specified atlas.\n    \"\"\"\n    start_time = time.time()\n    if not self.quiet:\n        self.logger.info(f\"Starting whole head analysis of atlas: {atlas_file}\")\n\n    # Extract atlas type from filename\n    atlas_type = self._extract_atlas_type(atlas_file)\n    self.logger.debug(f\"Detected atlas type: {atlas_type}\")\n\n    try:\n        # Load region information once\n        region_info = self.get_atlas_regions(atlas_file)\n\n        # Load atlas and field data once\n        self.logger.debug(f\"Loading atlas from {atlas_file}...\")\n        atlas_tuple = self.load_brain_image(atlas_file)\n        atlas_img, atlas_arr = atlas_tuple\n\n        self.logger.debug(f\"Loading field from {self.field_nifti}...\")\n        field_tuple = self.load_brain_image(self.field_nifti)\n        field_img, field_arr = field_tuple\n\n        # Handle 4D field data (extract first volume if multiple volumes)\n        if len(field_arr.shape) == 4:\n            self.logger.debug(f\"Detected 4D field data with shape {field_arr.shape}\")\n            field_shape_3d = field_arr.shape[:3]\n            # If time dimension is 1, we can simply reshape to 3D\n            if field_arr.shape[3] == 1:\n                self.logger.debug(\"Reshaping 4D field data to 3D\")\n                field_arr = field_arr[:,:,:,0]\n            else:\n                self.logger.warning(f\"4D field has {field_arr.shape[3]} volumes. Using only the first volume.\")\n                field_arr = field_arr[:,:,:,0]\n        else:\n            field_shape_3d = field_arr.shape\n\n        # Check if resampling is needed and do it once if necessary\n        if atlas_arr.shape != field_shape_3d:\n            self.logger.debug(\"Atlas and field dimensions don't match, attempting to resample...\")\n            self.logger.debug(f\"Atlas shape: {atlas_arr.shape}\")\n            self.logger.debug(f\"Field shape: {field_arr.shape}\")\n\n            # Resample the atlas to match the field data, passing atlas_file\n            atlas_img, atlas_arr = self.resample_to_match(\n                atlas_img,\n                field_shape_3d,  # Use only the spatial dimensions\n                field_img.affine,\n                source_path=atlas_file  # Pass the atlas file path\n            )\n\n            # Verify the resampling worked\n            if atlas_arr.shape != field_shape_3d:\n                raise ValueError(f\"Failed to resample atlas to match field dimensions: {atlas_arr.shape} vs {field_shape_3d}\")\n        else:\n            self.logger.debug(\"Atlas and field dimensions already match - skipping resampling\")\n\n        field_tuple = (field_img, field_arr)\n\n        # Dictionary to store results for each region\n        results = {}\n\n        # Analyze each region in the atlas\n        for region_id, info in region_info.items():\n            region_name = info['name']\n            try:\n                self.logger.debug(f\"Processing region: {region_name}\")\n\n                # Create a directory for this region in the main output directory\n                region_dir = os.path.join(self.output_dir, region_name)\n                os.makedirs(region_dir, exist_ok=True)\n\n                # Create mask for this region\n                region_mask = (atlas_arr == region_id)\n\n                # Check if the mask contains any voxels\n                mask_count = np.sum(region_mask)\n                if mask_count == 0:\n                    self.logger.warning(f\"Warning: Region {region_name} (ID: {region_id}) contains 0 voxels in the atlas\")\n                    region_results = {\n                        'mean_value': None,\n                        'max_value': None,\n                        'min_value': None,\n                        'focality': None,\n                        'voxels_in_roi': 0\n                    }\n\n                    # Store in the overall results\n                    results[region_name] = region_results\n\n                    continue\n\n                # Filter for voxels with positive values\n                value_mask = (field_arr &gt; 0)\n                combined_mask = region_mask &amp; value_mask\n\n                # Extract field values after filtering\n                field_values = field_arr[combined_mask]\n\n                # Check if any voxels remain after filtering\n                filtered_count = len(field_values)\n                if filtered_count == 0:\n                    self.logger.warning(f\"Warning: Region {region_name} (ID: {region_id}) has no voxels with positive values\")\n                    region_results = {\n                        'mean_value': None,\n                        'max_value': None,\n                        'min_value': None,\n                        'focality': None,\n                        'voxels_in_roi': 0\n                    }\n\n                    # Store in the overall results\n                    results[region_name] = region_results\n\n                    continue\n\n                # Calculate statistics\n                max_value = np.max(field_values)\n                min_value = np.min(field_values)\n\n                whole_brain_positive_mask = field_arr &gt; 0\n                gm_values = field_arr[whole_brain_positive_mask]\n                metrics = calculate_roi_metrics(\n                    field_values,\n                    np.ones(len(field_values), dtype=float),\n                    ti_field_gm=gm_values,\n                    gm_volumes=np.ones(len(gm_values), dtype=float),\n                )\n                mean_value = metrics[\"TImean_ROI\"]\n                focality = metrics.get(\"Focality\")\n                whole_brain_average = metrics.get(\"TImean_GM\")\n\n                # Log the whole brain average for debugging\n                if not self.quiet:\n                    self.logger.info(f\"Whole brain average (denominator for focality): {whole_brain_average:.6f}\")\n\n                # Create result dictionary for this region\n                region_results = {\n                    'mean_value': mean_value,\n                    'max_value': max_value,\n                    'min_value': min_value,\n                    'focality': focality,\n                    'voxels_in_roi': filtered_count  # Store the number of voxels\n                }\n\n                # Store in the overall results\n                results[region_name] = region_results\n\n                # Generate visualizations if requested\n                if visualize:\n                    # Create visualization NIfTI file directly in the region directory\n                    viz_file = self.visualizer._generate_region_visualization(\n                        atlas_img=atlas_img,\n                        atlas_arr=atlas_arr,\n                        field_arr=field_arr,\n                        region_id=region_id,\n                        region_name=region_name,\n                        output_dir=region_dir\n                    )\n\n                    # Generate focality histogram for this region\n                    if len(field_values) &gt; 0:\n                        try:\n                            if not self.quiet:\n                                self.logger.info(f\"Generating focality histogram for region: {region_name}\")\n                            # Get voxel dimensions from the field image\n                            field_img_tuple = self.load_brain_image(self.field_nifti)\n                            voxel_dims = field_img_tuple[0].header.get_zooms()[:3]\n\n                            # Filter out zero values from whole head data for histogram\n                            whole_head_positive_mask = field_arr &gt; 0\n                            whole_head_filtered = field_arr[whole_head_positive_mask]\n\n                            self.visualizer.generate_focality_histogram(\n                                whole_head_field_data=whole_head_filtered,\n                                roi_field_data=field_values,\n                                region_name=region_name,\n                                roi_field_value=mean_value,\n                                data_type='voxel',\n                                voxel_dims=voxel_dims\n                            )\n                        except Exception as e:\n                            self.logger.warning(f\"Could not generate focality histogram for {region_name}: {str(e)}\")\n\n            except Exception as e:\n                self.logger.warning(f\"Warning: Failed to analyze region {region_name}: {str(e)}\")\n                results[region_name] = {\n                    'mean_value': None,\n                    'max_value': None,\n                    'min_value': None,\n                    'focality': None,\n                    'voxels_in_roi': 0\n                }\n\n        # Generate global visualization plots are disabled - only histograms are generated per region\n\n            # Generate and save summary CSV\n            self.visualizer.save_whole_head_results_to_csv(results, atlas_type, 'voxel')\n\n        return results\n\n    finally:\n        # Clean up all temporary data\n        try:\n            del atlas_arr\n            del field_arr\n            del region_info\n            if 'atlas_tuple' in locals():\n                del atlas_tuple\n            if 'field_tuple' in locals():\n                del field_tuple\n        except NameError:\n            # Variables may not be defined if cleanup failed earlier\n            pass\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.voxel_analyzer.VoxelAnalyzer.find_region","title":"find_region","text":"<pre><code>find_region(target_region, region_info)\n</code></pre> <p>Find region ID and name based on input.</p> <p>Parameters:</p> Name Type Description Default <code>target_region</code> <code>str or int</code> <p>Target region name or ID</p> required <code>region_info</code> <code>dict</code> <p>Dictionary with region information</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(region_id, region_name)</p> Source code in <code>tit/analyzer/voxel_analyzer.py</code> <pre><code>def find_region(self, target_region, region_info):\n    \"\"\"Find region ID and name based on input.\n\n    Parameters\n    ----------\n    target_region : str or int\n        Target region name or ID\n    region_info : dict\n        Dictionary with region information\n\n    Returns\n    -------\n    tuple\n        (region_id, region_name)\n    \"\"\"\n    # Check if target_region is an ID (int) or can be converted to one\n    try:\n        region_id = int(target_region)\n        # If it's an ID, get the name from region_info if available\n        if region_info and region_id in region_info:\n            region_name = region_info[region_id]['name']\n        else:\n            region_name = f\"Region {region_id}\"\n        return region_id, region_name\n    except ValueError:\n        # target_region is a string name, need to find the corresponding ID\n        if not region_info:\n            raise ValueError(\"Region labels are required to look up regions by name\")\n\n        # Search for the region name (case-insensitive)\n        target_lower = target_region.lower()\n        for region_id, info in region_info.items():\n            if target_lower in info['name'].lower():\n                return region_id, info['name']\n\n        # If we get here, region name was not found\n        raise ValueError(f\"Region name '{target_region}' not found in region labels\")\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.voxel_analyzer.VoxelAnalyzer.get_atlas_regions","title":"get_atlas_regions","text":"<pre><code>get_atlas_regions(atlas_file)\n</code></pre> <p>Extract region information from atlas file using FreeSurfer's mri_segstats.</p> <p>Parameters:</p> Name Type Description Default <code>atlas_file</code> <code>str</code> <p>Path to the atlas file (.nii or .mgz)</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary mapping region IDs to region information</p> Source code in <code>tit/analyzer/voxel_analyzer.py</code> <pre><code>def get_atlas_regions(self, atlas_file):\n    \"\"\"Extract region information from atlas file using FreeSurfer's mri_segstats.\n\n    Parameters\n    ----------\n    atlas_file : str\n        Path to the atlas file (.nii or .mgz)\n\n    Returns\n    -------\n    dict\n        Dictionary mapping region IDs to region information\n    \"\"\"\n    region_info = {}\n\n    # Get the atlas name from the file path\n    atlas_name = os.path.basename(atlas_file)\n    atlas_name = os.path.splitext(atlas_name)[0]  # Remove extension\n    if atlas_name.endswith('.nii'):  # Handle .nii.gz case\n        atlas_name = os.path.splitext(atlas_name)[0]\n\n    # Get the FreeSurfer subject directory (parent of mri directory)\n    mri_dir = os.path.dirname(atlas_file)\n    freesurfer_dir = os.path.dirname(mri_dir)\n\n    # Define the output file path in the mri directory\n    output_file = os.path.join(mri_dir, f\"{atlas_name}_labels.txt\")\n\n    # Create mri directory if it doesn't exist\n    os.makedirs(mri_dir, exist_ok=True)\n\n    # Function to generate the labels file\n    def generate_labels_file():\n        cmd = [\n            'mri_segstats',\n            '--seg', atlas_file,\n            '--excludeid', '0',  # Exclude background\n            '--ctab-default',    # Use default color table\n            '--sum', output_file\n        ]\n\n        try:\n            if not self.quiet:\n                self.logger.info(f\"Running: {' '.join(cmd)}\")\n            subprocess.run(cmd, check=True, capture_output=True)\n            return True\n        except subprocess.CalledProcessError as e:\n            self.logger.warning(f\"Warning: Could not extract region information using mri_segstats: {str(e)}\")\n            self.logger.debug(f\"Command output: {e.stdout.decode() if e.stdout else ''}\")\n            self.logger.debug(f\"Command error: {e.stderr.decode() if e.stderr else ''}\")\n            return False\n\n    # Function to parse the labels file\n    def parse_labels_file():\n        try:\n            with open(output_file, 'r') as f:\n                # Skip header lines (all lines starting with #)\n                in_header = True\n                for line in f:\n                    # Check if we've reached the end of the header\n                    if in_header and not line.startswith('#'):\n                        in_header = False\n\n                    # Process data lines (non-header)\n                    if not in_header and line.strip():\n                        parts = line.strip().split()\n\n                        # The format is:\n                        # Index SegId NVoxels Volume_mm3 StructName\n                        # We need at least 5 columns\n                        if len(parts) &gt;= 5:\n                            try:\n                                region_id = int(parts[1])  # SegId is the second column\n                                n_voxels = int(parts[2])   # NVoxels is the third column\n\n                                # Structure name can contain spaces, so join the remaining parts\n                                region_name = ' '.join(parts[4:])\n\n                                # Generate a random color based on region_id for visualization\n                                # This creates a consistent color for each region\n                                import random\n                                random.seed(region_id)\n                                r = random.uniform(0.2, 0.8)\n                                g = random.uniform(0.2, 0.8)\n                                b = random.uniform(0.2, 0.8)\n\n                                region_info[region_id] = {\n                                    'name': region_name,\n                                    'voxel_count': n_voxels,\n                                    'color': (r, g, b)\n                                }\n                            except (ValueError, IndexError) as e:\n                                self.logger.warning(f\"Warning: Could not parse line: {line.strip()}\")\n                                self.logger.debug(f\"Error: {str(e)}\")\n\n            return len(region_info) &gt; 0\n        except Exception as e:\n            self.logger.error(f\"Error reading labels file: {str(e)}\")\n            return False\n\n    # Try to use existing file first\n    if os.path.exists(output_file):\n        if not self.quiet:\n            self.logger.info(f\"Found existing labels file: {output_file}\")\n        if parse_labels_file():\n            if not self.quiet:\n                self.logger.info(f\"Successfully parsed {len(region_info)} regions from existing file\")\n            return region_info\n        else:\n            if not self.quiet:\n                self.logger.info(\"Existing file is invalid or empty, regenerating...\")\n\n    # Generate new file if needed\n    if generate_labels_file():\n        if parse_labels_file():\n            if not self.quiet:\n                self.logger.info(f\"Successfully generated and parsed {len(region_info)} regions\")\n            return region_info\n\n    self.logger.warning(\"Warning: Could not get region information from atlas file\")\n    return region_info\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.voxel_analyzer.VoxelAnalyzer.get_grey_matter_statistics","title":"get_grey_matter_statistics","text":"<pre><code>get_grey_matter_statistics()\n</code></pre> <p>Calculate grey matter field statistics from the NIfTI data.</p> <p>Returns:     dict: Dictionary containing grey matter statistics (mean, max, min)</p> Source code in <code>tit/analyzer/voxel_analyzer.py</code> <pre><code>def get_grey_matter_statistics(self):\n    \"\"\"\n    Calculate grey matter field statistics from the NIfTI data.\n\n    Returns:\n        dict: Dictionary containing grey matter statistics (mean, max, min)\n    \"\"\"\n    if not self.quiet:\n        self.logger.info(\"Calculating grey matter field statistics...\")\n\n    try:\n        # Load the field data\n        img = nib.load(self.field_nifti)\n        field_data = img.get_fdata()\n\n        # Handle 4D field data (extract first volume if multiple volumes)\n        if len(field_data.shape) == 4:\n            if field_data.shape[3] == 1:\n                field_data = field_data[:,:,:,0]\n            else:\n                field_data = field_data[:,:,:,0]\n\n        # For voxel analysis, we assume the entire field is grey matter\n        # (since we're typically working with grey matter NIfTI files)\n        # Filter for positive values only (matching ROI analysis behavior)\n        positive_mask = field_data &gt; 0\n        field_data_positive = field_data[positive_mask]\n\n        # Check if we have any positive values\n        if len(field_data_positive) == 0:\n            self.logger.warning(\"No positive values found in grey matter data\")\n            return {'grey_mean': 0.0, 'grey_max': 0.0, 'grey_min': 0.0}\n\n        # Calculate statistics on positive values only\n        grey_mean = np.mean(field_data_positive)\n        grey_max = np.max(field_data_positive)\n        grey_min = np.min(field_data_positive)\n\n        if not self.quiet:\n            self.logger.info(f\"Grey matter statistics for field '{os.path.basename(self.field_nifti)}' (positive values only): \"\n                           f\"mean={grey_mean:.6f}, max={grey_max:.6f}, min={grey_min:.6f}\")\n\n        return {\n            'grey_mean': float(grey_mean),\n            'grey_max': float(grey_max),\n            'grey_min': float(grey_min)\n        }\n\n    except Exception as e:\n        self.logger.error(f\"Error calculating grey matter statistics: {str(e)}\")\n        return {'grey_mean': 0.0, 'grey_max': 0.0, 'grey_min': 0.0}\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.voxel_analyzer.VoxelAnalyzer.load_brain_image","title":"load_brain_image","text":"<pre><code>load_brain_image(file_path)\n</code></pre> <p>Load brain image data from file, handling different formats.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the image file</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(nibabel image object, numpy array of data)</p> Source code in <code>tit/analyzer/voxel_analyzer.py</code> <pre><code>def load_brain_image(self, file_path):\n    \"\"\"Load brain image data from file, handling different formats.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the image file\n\n    Returns\n    -------\n    tuple\n        (nibabel image object, numpy array of data)\n    \"\"\"\n    file_ext = Path(file_path).suffix.lower()\n\n    if file_ext == '.mgz':\n        # Try to use nibabel directly first\n        try:\n            img = nib.load(file_path)\n            data = img.get_fdata()\n            return img, data\n        except Exception as e:\n            self.logger.warning(f\"Could not load MGZ file directly: {str(e)}\")\n\n            # Try to convert MGZ to NIfTI using mri_convert\n            try:\n                # Create a temporary file for the converted image\n                with tempfile.NamedTemporaryFile(suffix='.nii.gz', delete=False) as temp:\n                    temp_path = temp.name\n\n                # Run mri_convert to convert MGZ to NIfTI\n                cmd = ['mri_convert', file_path, temp_path]\n                subprocess.run(cmd, check=True)\n\n                # Load the converted file\n                img = nib.load(temp_path)\n                data = img.get_fdata()\n\n                # Clean up\n                os.unlink(temp_path)\n\n                return img, data\n            except Exception as e2:\n                raise RuntimeError(f\"Failed to convert MGZ file: {str(e2)}\")\n    else:\n        # For NIfTI and other formats, use nibabel directly\n        img = nib.load(file_path)\n        data = img.get_fdata()\n        return img, data\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.voxel_analyzer.VoxelAnalyzer.resample_to_match","title":"resample_to_match","text":"<pre><code>resample_to_match(source_img, target_shape, target_affine, source_path=None)\n</code></pre> <p>Resample source image to match target dimensions and affine using FreeSurfer's mri_convert.</p> <p>If a resampled version already exists, it will be loaded instead of generating a new one.</p> <p>Parameters:</p> Name Type Description Default <code>source_img</code> <code>Nifti1Image</code> <p>Source image to resample</p> required <code>target_shape</code> <code>tuple</code> <p>Target shape (x, y, z) or (x, y, z, t)</p> required <code>target_affine</code> <code>ndarray</code> <p>Target affine transformation matrix</p> required <code>source_path</code> <code>str</code> <p>Path to the source file. If provided, will be used to generate a resampled file name.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(resampled nibabel image, resampled data array)</p> Source code in <code>tit/analyzer/voxel_analyzer.py</code> <pre><code>def resample_to_match(self, source_img, target_shape, target_affine, source_path=None):\n    \"\"\"Resample source image to match target dimensions and affine using FreeSurfer's mri_convert.\n\n    If a resampled version already exists, it will be loaded instead of generating a new one.\n\n    Parameters\n    ----------\n    source_img : nibabel.Nifti1Image\n        Source image to resample\n    target_shape : tuple\n        Target shape (x, y, z) or (x, y, z, t)\n    target_affine : numpy.ndarray\n        Target affine transformation matrix\n    source_path : str, optional\n        Path to the source file. If provided, will be used to generate a resampled file name.\n\n    Returns\n    -------\n    tuple\n        (resampled nibabel image, resampled data array)\n    \"\"\"\n    if not self.quiet:\n        self.logger.info(f\"Resampling image from shape {source_img.shape} to {target_shape}\")\n\n    # If source_path is provided, try to find an existing resampled file\n    if source_path:\n        resampled_path = self._get_resampled_atlas_filename(source_path, target_shape)\n        if os.path.exists(resampled_path):\n            if not self.quiet:\n                self.logger.info(f\"Found existing resampled atlas: {resampled_path}\")\n            try:\n                resampled_img = nib.load(resampled_path)\n                resampled_data = resampled_img.get_fdata()\n\n                # Check if the resampled image has the correct shape\n                if resampled_data.shape[:3] == target_shape[:3]:\n                    if not self.quiet:\n                        self.logger.info(\"Loaded previously resampled atlas\")\n\n                    # If target is 4D but resampled is 3D, reshape it to match\n                    is_target_4d = len(target_shape) == 4\n                    if is_target_4d and len(resampled_data.shape) == 3:\n                        if not self.quiet:\n                            self.logger.info(f\"Reshaping 3D data {resampled_data.shape} to match 4D target {target_shape}\")\n                        # Add a dimension to match the 4D target shape\n                        resampled_data = np.expand_dims(resampled_data, axis=3)\n\n                        # Create a new 4D NIfTI image\n                        new_header = resampled_img.header.copy()\n                        new_header.set_data_shape(resampled_data.shape)\n                        resampled_img = nib.Nifti1Image(resampled_data, resampled_img.affine, header=new_header)\n\n                    return resampled_img, resampled_data\n                else:\n                    self.logger.warning(f\"Existing resampled atlas has wrong shape: {resampled_data.shape[:3]} vs expected {target_shape[:3]}\")\n            except Exception as e:\n                self.logger.error(f\"Error loading existing resampled atlas: {str(e)}\")\n                if not self.quiet:\n                    self.logger.info(\"Will generate a new one\")\n\n    if not self.quiet:\n        self.logger.info(\"Generating new resampled atlas...\")\n\n    # If target shape is 4D but source is 3D, we need to handle this specially\n    is_target_4d = len(target_shape) == 4\n    spatial_shape = target_shape[:3]  # Extract just the spatial dimensions\n\n    # Create a temporary file for the target template\n    with tempfile.NamedTemporaryFile(suffix='.nii.gz', delete=False) as temp_template:\n        template_path = temp_template.name\n\n    # Create a temporary file for the resampled output\n    with tempfile.NamedTemporaryFile(suffix='.nii.gz', delete=False) as temp_output:\n        output_path = temp_output.name\n\n    try:\n        # Save the source image to a temporary file if not provided\n        if source_path and os.path.exists(source_path):\n            # Use the provided source path\n            temp_source_created = False\n        else:\n            # Save the source image to a temporary file\n            with tempfile.NamedTemporaryFile(suffix='.nii.gz', delete=False) as temp_source:\n                source_path = temp_source.name\n                nib.save(source_img, source_path)\n                temp_source_created = True\n\n        # Create a template image with target dimensions (3D only)\n        template_img = nib.Nifti1Image(np.zeros(spatial_shape), target_affine)\n        nib.save(template_img, template_path)\n\n        # Run mri_convert to resample the image\n        cmd = [\n            'mri_convert',\n            '--reslice_like', template_path,  # Use template for resampling\n            source_path,                      # Source image\n            output_path                       # Output image\n        ]\n\n        if not self.quiet:\n            self.logger.info(f\"Running: {' '.join(cmd)}\")\n        result = subprocess.run(cmd, check=True, capture_output=True)\n\n        # Load the resampled image\n        resampled_img = nib.load(output_path)\n        resampled_data = resampled_img.get_fdata()\n\n        # If target is 4D but resampled is 3D, reshape it to match\n        if is_target_4d and len(resampled_data.shape) == 3:\n            if not self.quiet:\n                self.logger.info(f\"Reshaping 3D data {resampled_data.shape} to match 4D target {target_shape}\")\n            # Add a dimension to match the 4D target shape\n            resampled_data = np.expand_dims(resampled_data, axis=3)\n\n            # Create a new 4D NIfTI image\n            new_header = resampled_img.header.copy()\n            new_header.set_data_shape(resampled_data.shape)\n            resampled_img = nib.Nifti1Image(resampled_data, resampled_img.affine, header=new_header)\n\n        # Save the resampled atlas for future use if source_path was provided and not temporary\n        if source_path and not temp_source_created:\n            resampled_save_path = self._get_resampled_atlas_filename(source_path, target_shape)\n            if not self.quiet:\n                self.logger.info(f\"Saving resampled atlas for future use: {resampled_save_path}\")\n            nib.save(resampled_img, resampled_save_path)\n\n        if not self.quiet:\n            self.logger.info(\"Resampling complete\")\n        return resampled_img, resampled_data\n\n    finally:\n        # Clean up temporary files\n        for temp_file in [template_path, output_path]:\n            try:\n                os.unlink(temp_file)\n            except (OSError, FileNotFoundError):\n                # Best-effort cleanup - file may already be deleted\n                pass\n\n        # Also remove temporary source file if we created it\n        if 'temp_source_created' in locals() and temp_source_created and 'source_path' in locals():\n            try:\n                os.unlink(source_path)\n            except (OSError, FileNotFoundError):\n                # Best-effort cleanup - file may already be deleted\n                pass\n</code></pre>"},{"location":"reference/analyzer/#group-analyzer-titanalyzergroup_analyzer","title":"Group analyzer (<code>tit.analyzer.group_analyzer</code>)","text":"<p>Group Analyzer Script</p> <p>Each subject's analysis will be saved under:     derivatives/SimNIBS//Simulations//Analyses// <p>Within each subject's Analyses folder, we also capture:   - overlays &amp; plots (via --visualize)   - CSV summary  (handled by main_analyzer.py)   - centralized group analysis log file (all subjects logged together)</p>"},{"location":"reference/analyzer/#tit.analyzer.group_analyzer.build_main_analyzer_command","title":"build_main_analyzer_command","text":"<pre><code>build_main_analyzer_command(args, subject_args: List[str], subject_output_dir: str) -&gt; List[str]\n</code></pre> <p>Build the command to run main_analyzer.py for a single subject, matching the exact structure used in analyzer_tab.py.</p> Source code in <code>tit/analyzer/group_analyzer.py</code> <pre><code>def build_main_analyzer_command(\n    args,\n    subject_args: List[str],\n    subject_output_dir: str\n) -&gt; List[str]:\n    \"\"\"\n    Build the command to run main_analyzer.py for a single subject,\n    matching the exact structure used in analyzer_tab.py.\n    \"\"\"\n    global group_logger\n\n    subject_id = subject_args[0]\n    m2m_path = subject_args[1]\n    field_path = subject_args[2]\n\n    # Locate main_analyzer.py (assume it sits next to this script)\n    script_dir = Path(__file__).parent\n    main_analyzer_path = script_dir / \"main_analyzer.py\"\n\n    # Use `simnibs_python` as the interpreter (matching analyzer_tab.py)\n    cmd = [\"simnibs_python\", str(main_analyzer_path)]\n\n    # Add arguments in the same order as analyzer_tab.py\n    cmd += [\"--m2m_subject_path\", m2m_path]\n\n    # For mesh analysis, field_path is the montage name directly\n    # For voxel analysis, field_path is the actual file path\n    if args.space == 'mesh':\n        # field_path is already the montage name\n        cmd += [\"--montage_name\", field_path]\n    else:\n        cmd += [\"--field_path\", field_path]\n\n    cmd += [\"--space\", args.space]\n    cmd += [\"--analysis_type\", args.analysis_type]\n\n    # Analysis-type-specific flags\n    if args.analysis_type == 'spherical':\n        # Pass coordinates and coordinate space - let main_analyzer.py handle transformation\n        cmd += [\"--coordinates\"] + [str(c) for c in args.coordinates]\n        cmd += [\"--coordinate-space\", args.coordinate_space]\n        cmd += [\"--radius\", str(args.radius)]\n\n    else:  # cortical\n        # For cortical, add atlas info first\n        if args.space == 'mesh':\n            cmd += [\"--atlas_name\", args.atlas_name]\n        else:  # voxel\n            atlas_path = subject_args[3]\n            cmd += [\"--atlas_path\", atlas_path]\n\n        # Then add region or whole_head flag\n        if args.whole_head:\n            cmd += [\"--whole_head\"]\n        else:\n            cmd += [\"--region\", args.region]\n\n    # Field name is now hardcoded in main_analyzer.py, no need to pass it\n\n    # Add output directory\n    cmd += [\"--output_dir\", subject_output_dir]\n\n    # Always enable visualizations (matching the request)\n    cmd += [\"--visualize\"]\n\n    # Add group analysis log file path if available\n    if group_logger and hasattr(group_logger, 'handlers') and group_logger.handlers:\n        # Get the log file path from the first handler\n        for handler in group_logger.handlers:\n            if hasattr(handler, 'baseFilename'):\n                log_file_path = getattr(handler, 'baseFilename')\n                cmd += [\"--log_file\", log_file_path]\n                break\n\n    # Add quiet flag if requested\n    if args.quiet:\n        cmd += [\"--quiet\"]\n\n    return cmd\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.group_analyzer.collect_analysis_paths","title":"collect_analysis_paths","text":"<pre><code>collect_analysis_paths(successful_dirs: List[str]) -&gt; List[str]\n</code></pre> <p>Each entry in successful_dirs is exactly the folder we passed to --output_dir for main_analyzer.py. We check that it contains at least one .csv before returning it.</p> Source code in <code>tit/analyzer/group_analyzer.py</code> <pre><code>def collect_analysis_paths(successful_dirs: List[str]) -&gt; List[str]:\n    \"\"\"\n    Each entry in successful_dirs is exactly the folder we passed to --output_dir\n    for main_analyzer.py. We check that it contains at least one .csv before returning it.\n    \"\"\"\n    global group_logger\n\n    analysis_paths = []\n    for d in successful_dirs:\n        if os.path.isdir(d):\n            csv_list = [f for f in os.listdir(d) if f.lower().endswith(\".csv\")]\n            if csv_list:\n                analysis_paths.append(d)\n            else:\n                if group_logger:\n                    group_logger.debug(f\"No CSV found under {d}\")\n        else:\n            if group_logger:\n                group_logger.debug(f\"Expected analysis directory not found: {d}\")\n\n    return analysis_paths\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.group_analyzer.compute_subject_output_dir","title":"compute_subject_output_dir","text":"<pre><code>compute_subject_output_dir(args, subject_args: List[str]) -&gt; str\n</code></pre> <p>Create a consistent output directory structure for analysis results. Structure: output_dir/sub-{subject_id}/Simulations/{montage_name}/Analyses/{Mesh|Voxel}/{analysis_name}</p> Source code in <code>tit/analyzer/group_analyzer.py</code> <pre><code>def compute_subject_output_dir(args, subject_args: List[str]) -&gt; str:\n    \"\"\"\n    Create a consistent output directory structure for analysis results.\n    Structure: output_dir/sub-{subject_id}/Simulations/{montage_name}/Analyses/{Mesh|Voxel}/{analysis_name}\n    \"\"\"\n    subject_id = subject_args[0]\n\n    # For mesh analysis, field_path is the montage name\n    # For voxel analysis, we need to extract montage name from field path\n    if args.space == 'mesh':\n        montage_name = subject_args[2]  # field_path is montage name for mesh\n    else:\n        # For voxel analysis, extract montage name from path structure\n        field_path = subject_args[2]\n        try:\n            path_parts = Path(field_path).parts\n            sim_idx = path_parts.index('Simulations')\n            montage_name = path_parts[sim_idx + 1]\n        except (ValueError, IndexError):\n            # If we can't parse the path, use a default montage name\n            montage_name = 'unknown_montage'\n\n    # Build consistent directory structure\n    space_dir = 'Mesh' if args.space == 'mesh' else 'Voxel'\n    base_dir = os.path.join(args.output_dir, f'sub-{subject_id}', 'Simulations', montage_name, 'Analyses', space_dir)\n\n    # Create analysis-specific directory name\n    if args.analysis_type == 'spherical':\n        coord_space_suffix = f\"_{args.coordinate_space}\"\n        coords = [float(c) for c in args.coordinates]\n        analysis_name = f\"sphere_x{coords[0]:.2f}_y{coords[1]:.2f}_z{coords[2]:.2f}_r{args.radius}{coord_space_suffix}\"\n    else:  # cortical\n        if args.whole_head:\n            if args.space == 'mesh':\n                analysis_name = f\"whole_head_{args.atlas_name}\"\n            else:\n                atlas_path = subject_args[3]\n                atlas_name = os.path.basename(atlas_path).split('.')[0]\n                analysis_name = f\"whole_head_{atlas_name}\"\n        else:\n            analysis_name = f\"region_{args.region}\"\n\n    output_dir = os.path.join(base_dir, analysis_name)\n    os.makedirs(output_dir, exist_ok=True)\n    return output_dir\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.group_analyzer.create_group_output_directory","title":"create_group_output_directory","text":"<pre><code>create_group_output_directory(first_subject_path: str) -&gt; str\n</code></pre> <p>Create centralized group analysis output directory.</p> <p>Args:     first_subject_path (str): Path from the first subject to extract project name</p> <p>Returns:     str: Path to the created group analysis directory</p> Source code in <code>tit/analyzer/group_analyzer.py</code> <pre><code>def create_group_output_directory(first_subject_path: str) -&gt; str:\n    \"\"\"\n    Create centralized group analysis output directory.\n\n    Args:\n        first_subject_path (str): Path from the first subject to extract project name\n\n    Returns:\n        str: Path to the created group analysis directory\n    \"\"\"\n    global group_logger\n\n    # Extract project name from the first subject's path\n    project_name = _extract_project_name(first_subject_path)\n\n    # No longer create centralized group analysis directory under SimNIBS\n    # Individual subject analyses are stored in their respective subject directories\n    # Group comparisons will be stored in the user-specified output directory\n\n    if group_logger:\n        group_logger.debug(f\"Using project: {project_name}\")\n\n    # Return a placeholder path - not actually used since we don't create central directories\n    return f\"/mnt/{project_name}/derivatives/SimNIBS\"\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.group_analyzer.determine_group_subfolder_name","title":"determine_group_subfolder_name","text":"<pre><code>determine_group_subfolder_name(args, first_subject_args: List[str]) -&gt; str\n</code></pre> <p>Determine the group subfolder name in format: {montage}_{roi}</p> <p>Args:     args: Command line arguments     first_subject_args: First subject's arguments to extract montage info</p> <p>Returns:     str: Subfolder name like \"montage_roi\"</p> Source code in <code>tit/analyzer/group_analyzer.py</code> <pre><code>def determine_group_subfolder_name(args, first_subject_args: List[str]) -&gt; str:\n    \"\"\"\n    Determine the group subfolder name in format: {montage}_{roi}\n\n    Args:\n        args: Command line arguments\n        first_subject_args: First subject's arguments to extract montage info\n\n    Returns:\n        str: Subfolder name like \"montage_roi\"\n    \"\"\"\n    # Extract montage name from field path\n    field_path = first_subject_args[2]\n    path_parts = Path(field_path).parts\n\n    try:\n        # Locate \"Simulations\" in the file path\n        sim_idx = path_parts.index('Simulations')\n        # Montage name is the folder immediately after \"Simulations\"\n        montage_name = path_parts[sim_idx + 1]\n    except (ValueError, IndexError):\n        # If \"Simulations\" not found in path, try to extract from filename\n        field_filename = Path(field_path).stem\n        if field_filename.endswith('_TINormal'):\n            montage_name = field_filename.replace('_TINormal', 'Normal')\n        elif field_filename.endswith('_TI_Normal'):\n            montage_name = field_filename.replace('_TI_Normal', '_Normal')\n        elif field_filename.endswith('_TI'):\n            montage_name = field_filename.replace('_TI', '')\n        else:\n            montage_name = field_filename\n\n    # Determine ROI description\n    if args.analysis_type == 'spherical':\n        roi_desc = f\"sphere_r{args.radius}\"\n    else:  # cortical\n        if args.whole_head:\n            if args.space == 'mesh':\n                roi_desc = f\"whole_head_{args.atlas_name}\"\n            else:  # voxel\n                atlas_path = first_subject_args[3] if len(first_subject_args) &gt; 3 else \"unknown_atlas\"\n                atlas_name = os.path.basename(atlas_path).split('.')[0] if atlas_path != \"unknown_atlas\" else \"unknown_atlas\"\n                roi_desc = f\"whole_head_{atlas_name}\"\n        else:\n            roi_desc = args.region\n\n    return f\"{montage_name}_{roi_desc}\"\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.group_analyzer.format_duration","title":"format_duration","text":"<pre><code>format_duration(total_seconds)\n</code></pre> <p>Format duration in human-readable format.</p> Source code in <code>tit/analyzer/group_analyzer.py</code> <pre><code>def format_duration(total_seconds):\n    \"\"\"Format duration in human-readable format.\"\"\"\n    total_seconds = int(total_seconds)\n    hours = total_seconds // 3600\n    minutes = (total_seconds % 3600) // 60\n    seconds = total_seconds % 60\n\n    if hours &gt; 0:\n        return f\"{hours}h {minutes}m {seconds}s\"\n    elif minutes &gt; 0:\n        return f\"{minutes}m {seconds}s\"\n    else:\n        return f\"{seconds}s\"\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.group_analyzer.log_group_analysis_complete","title":"log_group_analysis_complete","text":"<pre><code>log_group_analysis_complete(num_successful, num_total, output_path='')\n</code></pre> <p>Log the completion of group analysis.</p> Source code in <code>tit/analyzer/group_analyzer.py</code> <pre><code>def log_group_analysis_complete(num_successful, num_total, output_path=\"\"):\n    \"\"\"Log the completion of group analysis.\"\"\"\n    global _group_start_time, SUMMARY_MODE\n\n    if _group_start_time is not None:\n        total_duration = time.time() - _group_start_time\n        duration_str = format_duration(total_duration)\n\n        if SUMMARY_MODE:\n            if num_successful == num_total:\n                print(f\"\u2514\u2500 Group analysis completed ({num_successful}/{num_total} subjects successful, Total: {duration_str})\")\n            else:\n                print(f\"\u2514\u2500 Group analysis completed with failures ({num_successful}/{num_total} subjects successful, Total: {duration_str})\")\n            if output_path:\n                # Show relative path from /mnt/ for cleaner display\n                display_path = output_path.replace('/mnt/', '') if output_path.startswith('/mnt/') else output_path\n                print(f\"   Group results saved to: {display_path}\")\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.group_analyzer.log_group_analysis_start","title":"log_group_analysis_start","text":"<pre><code>log_group_analysis_start(num_subjects, analysis_description)\n</code></pre> <p>Log the start of group analysis.</p> Source code in <code>tit/analyzer/group_analyzer.py</code> <pre><code>def log_group_analysis_start(num_subjects, analysis_description):\n    \"\"\"Log the start of group analysis.\"\"\"\n    global _group_start_time, SUMMARY_MODE\n    _group_start_time = time.time()\n\n    if SUMMARY_MODE:\n        print(f\"\\nBeginning group analysis for {num_subjects} subjects ({analysis_description})\")\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.group_analyzer.log_group_subject_status","title":"log_group_subject_status","text":"<pre><code>log_group_subject_status(subject_id, status, duration_str, error_msg='')\n</code></pre> <p>Log the status of a subject in group analysis.</p> Source code in <code>tit/analyzer/group_analyzer.py</code> <pre><code>def log_group_subject_status(subject_id, status, duration_str, error_msg=\"\"):\n    \"\"\"Log the status of a subject in group analysis.\"\"\"\n    global SUMMARY_MODE\n\n    if SUMMARY_MODE:\n        if status == \"complete\":\n            print(f\"\u251c\u2500 Subject {subject_id}: \u2713 Complete ({duration_str})\")\n        else:\n            print(f\"\u251c\u2500 Subject {subject_id}: \u2717 Failed ({duration_str}) - {error_msg}\")\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.group_analyzer.run_comprehensive_group_analysis","title":"run_comprehensive_group_analysis","text":"<pre><code>run_comprehensive_group_analysis(analysis_paths: List[str], project_name: Optional[str] = None) -&gt; str\n</code></pre> <p>Run comprehensive group analysis using all available comparison methods.</p> <p>Args:     analysis_paths (List[str]): List of paths to individual subject analysis directories     project_name (str, optional): Project name. If None, extracted from first path.</p> <p>Returns:     str: Path to the group analysis output directory</p> Source code in <code>tit/analyzer/group_analyzer.py</code> <pre><code>def run_comprehensive_group_analysis(analysis_paths: List[str], project_name: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Run comprehensive group analysis using all available comparison methods.\n\n    Args:\n        analysis_paths (List[str]): List of paths to individual subject analysis directories\n        project_name (str, optional): Project name. If None, extracted from first path.\n\n    Returns:\n        str: Path to the group analysis output directory\n    \"\"\"\n    global group_logger\n\n    if len(analysis_paths) == 0:\n        if group_logger:\n            group_logger.debug(\"No analysis paths provided for group comparison.\")\n        return \"\"\n\n    if group_logger:\n        group_logger.debug(f\"\\n=== Running comprehensive group analysis on {len(analysis_paths)} analyses ===\")\n\n    try:\n        # Use the comprehensive comparison function from compare_analyses.py\n        group_output_dir = run_all_group_comparisons(analysis_paths, project_name)\n        if group_logger:\n            group_logger.debug(f\"\u2714 Comprehensive group analysis completed successfully.\")\n            group_logger.debug(f\"  All results saved to: {group_output_dir}\")\n        return group_output_dir\n    except Exception as e:\n        if group_logger:\n            group_logger.error(f\"\u2716 Group analysis failed with error: {e}\")\n        return \"\"\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.group_analyzer.run_subject_analysis","title":"run_subject_analysis","text":"<pre><code>run_subject_analysis(args, subject_args: List[str]) -&gt; Tuple[bool, str]\n</code></pre> <p>Run analysis for a single subject and return (success, output_dir). All output is logged to the centralized group analysis log file.</p> Source code in <code>tit/analyzer/group_analyzer.py</code> <pre><code>def run_subject_analysis(args, subject_args: List[str]) -&gt; Tuple[bool, str]:\n    \"\"\"\n    Run analysis for a single subject and return (success, output_dir).\n    All output is logged to the centralized group analysis log file.\n    \"\"\"\n    global group_logger\n\n    subject_id = subject_args[0]\n    m2m_path = subject_args[1]\n    field_path = subject_args[2]\n\n    # Compute the target output folder for this subject\n    subject_output_dir = compute_subject_output_dir(args, subject_args)\n\n    # Build the command\n    cmd = build_main_analyzer_command(args, subject_args, subject_output_dir)\n\n    if group_logger:\n        group_logger.debug(f\"Starting analysis for subject: {subject_id}\")\n\n    # Track timing for subject analysis\n    import time\n    start_time = time.time()\n\n    # Run main_analyzer.py with real-time output streaming\n    if args.quiet:\n        # In summary mode, stream output in real-time to show task steps as they happen\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n                               text=True, bufsize=1, universal_newlines=True)\n\n        # Stream output in real-time\n        output_lines = []\n        while True:\n            line = proc.stdout.readline()\n            if not line and proc.poll() is not None:\n                break\n            if line:\n                line = line.strip()\n                if line:\n                    output_lines.append(line)\n                    # Display task steps in real-time\n                    if line.startswith('Beginning analysis for subject:'):\n                        # This is the subject start message, display it with proper indentation\n                        print(f\"  {line}\")\n                    elif line.startswith('\u251c\u2500 ') or line.startswith('\u2514\u2500 '):\n                        # This is a task step, display it with proper indentation\n                        clean_line = line[2:]  # Remove the tree symbols\n                        print(f\"  \u251c\u2500 {clean_line}\")\n                    elif 'Starting...' in line and 'Subject' in line:\n                        # Skip the \"Subject X: Starting...\" line as it's already shown\n                        continue\n                    elif '\u2713 Complete' in line or '\u2717 Failed' in line:\n                        # Skip completion lines as they're handled separately\n                        continue\n                    elif 'Analysis completed successfully' in line:\n                        # Skip the final completion message as it's handled separately\n                        continue\n\n        # Wait for process to complete\n        return_code = proc.wait()\n        stdout_output = '\\n'.join(output_lines)\n    else:\n        # In debug mode, capture output as before\n        proc = subprocess.run(cmd, capture_output=True, text=True)\n        return_code = proc.returncode\n        stdout_output = proc.stdout\n\n    end_time = time.time()\n    duration = int(end_time - start_time)\n    duration_str = format_duration(duration)\n\n    if return_code == 0:\n        if group_logger:\n            group_logger.debug(f\"Subject {subject_id} analysis completed successfully\")\n\n        # Log subject status for summary\n        if args.quiet:\n            log_group_subject_status(subject_id, \"complete\", duration_str)\n\n        return True, subject_output_dir\n    else:\n        if group_logger:\n            group_logger.error(f\"Subject {subject_id} analysis failed\")\n            # Log any additional error output that might not have been captured by main_analyzer.py\n            if stdout_output.strip():\n                group_logger.error(f\"stdout: {stdout_output.strip()}\")\n            # For streaming mode, stderr is redirected to stdout, so no separate stderr handling needed\n\n        # Extract meaningful error message for summary\n        error_msg = \"\"\n        if stdout_output.strip():\n            # Look for error patterns in stdout\n            stdout_lines = stdout_output.strip().split('\\n')\n            for line in stdout_lines:\n                if any(keyword in line.lower() for keyword in ['error:', 'failed', 'exception', 'critical']):\n                    error_msg = line.strip()\n                    break\n\n        # Log subject status for summary\n        if args.quiet:\n            log_group_subject_status(subject_id, \"failed\", duration_str, error_msg)\n\n        return False, \"\"\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.group_analyzer.setup_parser","title":"setup_parser","text":"<pre><code>setup_parser()\n</code></pre> <p>Set up command line argument parser.</p> Source code in <code>tit/analyzer/group_analyzer.py</code> <pre><code>def setup_parser():\n    \"\"\"Set up command line argument parser.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Run analysis across multiple subjects and compare results\")\n\n    # Analysis parameters (same as main_analyzer.py)\n    parser.add_argument(\"--space\", required=True, choices=['mesh', 'voxel'],\n                        help=\"Analysis space: mesh or voxel\")\n    parser.add_argument(\"--analysis_type\", required=True, choices=['spherical', 'cortical'],\n                        help=\"Type of analysis to perform\")\n\n    # Analysis\u2010specific parameters\n    parser.add_argument(\"--atlas_name\",\n                        help=\"Atlas name for mesh-based cortical analysis (e.g., DK40)\")\n    parser.add_argument(\"--coordinates\", nargs=3,\n                        help=\"x y z coordinates for spherical analysis\")\n    parser.add_argument(\"--coordinate-space\", choices=['MNI', 'subject'],\n                        help=\"Coordinate space of the input coordinates (MNI or subject) - required for spherical analysis\")\n    parser.add_argument(\"--radius\", type=float,\n                        help=\"Radius for spherical analysis\")\n    parser.add_argument(\"--region\",\n                        help=\"Region name for cortical analysis\")\n    parser.add_argument(\"--whole_head\", action=\"store_true\",\n                        help=\"Analyze the whole head instead of a specific region\")\n\n    # Subject specification; for voxel\u2010cortical we expect an extra atlas_path\n    parser.add_argument(\"--subject\", action=\"append\", nargs='+', metavar=\"ARG\",\n                        help=\"Subject specification: subject_id m2m_path field_path [atlas_path] \"\n                             \"(atlas_path required for voxel-based cortical analysis)\")\n\n    # Output and comparison options\n    parser.add_argument(\"--output_dir\", required=True,\n                        help=\"Directory for legacy group analysis outputs (comprehensive results go to centralized location)\")\n    parser.add_argument(\"--quiet\", action=\"store_true\",\n                        help=\"Enable summary mode for clean output (non-debug mode)\")\n    parser.add_argument(\"--no-compare\", action=\"store_true\",\n                        help=\"Skip comparison analysis after all subjects are processed (comparison runs by default)\")\n    parser.add_argument(\"--visualize\", action=\"store_true\",\n                        help=\"Generate visualization outputs for each analysis\")\n\n    return parser\n</code></pre>"},{"location":"reference/analyzer/#tit.analyzer.group_analyzer.validate_args","title":"validate_args","text":"<pre><code>validate_args(args)\n</code></pre> <p>Validate command line arguments.</p> Source code in <code>tit/analyzer/group_analyzer.py</code> <pre><code>def validate_args(args):\n    \"\"\"Validate command line arguments.\"\"\"\n    if not args.subject or len(args.subject) == 0:\n        raise ValueError(\"At least one --subject must be specified\")\n\n    # Validate analysis\u2010specific arguments\n    if args.analysis_type == 'spherical':\n        if not args.coordinates:\n            raise ValueError(\"Coordinates are required for spherical analysis\")\n        if args.radius is None:\n            raise ValueError(\"Radius is required for spherical analysis\")\n        if not args.coordinate_space:\n            raise ValueError(\"Coordinate space (--coordinate-space) is required for spherical analysis\")\n\n        # Spherical: each subject must supply exactly (id, m2m_path, field_path)\n        for i, subject_args in enumerate(args.subject):\n            if len(subject_args) != 3:\n                raise ValueError(\n                    f\"Subject {i+1}: Spherical analysis requires exactly 3 arguments: \"\n                    \"subject_id m2m_path field_path\"\n                )\n\n    else:  # cortical\n        if args.space == 'mesh':\n            if not args.atlas_name:\n                raise ValueError(\"Atlas name is required for mesh-based cortical analysis\")\n            # Mesh\u2010cortical: each subject still has (id, m2m_path, field_path)\n            for i, subject_args in enumerate(args.subject):\n                if len(subject_args) != 3:\n                    raise ValueError(\n                        f\"Subject {i+1}: Mesh cortical analysis requires exactly 3 arguments: \"\n                        \"subject_id m2m_path field_path\"\n                    )\n        else:\n            # Voxel\u2010cortical: each subject must supply (id, m2m_path, field_path, atlas_path)\n            for i, subject_args in enumerate(args.subject):\n                if len(subject_args) != 4:\n                    raise ValueError(\n                        f\"Subject {i+1}: Voxel cortical analysis requires exactly 4 arguments: \"\n                        \"subject_id m2m_path field_path atlas_path\"\n                    )\n\n        if not args.whole_head and not args.region:\n            raise ValueError(\"Either --whole_head flag or --region must be specified for cortical analysis\")\n\n    # Validate space\u2010specific: mesh needs a field_name (now hardcoded)\n    # Field name is now hardcoded to \"TI_max\", no validation needed\n\n    # Validate existence of provided paths\n    for subject_args in args.subject:\n        subject_id = subject_args[0]\n        m2m_path = subject_args[1]\n        field_path = subject_args[2]\n\n        if not os.path.isdir(m2m_path):\n            raise ValueError(f\"Subject {subject_id} m2m directory not found: {m2m_path}\")\n        # For mesh analysis, field_path is actually a montage name, not a file path\n        # Skip file existence check for mesh analysis\n        if args.space != 'mesh' and not os.path.exists(field_path):\n            raise ValueError(f\"Subject {subject_id} field file not found: {field_path}\")\n\n        if args.space == 'voxel' and args.analysis_type == 'cortical':\n            atlas_path = subject_args[3]\n            if not os.path.exists(atlas_path):\n                raise ValueError(f\"Subject {subject_id} atlas file not found: {atlas_path}\")\n</code></pre>"},{"location":"reference/core/","title":"Core (<code>tit.core</code>)","text":""},{"location":"reference/core/#tit.core.PathManager","title":"PathManager","text":"<pre><code>PathManager(project_dir: Optional[str] = None)\n</code></pre> <p>Centralized path management for TI-Toolbox.</p> <p>This class provides consistent path resolution across all components: - Project directory detection - Subject listing and validation - Common path patterns (derivatives, SimNIBS, m2m, etc.) - BIDS-compliant directory structure handling</p> <p>Usage:     pm = PathManager()     pm.project_dir = \"/path/to/project\"  # Explicit set     print(pm.project_dir)                # Get current     print(pm.project_dir_name)           # Just the name</p> <p>Initialize the path manager.</p> <p>Args:     project_dir: Optional explicit project directory. If not provided,                 auto-detection from environment variables is attempted                 on first access of project_dir property.</p> Source code in <code>tit/core/paths.py</code> <pre><code>def __init__(self, project_dir: Optional[str] = None):\n    \"\"\"\n    Initialize the path manager.\n\n    Args:\n        project_dir: Optional explicit project directory. If not provided,\n                    auto-detection from environment variables is attempted\n                    on first access of project_dir property.\n    \"\"\"\n    self._project_dir: Optional[str] = None\n    _ensure_compiled_templates(self.__class__)\n\n    if project_dir:\n        self.project_dir = project_dir  # Use setter for validation\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.project_dir","title":"project_dir  <code>property</code> <code>writable</code>","text":"<pre><code>project_dir: Optional[str]\n</code></pre> <p>Get/set the project directory path.</p> <p>Auto-detects from environment on first access if not set. Setting validates the path exists.</p> <p>Usage:     pm.project_dir = \"/path/to/project\"  # set     path = pm.project_dir                # get</p>"},{"location":"reference/core/#tit.core.PathManager.project_dir_name","title":"project_dir_name  <code>property</code>","text":"<pre><code>project_dir_name: Optional[str]\n</code></pre> <p>Get the project directory name (basename of project_dir).</p>"},{"location":"reference/core/#tit.core.PathManager.analysis_space_dir_name","title":"analysis_space_dir_name  <code>staticmethod</code>","text":"<pre><code>analysis_space_dir_name(space: str) -&gt; str\n</code></pre> <p>Map analyzer space ('mesh'|'voxel') to folder name ('Mesh'|'Voxel').</p> Source code in <code>tit/core/paths.py</code> <pre><code>@staticmethod\ndef analysis_space_dir_name(space: str) -&gt; str:\n    \"\"\"Map analyzer space ('mesh'|'voxel') to folder name ('Mesh'|'Voxel').\"\"\"\n    return \"Mesh\" if str(space).lower() == \"mesh\" else \"Voxel\"\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.cortical_analysis_name","title":"cortical_analysis_name  <code>classmethod</code>","text":"<pre><code>cortical_analysis_name(*, whole_head: bool, region: Optional[str], atlas_name: Optional[str] = None, atlas_path: Optional[str] = None) -&gt; str\n</code></pre> <p>Match GUI/CLI naming for cortical analysis folders.</p> Source code in <code>tit/core/paths.py</code> <pre><code>@classmethod\ndef cortical_analysis_name(\n    cls,\n    *,\n    whole_head: bool,\n    region: Optional[str],\n    atlas_name: Optional[str] = None,\n    atlas_path: Optional[str] = None,\n) -&gt; str:\n    \"\"\"Match GUI/CLI naming for cortical analysis folders.\"\"\"\n    atlas_clean = cls._atlas_name_clean(atlas_name or atlas_path or \"unknown_atlas\")\n    if whole_head:\n        return f\"whole_head_{atlas_clean}\"\n    region_val = str(region or \"\").strip()\n    if not region_val:\n        raise ValueError(\"region is required for cortical analysis unless whole_head=True\")\n    return f\"region_{region_val}_{atlas_clean}\"\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.ensure_dir","title":"ensure_dir","text":"<pre><code>ensure_dir(key: str, /, **kwargs) -&gt; str\n</code></pre> <p>Resolve a directory path and create it (parents=True, exist_ok=True).</p> Source code in <code>tit/core/paths.py</code> <pre><code>def ensure_dir(self, key: str, /, **kwargs) -&gt; str:\n    \"\"\"Resolve a directory path and create it (parents=True, exist_ok=True).\"\"\"\n    p = self.path(key, **kwargs)\n    os.makedirs(p, exist_ok=True)\n    return p\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.exists","title":"exists","text":"<pre><code>exists(key: str, /, **kwargs) -&gt; bool\n</code></pre> <p>Check if a path exists.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Path template key</p> required <code>**kwargs</code> <p>Template entities</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if path can be resolved and exists on filesystem</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pm = PathManager()\n&gt;&gt;&gt; if pm.exists(\"m2m\", subject_id=\"001\"):\n...     print(\"m2m directory exists\")\n</code></pre> Source code in <code>tit/core/paths.py</code> <pre><code>def exists(self, key: str, /, **kwargs) -&gt; bool:\n    \"\"\"\n    Check if a path exists.\n\n    Parameters\n    ----------\n    key : str\n        Path template key\n    **kwargs\n        Template entities\n\n    Returns\n    -------\n    bool\n        True if path can be resolved and exists on filesystem\n\n    Examples\n    --------\n    &gt;&gt;&gt; pm = PathManager()\n    &gt;&gt;&gt; if pm.exists(\"m2m\", subject_id=\"001\"):\n    ...     print(\"m2m directory exists\")\n    \"\"\"\n    p = self.path_optional(key, **kwargs)\n    return bool(p and os.path.exists(p))\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.get_analysis_output_dir","title":"get_analysis_output_dir","text":"<pre><code>get_analysis_output_dir(*, subject_id: str, simulation_name: str, space: str, analysis_type: str, coordinates: Optional[List[float]] = None, radius: Optional[float] = None, coordinate_space: str = 'subject', whole_head: bool = False, region: Optional[str] = None, atlas_name: Optional[str] = None, atlas_path: Optional[str] = None) -&gt; Optional[str]\n</code></pre> <p>Centralized analysis output directory used by GUI/CLI. Returns the folder but does NOT create it.</p> Source code in <code>tit/core/paths.py</code> <pre><code>def get_analysis_output_dir(\n    self,\n    *,\n    subject_id: str,\n    simulation_name: str,\n    space: str,\n    analysis_type: str,\n    coordinates: Optional[List[float]] = None,\n    radius: Optional[float] = None,\n    coordinate_space: str = \"subject\",\n    whole_head: bool = False,\n    region: Optional[str] = None,\n    atlas_name: Optional[str] = None,\n    atlas_path: Optional[str] = None,\n) -&gt; Optional[str]:\n    \"\"\"\n    Centralized analysis output directory used by GUI/CLI.\n    Returns the folder but does NOT create it.\n    \"\"\"\n    base = self.get_analysis_space_dir(subject_id, simulation_name, space)\n    if not base:\n        return None\n\n    at = str(analysis_type).lower()\n    if at == \"spherical\":\n        if not coordinates or len(coordinates) != 3 or radius is None:\n            raise ValueError(\"coordinates(3) and radius are required for spherical analysis output path\")\n        name = self.spherical_analysis_name(float(coordinates[0]), float(coordinates[1]), float(coordinates[2]), float(radius), coordinate_space)\n    else:\n        name = self.cortical_analysis_name(whole_head=bool(whole_head), region=region, atlas_name=atlas_name, atlas_path=atlas_path)\n\n    return os.path.join(base, name)\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.get_analysis_space_dir","title":"get_analysis_space_dir","text":"<pre><code>get_analysis_space_dir(subject_id: str, simulation_name: str, space: str) -&gt; Optional[str]\n</code></pre> <p>Get base analysis dir: .../Simulations//Analyses/. Source code in <code>tit/core/paths.py</code> <pre><code>def get_analysis_space_dir(self, subject_id: str, simulation_name: str, space: str) -&gt; Optional[str]:\n    \"\"\"Get base analysis dir: .../Simulations/&lt;sim&gt;/Analyses/&lt;Mesh|Voxel&gt;.\"\"\"\n    sim_dir = self.path_optional(\"simulation\", subject_id=subject_id, simulation_name=simulation_name)\n    if not sim_dir:\n        return None\n    return os.path.join(sim_dir, const.DIR_ANALYSIS, self.analysis_space_dir_name(space))\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.get_derivatives_dir","title":"get_derivatives_dir","text":"<pre><code>get_derivatives_dir() -&gt; Optional[str]\n</code></pre> <p>Get the derivatives directory path.</p> Source code in <code>tit/core/paths.py</code> <pre><code>def get_derivatives_dir(self) -&gt; Optional[str]:\n    \"\"\"Get the derivatives directory path.\"\"\"\n    return self.path_optional(\"derivatives\")\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.is_dir","title":"is_dir","text":"<pre><code>is_dir(key: str, /, **kwargs) -&gt; bool\n</code></pre> <p>Check if a path exists and is a directory.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Path template key</p> required <code>**kwargs</code> <p>Template entities</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if path can be resolved and is an existing directory</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pm = PathManager()\n&gt;&gt;&gt; if pm.is_dir(\"simulations\", subject_id=\"001\"):\n...     simulations = pm.list_simulations(\"001\")\n</code></pre> Source code in <code>tit/core/paths.py</code> <pre><code>def is_dir(self, key: str, /, **kwargs) -&gt; bool:\n    \"\"\"\n    Check if a path exists and is a directory.\n\n    Parameters\n    ----------\n    key : str\n        Path template key\n    **kwargs\n        Template entities\n\n    Returns\n    -------\n    bool\n        True if path can be resolved and is an existing directory\n\n    Examples\n    --------\n    &gt;&gt;&gt; pm = PathManager()\n    &gt;&gt;&gt; if pm.is_dir(\"simulations\", subject_id=\"001\"):\n    ...     simulations = pm.list_simulations(\"001\")\n    \"\"\"\n    p = self.path_optional(key, **kwargs)\n    return bool(p and os.path.isdir(p))\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.is_file","title":"is_file","text":"<pre><code>is_file(key: str, /, **kwargs) -&gt; bool\n</code></pre> <p>Check if a path exists and is a file.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Path template key</p> required <code>**kwargs</code> <p>Template entities</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if path can be resolved and is an existing file</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pm = PathManager()\n&gt;&gt;&gt; if pm.is_file(\"ti_mesh\", subject_id=\"001\", simulation_name=\"montage1\"):\n...     print(\"TI mesh file exists\")\n</code></pre> Source code in <code>tit/core/paths.py</code> <pre><code>def is_file(self, key: str, /, **kwargs) -&gt; bool:\n    \"\"\"\n    Check if a path exists and is a file.\n\n    Parameters\n    ----------\n    key : str\n        Path template key\n    **kwargs\n        Template entities\n\n    Returns\n    -------\n    bool\n        True if path can be resolved and is an existing file\n\n    Examples\n    --------\n    &gt;&gt;&gt; pm = PathManager()\n    &gt;&gt;&gt; if pm.is_file(\"ti_mesh\", subject_id=\"001\", simulation_name=\"montage1\"):\n    ...     print(\"TI mesh file exists\")\n    \"\"\"\n    p = self.path_optional(key, **kwargs)\n    return bool(p and os.path.isfile(p))\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.list_eeg_caps","title":"list_eeg_caps","text":"<pre><code>list_eeg_caps(subject_id: str) -&gt; List[str]\n</code></pre> <p>List available EEG cap files for a subject.</p> <p>Args:     subject_id: Subject ID</p> <p>Returns:     List of EEG cap CSV filenames, sorted alphabetically</p> Source code in <code>tit/core/paths.py</code> <pre><code>def list_eeg_caps(self, subject_id: str) -&gt; List[str]:\n    \"\"\"\n    List available EEG cap files for a subject.\n\n    Args:\n        subject_id: Subject ID\n\n    Returns:\n        List of EEG cap CSV filenames, sorted alphabetically\n    \"\"\"\n    eeg_pos_dir = self.path_optional(\"eeg_positions\", subject_id=subject_id)\n    if not eeg_pos_dir or not os.path.isdir(eeg_pos_dir):\n        return []\n\n    caps = []\n    for file in os.listdir(eeg_pos_dir):\n        if file.endswith(const.EXT_CSV) and not file.startswith('.'):\n            caps.append(file)\n\n    caps.sort()\n    return caps\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.list_flex_search_runs","title":"list_flex_search_runs","text":"<pre><code>list_flex_search_runs(subject_id: str) -&gt; List[str]\n</code></pre> <p>List flex-search run folders that contain electrode_positions.json. Uses os.scandir for efficiency.</p> Source code in <code>tit/core/paths.py</code> <pre><code>def list_flex_search_runs(self, subject_id: str) -&gt; List[str]:\n    \"\"\"\n    List flex-search run folders that contain electrode_positions.json.\n    Uses os.scandir for efficiency.\n    \"\"\"\n    root = self.path_optional(\"flex_search\", subject_id=subject_id)\n    if not root or not os.path.isdir(root):\n        return []\n    out: List[str] = []\n    fname = \"electrode_positions.json\"\n    try:\n        with os.scandir(root) as it:\n            for entry in it:\n                if not entry.is_dir():\n                    continue\n                if entry.name.startswith(\".\"):\n                    continue\n                if os.path.isfile(os.path.join(entry.path, fname)):\n                    out.append(entry.name)\n    except OSError:\n        return []\n    out.sort()\n    return out\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.list_simulations","title":"list_simulations","text":"<pre><code>list_simulations(subject_id: str) -&gt; List[str]\n</code></pre> <p>List all simulations for a subject.</p> <p>Args:     subject_id: Subject ID</p> <p>Returns:     List of simulation names, sorted alphabetically</p> Source code in <code>tit/core/paths.py</code> <pre><code>def list_simulations(self, subject_id: str) -&gt; List[str]:\n    \"\"\"\n    List all simulations for a subject.\n\n    Args:\n        subject_id: Subject ID\n\n    Returns:\n        List of simulation names, sorted alphabetically\n    \"\"\"\n    sim_root = self.path_optional(\"simulations\", subject_id=subject_id)\n    if not sim_root or not os.path.isdir(sim_root):\n        return []\n\n    simulations: List[str] = []\n    try:\n        with os.scandir(sim_root) as it:\n            for entry in it:\n                if entry.is_dir() and not entry.name.startswith(\".\"):\n                    simulations.append(entry.name)\n    except OSError:\n        return []\n    simulations.sort()\n    return simulations\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.list_subjects","title":"list_subjects","text":"<pre><code>list_subjects() -&gt; List[str]\n</code></pre> <p>List all available subjects in the project.</p> <p>Returns:     List of subject IDs (without 'sub-' prefix), sorted naturally</p> Source code in <code>tit/core/paths.py</code> <pre><code>def list_subjects(self) -&gt; List[str]:\n    \"\"\"\n    List all available subjects in the project.\n\n    Returns:\n        List of subject IDs (without 'sub-' prefix), sorted naturally\n    \"\"\"\n    simnibs_dir = self.path_optional(\"simnibs\")\n    if not simnibs_dir or not os.path.isdir(simnibs_dir):\n        return []\n\n    subjects = []\n    for item in os.listdir(simnibs_dir):\n        if not item.startswith(const.PREFIX_SUBJECT):\n            continue\n        subject_id = item.replace(const.PREFIX_SUBJECT, \"\")\n        # Only list subjects that have an m2m folder (SimNIBS-ready).\n        if self.is_dir(\"m2m\", subject_id=subject_id):\n            subjects.append(subject_id)\n\n    # Sort subjects naturally (001, 002, 010, 100)\n    subjects.sort(key=lambda x: [int(c) if c.isdigit() else c.lower() \n                                 for c in re.split('([0-9]+)', x)])\n    return subjects\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.path","title":"path","text":"<pre><code>path(key: str, /, **kwargs) -&gt; str\n</code></pre> <p>Resolve a canonical path by key from predefined templates.</p> <p>This is the primary path resolution method. It provides fast, cached access to all standard TI-Toolbox paths using pre-compiled templates.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Path template key (e.g., 'm2m', 'simulation', 'ti_mesh')</p> required <code>**kwargs</code> <p>Required entities for the template (e.g., subject_id='001', simulation_name='montage1')</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Resolved absolute path</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If project_dir is not resolved</p> <code>KeyError</code> <p>If key is unknown</p> <code>ValueError</code> <p>If required template entities are missing</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pm = PathManager()\n&gt;&gt;&gt; m2m_path = pm.path(\"m2m\", subject_id=\"001\")\n&gt;&gt;&gt; sim_path = pm.path(\"simulation\", subject_id=\"001\", simulation_name=\"montage1\")\n&gt;&gt;&gt; mesh_path = pm.path(\"ti_mesh\", subject_id=\"001\", simulation_name=\"montage1\")\n</code></pre> Notes <ul> <li>Results are cached for performance (8192 entry LRU cache)</li> <li>All paths are resolved relative to project_dir</li> <li>Template entities are type-checked and missing entities raise ValueError</li> </ul> Source code in <code>tit/core/paths.py</code> <pre><code>def path(self, key: str, /, **kwargs) -&gt; str:\n    \"\"\"\n    Resolve a canonical path by key from predefined templates.\n\n    This is the primary path resolution method. It provides fast, cached access\n    to all standard TI-Toolbox paths using pre-compiled templates.\n\n    Parameters\n    ----------\n    key : str\n        Path template key (e.g., 'm2m', 'simulation', 'ti_mesh')\n    **kwargs\n        Required entities for the template (e.g., subject_id='001', simulation_name='montage1')\n\n    Returns\n    -------\n    str\n        Resolved absolute path\n\n    Raises\n    ------\n    RuntimeError\n        If project_dir is not resolved\n    KeyError\n        If key is unknown\n    ValueError\n        If required template entities are missing\n\n    Examples\n    --------\n    &gt;&gt;&gt; pm = PathManager()\n    &gt;&gt;&gt; m2m_path = pm.path(\"m2m\", subject_id=\"001\")\n    &gt;&gt;&gt; sim_path = pm.path(\"simulation\", subject_id=\"001\", simulation_name=\"montage1\")\n    &gt;&gt;&gt; mesh_path = pm.path(\"ti_mesh\", subject_id=\"001\", simulation_name=\"montage1\")\n\n    Notes\n    -----\n    - Results are cached for performance (8192 entry LRU cache)\n    - All paths are resolved relative to project_dir\n    - Template entities are type-checked and missing entities raise ValueError\n    \"\"\"\n    if not self.project_dir:\n        raise RuntimeError(\"Project directory not resolved. Set PROJECT_DIR_NAME or PROJECT_DIR in Docker.\")\n    tpl = _COMPILED_TEMPLATES.get(key)\n    if tpl is None:\n        raise KeyError(f\"Unknown path key: {key}\")\n    missing = [e for e in tpl.required_entities if e not in kwargs]\n    if missing:\n        raise ValueError(f\"Missing required path entities for {key!r}: {', '.join(missing)}\")\n    return _cached_render(self.project_dir, key, _freeze_kwargs(kwargs))\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.path_optional","title":"path_optional","text":"<pre><code>path_optional(key: str, /, **kwargs) -&gt; Optional[str]\n</code></pre> <p>Resolve a path without raising exceptions.</p> <p>Similar to path() but returns None instead of raising exceptions. Useful for checking if paths exist or can be resolved without error handling.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Path template key</p> required <code>**kwargs</code> <p>Template entities (e.g., subject_id='001')</p> <code>{}</code> <p>Returns:</p> Type Description <code>str or None</code> <p>Resolved path if successful, None otherwise</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pm = PathManager()\n&gt;&gt;&gt; m2m_path = pm.path_optional(\"m2m\", subject_id=\"001\")\n&gt;&gt;&gt; if m2m_path:\n...     print(f\"m2m exists at {m2m_path}\")\n</code></pre> Notes <p>Returns None if: - project_dir is not resolved - key is unknown - required entities are missing</p> Source code in <code>tit/core/paths.py</code> <pre><code>def path_optional(self, key: str, /, **kwargs) -&gt; Optional[str]:\n    \"\"\"\n    Resolve a path without raising exceptions.\n\n    Similar to path() but returns None instead of raising exceptions.\n    Useful for checking if paths exist or can be resolved without error handling.\n\n    Parameters\n    ----------\n    key : str\n        Path template key\n    **kwargs\n        Template entities (e.g., subject_id='001')\n\n    Returns\n    -------\n    str or None\n        Resolved path if successful, None otherwise\n\n    Examples\n    --------\n    &gt;&gt;&gt; pm = PathManager()\n    &gt;&gt;&gt; m2m_path = pm.path_optional(\"m2m\", subject_id=\"001\")\n    &gt;&gt;&gt; if m2m_path:\n    ...     print(f\"m2m exists at {m2m_path}\")\n\n    Notes\n    -----\n    Returns None if:\n    - project_dir is not resolved\n    - key is unknown\n    - required entities are missing\n    \"\"\"\n    if not self.project_dir:\n        return None\n    tpl = _COMPILED_TEMPLATES.get(key)\n    if tpl is None:\n        return None\n    # Optional resolver: if required entities are missing, return None.\n    for e in tpl.required_entities:\n        if e not in kwargs:\n            return None\n    try:\n        return _cached_render(self.project_dir, key, _freeze_kwargs(kwargs))\n    except KeyError:\n        return None\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.spherical_analysis_name","title":"spherical_analysis_name  <code>staticmethod</code>","text":"<pre><code>spherical_analysis_name(x: float, y: float, z: float, radius: float, coordinate_space: str) -&gt; str\n</code></pre> <p>Match GUI/CLI naming: sphere_x..y.._z.._r..{_MNI|_subject}.</p> Source code in <code>tit/core/paths.py</code> <pre><code>@staticmethod\ndef spherical_analysis_name(x: float, y: float, z: float, radius: float, coordinate_space: str) -&gt; str:\n    \"\"\"Match GUI/CLI naming: sphere_x.._y.._z.._r.._{_MNI|_subject}.\"\"\"\n    coord_space_suffix = \"_MNI\" if str(coordinate_space).upper() == \"MNI\" else \"_subject\"\n    return f\"sphere_x{x:.2f}_y{y:.2f}_z{z:.2f}_r{float(radius)}{coord_space_suffix}\"\n</code></pre>"},{"location":"reference/core/#tit.core.PathManager.validate_subject_structure","title":"validate_subject_structure","text":"<pre><code>validate_subject_structure(subject_id: str) -&gt; Dict[str, any]\n</code></pre> <p>Validate that a subject has the required directory structure.</p> <p>Args:     subject_id: Subject ID</p> <p>Returns:     Dictionary with validation results:     - 'valid': bool indicating if structure is valid     - 'missing': list of missing required components     - 'warnings': list of optional missing components</p> Source code in <code>tit/core/paths.py</code> <pre><code>def validate_subject_structure(self, subject_id: str) -&gt; Dict[str, any]:\n    \"\"\"\n    Validate that a subject has the required directory structure.\n\n    Args:\n        subject_id: Subject ID\n\n    Returns:\n        Dictionary with validation results:\n        - 'valid': bool indicating if structure is valid\n        - 'missing': list of missing required components\n        - 'warnings': list of optional missing components\n    \"\"\"\n    results = {\n        'valid': True,\n        'missing': [],\n        'warnings': []\n    }\n\n    # Check subject directory\n    subject_dir = self.path_optional(\"simnibs_subject\", subject_id=subject_id)\n    if not subject_dir or not os.path.isdir(subject_dir):\n        results['valid'] = False\n        results['missing'].append(f\"Subject directory: {const.PREFIX_SUBJECT}{subject_id}\")\n        return results\n\n    # Check m2m directory\n    if not self.is_dir(\"m2m\", subject_id=subject_id):\n        results['valid'] = False\n        results['missing'].append(f\"m2m directory: {const.DIR_M2M_PREFIX}{subject_id}\")\n\n    # Check for EEG positions (optional warning)\n    if not self.is_dir(\"eeg_positions\", subject_id=subject_id):\n        results['warnings'].append(const.WARNING_NO_EEG_POSITIONS)\n\n    return results\n</code></pre>"},{"location":"reference/core/#tit.core.get_path_manager","title":"get_path_manager","text":"<pre><code>get_path_manager() -&gt; PathManager\n</code></pre> <p>Get the global PathManager singleton instance.</p> <p>Returns:     The global path manager instance</p> Source code in <code>tit/core/paths.py</code> <pre><code>def get_path_manager() -&gt; PathManager:\n    \"\"\"\n    Get the global PathManager singleton instance.\n\n    Returns:\n        The global path manager instance\n    \"\"\"\n    global _path_manager_instance\n    if _path_manager_instance is None:\n        _path_manager_instance = PathManager()\n    return _path_manager_instance\n</code></pre>"},{"location":"reference/core/#tit.core.reset_path_manager","title":"reset_path_manager","text":"<pre><code>reset_path_manager()\n</code></pre> <p>Reset the global PathManager singleton instance. Useful for testing or when project directory changes.</p> Source code in <code>tit/core/paths.py</code> <pre><code>def reset_path_manager():\n    \"\"\"\n    Reset the global PathManager singleton instance.\n    Useful for testing or when project directory changes.\n    \"\"\"\n    global _path_manager_instance\n    _path_manager_instance = None\n    try:\n        _cached_render.cache_clear()\n    except Exception:\n        pass\n</code></pre>"},{"location":"reference/core/#paths-titcorepaths","title":"Paths (<code>tit.core.paths</code>)","text":"<p>TI-Toolbox Path Management Centralized path management for the entire TI-Toolbox codebase.</p> <p>This module provides a professional path management system for BIDS-compliant directory structures, handling project directories, subject paths, and common path patterns consistently across all tools.</p> <p>Usage:     # Use the singleton instance     from tit.core import get_path_manager</p> <pre><code>pm = get_path_manager()\nsubjects = pm.list_subjects()\nm2m_dir = pm.path(\"m2m\", subject_id=\"001\")\nsim_dir = pm.path(\"simulation\", subject_id=\"001\", simulation_name=\"montage1\")\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager","title":"PathManager","text":"<pre><code>PathManager(project_dir: Optional[str] = None)\n</code></pre> <p>Centralized path management for TI-Toolbox.</p> <p>This class provides consistent path resolution across all components: - Project directory detection - Subject listing and validation - Common path patterns (derivatives, SimNIBS, m2m, etc.) - BIDS-compliant directory structure handling</p> <p>Usage:     pm = PathManager()     pm.project_dir = \"/path/to/project\"  # Explicit set     print(pm.project_dir)                # Get current     print(pm.project_dir_name)           # Just the name</p> <p>Initialize the path manager.</p> <p>Args:     project_dir: Optional explicit project directory. If not provided,                 auto-detection from environment variables is attempted                 on first access of project_dir property.</p> Source code in <code>tit/core/paths.py</code> <pre><code>def __init__(self, project_dir: Optional[str] = None):\n    \"\"\"\n    Initialize the path manager.\n\n    Args:\n        project_dir: Optional explicit project directory. If not provided,\n                    auto-detection from environment variables is attempted\n                    on first access of project_dir property.\n    \"\"\"\n    self._project_dir: Optional[str] = None\n    _ensure_compiled_templates(self.__class__)\n\n    if project_dir:\n        self.project_dir = project_dir  # Use setter for validation\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.project_dir","title":"project_dir  <code>property</code> <code>writable</code>","text":"<pre><code>project_dir: Optional[str]\n</code></pre> <p>Get/set the project directory path.</p> <p>Auto-detects from environment on first access if not set. Setting validates the path exists.</p> <p>Usage:     pm.project_dir = \"/path/to/project\"  # set     path = pm.project_dir                # get</p>"},{"location":"reference/core/#tit.core.paths.PathManager.project_dir_name","title":"project_dir_name  <code>property</code>","text":"<pre><code>project_dir_name: Optional[str]\n</code></pre> <p>Get the project directory name (basename of project_dir).</p>"},{"location":"reference/core/#tit.core.paths.PathManager.analysis_space_dir_name","title":"analysis_space_dir_name  <code>staticmethod</code>","text":"<pre><code>analysis_space_dir_name(space: str) -&gt; str\n</code></pre> <p>Map analyzer space ('mesh'|'voxel') to folder name ('Mesh'|'Voxel').</p> Source code in <code>tit/core/paths.py</code> <pre><code>@staticmethod\ndef analysis_space_dir_name(space: str) -&gt; str:\n    \"\"\"Map analyzer space ('mesh'|'voxel') to folder name ('Mesh'|'Voxel').\"\"\"\n    return \"Mesh\" if str(space).lower() == \"mesh\" else \"Voxel\"\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.cortical_analysis_name","title":"cortical_analysis_name  <code>classmethod</code>","text":"<pre><code>cortical_analysis_name(*, whole_head: bool, region: Optional[str], atlas_name: Optional[str] = None, atlas_path: Optional[str] = None) -&gt; str\n</code></pre> <p>Match GUI/CLI naming for cortical analysis folders.</p> Source code in <code>tit/core/paths.py</code> <pre><code>@classmethod\ndef cortical_analysis_name(\n    cls,\n    *,\n    whole_head: bool,\n    region: Optional[str],\n    atlas_name: Optional[str] = None,\n    atlas_path: Optional[str] = None,\n) -&gt; str:\n    \"\"\"Match GUI/CLI naming for cortical analysis folders.\"\"\"\n    atlas_clean = cls._atlas_name_clean(atlas_name or atlas_path or \"unknown_atlas\")\n    if whole_head:\n        return f\"whole_head_{atlas_clean}\"\n    region_val = str(region or \"\").strip()\n    if not region_val:\n        raise ValueError(\"region is required for cortical analysis unless whole_head=True\")\n    return f\"region_{region_val}_{atlas_clean}\"\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.ensure_dir","title":"ensure_dir","text":"<pre><code>ensure_dir(key: str, /, **kwargs) -&gt; str\n</code></pre> <p>Resolve a directory path and create it (parents=True, exist_ok=True).</p> Source code in <code>tit/core/paths.py</code> <pre><code>def ensure_dir(self, key: str, /, **kwargs) -&gt; str:\n    \"\"\"Resolve a directory path and create it (parents=True, exist_ok=True).\"\"\"\n    p = self.path(key, **kwargs)\n    os.makedirs(p, exist_ok=True)\n    return p\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.exists","title":"exists","text":"<pre><code>exists(key: str, /, **kwargs) -&gt; bool\n</code></pre> <p>Check if a path exists.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Path template key</p> required <code>**kwargs</code> <p>Template entities</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if path can be resolved and exists on filesystem</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pm = PathManager()\n&gt;&gt;&gt; if pm.exists(\"m2m\", subject_id=\"001\"):\n...     print(\"m2m directory exists\")\n</code></pre> Source code in <code>tit/core/paths.py</code> <pre><code>def exists(self, key: str, /, **kwargs) -&gt; bool:\n    \"\"\"\n    Check if a path exists.\n\n    Parameters\n    ----------\n    key : str\n        Path template key\n    **kwargs\n        Template entities\n\n    Returns\n    -------\n    bool\n        True if path can be resolved and exists on filesystem\n\n    Examples\n    --------\n    &gt;&gt;&gt; pm = PathManager()\n    &gt;&gt;&gt; if pm.exists(\"m2m\", subject_id=\"001\"):\n    ...     print(\"m2m directory exists\")\n    \"\"\"\n    p = self.path_optional(key, **kwargs)\n    return bool(p and os.path.exists(p))\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.get_analysis_output_dir","title":"get_analysis_output_dir","text":"<pre><code>get_analysis_output_dir(*, subject_id: str, simulation_name: str, space: str, analysis_type: str, coordinates: Optional[List[float]] = None, radius: Optional[float] = None, coordinate_space: str = 'subject', whole_head: bool = False, region: Optional[str] = None, atlas_name: Optional[str] = None, atlas_path: Optional[str] = None) -&gt; Optional[str]\n</code></pre> <p>Centralized analysis output directory used by GUI/CLI. Returns the folder but does NOT create it.</p> Source code in <code>tit/core/paths.py</code> <pre><code>def get_analysis_output_dir(\n    self,\n    *,\n    subject_id: str,\n    simulation_name: str,\n    space: str,\n    analysis_type: str,\n    coordinates: Optional[List[float]] = None,\n    radius: Optional[float] = None,\n    coordinate_space: str = \"subject\",\n    whole_head: bool = False,\n    region: Optional[str] = None,\n    atlas_name: Optional[str] = None,\n    atlas_path: Optional[str] = None,\n) -&gt; Optional[str]:\n    \"\"\"\n    Centralized analysis output directory used by GUI/CLI.\n    Returns the folder but does NOT create it.\n    \"\"\"\n    base = self.get_analysis_space_dir(subject_id, simulation_name, space)\n    if not base:\n        return None\n\n    at = str(analysis_type).lower()\n    if at == \"spherical\":\n        if not coordinates or len(coordinates) != 3 or radius is None:\n            raise ValueError(\"coordinates(3) and radius are required for spherical analysis output path\")\n        name = self.spherical_analysis_name(float(coordinates[0]), float(coordinates[1]), float(coordinates[2]), float(radius), coordinate_space)\n    else:\n        name = self.cortical_analysis_name(whole_head=bool(whole_head), region=region, atlas_name=atlas_name, atlas_path=atlas_path)\n\n    return os.path.join(base, name)\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.get_analysis_space_dir","title":"get_analysis_space_dir","text":"<pre><code>get_analysis_space_dir(subject_id: str, simulation_name: str, space: str) -&gt; Optional[str]\n</code></pre> <p>Get base analysis dir: .../Simulations//Analyses/. Source code in <code>tit/core/paths.py</code> <pre><code>def get_analysis_space_dir(self, subject_id: str, simulation_name: str, space: str) -&gt; Optional[str]:\n    \"\"\"Get base analysis dir: .../Simulations/&lt;sim&gt;/Analyses/&lt;Mesh|Voxel&gt;.\"\"\"\n    sim_dir = self.path_optional(\"simulation\", subject_id=subject_id, simulation_name=simulation_name)\n    if not sim_dir:\n        return None\n    return os.path.join(sim_dir, const.DIR_ANALYSIS, self.analysis_space_dir_name(space))\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.get_derivatives_dir","title":"get_derivatives_dir","text":"<pre><code>get_derivatives_dir() -&gt; Optional[str]\n</code></pre> <p>Get the derivatives directory path.</p> Source code in <code>tit/core/paths.py</code> <pre><code>def get_derivatives_dir(self) -&gt; Optional[str]:\n    \"\"\"Get the derivatives directory path.\"\"\"\n    return self.path_optional(\"derivatives\")\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.is_dir","title":"is_dir","text":"<pre><code>is_dir(key: str, /, **kwargs) -&gt; bool\n</code></pre> <p>Check if a path exists and is a directory.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Path template key</p> required <code>**kwargs</code> <p>Template entities</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if path can be resolved and is an existing directory</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pm = PathManager()\n&gt;&gt;&gt; if pm.is_dir(\"simulations\", subject_id=\"001\"):\n...     simulations = pm.list_simulations(\"001\")\n</code></pre> Source code in <code>tit/core/paths.py</code> <pre><code>def is_dir(self, key: str, /, **kwargs) -&gt; bool:\n    \"\"\"\n    Check if a path exists and is a directory.\n\n    Parameters\n    ----------\n    key : str\n        Path template key\n    **kwargs\n        Template entities\n\n    Returns\n    -------\n    bool\n        True if path can be resolved and is an existing directory\n\n    Examples\n    --------\n    &gt;&gt;&gt; pm = PathManager()\n    &gt;&gt;&gt; if pm.is_dir(\"simulations\", subject_id=\"001\"):\n    ...     simulations = pm.list_simulations(\"001\")\n    \"\"\"\n    p = self.path_optional(key, **kwargs)\n    return bool(p and os.path.isdir(p))\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.is_file","title":"is_file","text":"<pre><code>is_file(key: str, /, **kwargs) -&gt; bool\n</code></pre> <p>Check if a path exists and is a file.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Path template key</p> required <code>**kwargs</code> <p>Template entities</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if path can be resolved and is an existing file</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pm = PathManager()\n&gt;&gt;&gt; if pm.is_file(\"ti_mesh\", subject_id=\"001\", simulation_name=\"montage1\"):\n...     print(\"TI mesh file exists\")\n</code></pre> Source code in <code>tit/core/paths.py</code> <pre><code>def is_file(self, key: str, /, **kwargs) -&gt; bool:\n    \"\"\"\n    Check if a path exists and is a file.\n\n    Parameters\n    ----------\n    key : str\n        Path template key\n    **kwargs\n        Template entities\n\n    Returns\n    -------\n    bool\n        True if path can be resolved and is an existing file\n\n    Examples\n    --------\n    &gt;&gt;&gt; pm = PathManager()\n    &gt;&gt;&gt; if pm.is_file(\"ti_mesh\", subject_id=\"001\", simulation_name=\"montage1\"):\n    ...     print(\"TI mesh file exists\")\n    \"\"\"\n    p = self.path_optional(key, **kwargs)\n    return bool(p and os.path.isfile(p))\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.list_eeg_caps","title":"list_eeg_caps","text":"<pre><code>list_eeg_caps(subject_id: str) -&gt; List[str]\n</code></pre> <p>List available EEG cap files for a subject.</p> <p>Args:     subject_id: Subject ID</p> <p>Returns:     List of EEG cap CSV filenames, sorted alphabetically</p> Source code in <code>tit/core/paths.py</code> <pre><code>def list_eeg_caps(self, subject_id: str) -&gt; List[str]:\n    \"\"\"\n    List available EEG cap files for a subject.\n\n    Args:\n        subject_id: Subject ID\n\n    Returns:\n        List of EEG cap CSV filenames, sorted alphabetically\n    \"\"\"\n    eeg_pos_dir = self.path_optional(\"eeg_positions\", subject_id=subject_id)\n    if not eeg_pos_dir or not os.path.isdir(eeg_pos_dir):\n        return []\n\n    caps = []\n    for file in os.listdir(eeg_pos_dir):\n        if file.endswith(const.EXT_CSV) and not file.startswith('.'):\n            caps.append(file)\n\n    caps.sort()\n    return caps\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.list_flex_search_runs","title":"list_flex_search_runs","text":"<pre><code>list_flex_search_runs(subject_id: str) -&gt; List[str]\n</code></pre> <p>List flex-search run folders that contain electrode_positions.json. Uses os.scandir for efficiency.</p> Source code in <code>tit/core/paths.py</code> <pre><code>def list_flex_search_runs(self, subject_id: str) -&gt; List[str]:\n    \"\"\"\n    List flex-search run folders that contain electrode_positions.json.\n    Uses os.scandir for efficiency.\n    \"\"\"\n    root = self.path_optional(\"flex_search\", subject_id=subject_id)\n    if not root or not os.path.isdir(root):\n        return []\n    out: List[str] = []\n    fname = \"electrode_positions.json\"\n    try:\n        with os.scandir(root) as it:\n            for entry in it:\n                if not entry.is_dir():\n                    continue\n                if entry.name.startswith(\".\"):\n                    continue\n                if os.path.isfile(os.path.join(entry.path, fname)):\n                    out.append(entry.name)\n    except OSError:\n        return []\n    out.sort()\n    return out\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.list_simulations","title":"list_simulations","text":"<pre><code>list_simulations(subject_id: str) -&gt; List[str]\n</code></pre> <p>List all simulations for a subject.</p> <p>Args:     subject_id: Subject ID</p> <p>Returns:     List of simulation names, sorted alphabetically</p> Source code in <code>tit/core/paths.py</code> <pre><code>def list_simulations(self, subject_id: str) -&gt; List[str]:\n    \"\"\"\n    List all simulations for a subject.\n\n    Args:\n        subject_id: Subject ID\n\n    Returns:\n        List of simulation names, sorted alphabetically\n    \"\"\"\n    sim_root = self.path_optional(\"simulations\", subject_id=subject_id)\n    if not sim_root or not os.path.isdir(sim_root):\n        return []\n\n    simulations: List[str] = []\n    try:\n        with os.scandir(sim_root) as it:\n            for entry in it:\n                if entry.is_dir() and not entry.name.startswith(\".\"):\n                    simulations.append(entry.name)\n    except OSError:\n        return []\n    simulations.sort()\n    return simulations\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.list_subjects","title":"list_subjects","text":"<pre><code>list_subjects() -&gt; List[str]\n</code></pre> <p>List all available subjects in the project.</p> <p>Returns:     List of subject IDs (without 'sub-' prefix), sorted naturally</p> Source code in <code>tit/core/paths.py</code> <pre><code>def list_subjects(self) -&gt; List[str]:\n    \"\"\"\n    List all available subjects in the project.\n\n    Returns:\n        List of subject IDs (without 'sub-' prefix), sorted naturally\n    \"\"\"\n    simnibs_dir = self.path_optional(\"simnibs\")\n    if not simnibs_dir or not os.path.isdir(simnibs_dir):\n        return []\n\n    subjects = []\n    for item in os.listdir(simnibs_dir):\n        if not item.startswith(const.PREFIX_SUBJECT):\n            continue\n        subject_id = item.replace(const.PREFIX_SUBJECT, \"\")\n        # Only list subjects that have an m2m folder (SimNIBS-ready).\n        if self.is_dir(\"m2m\", subject_id=subject_id):\n            subjects.append(subject_id)\n\n    # Sort subjects naturally (001, 002, 010, 100)\n    subjects.sort(key=lambda x: [int(c) if c.isdigit() else c.lower() \n                                 for c in re.split('([0-9]+)', x)])\n    return subjects\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.path","title":"path","text":"<pre><code>path(key: str, /, **kwargs) -&gt; str\n</code></pre> <p>Resolve a canonical path by key from predefined templates.</p> <p>This is the primary path resolution method. It provides fast, cached access to all standard TI-Toolbox paths using pre-compiled templates.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Path template key (e.g., 'm2m', 'simulation', 'ti_mesh')</p> required <code>**kwargs</code> <p>Required entities for the template (e.g., subject_id='001', simulation_name='montage1')</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Resolved absolute path</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If project_dir is not resolved</p> <code>KeyError</code> <p>If key is unknown</p> <code>ValueError</code> <p>If required template entities are missing</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pm = PathManager()\n&gt;&gt;&gt; m2m_path = pm.path(\"m2m\", subject_id=\"001\")\n&gt;&gt;&gt; sim_path = pm.path(\"simulation\", subject_id=\"001\", simulation_name=\"montage1\")\n&gt;&gt;&gt; mesh_path = pm.path(\"ti_mesh\", subject_id=\"001\", simulation_name=\"montage1\")\n</code></pre> Notes <ul> <li>Results are cached for performance (8192 entry LRU cache)</li> <li>All paths are resolved relative to project_dir</li> <li>Template entities are type-checked and missing entities raise ValueError</li> </ul> Source code in <code>tit/core/paths.py</code> <pre><code>def path(self, key: str, /, **kwargs) -&gt; str:\n    \"\"\"\n    Resolve a canonical path by key from predefined templates.\n\n    This is the primary path resolution method. It provides fast, cached access\n    to all standard TI-Toolbox paths using pre-compiled templates.\n\n    Parameters\n    ----------\n    key : str\n        Path template key (e.g., 'm2m', 'simulation', 'ti_mesh')\n    **kwargs\n        Required entities for the template (e.g., subject_id='001', simulation_name='montage1')\n\n    Returns\n    -------\n    str\n        Resolved absolute path\n\n    Raises\n    ------\n    RuntimeError\n        If project_dir is not resolved\n    KeyError\n        If key is unknown\n    ValueError\n        If required template entities are missing\n\n    Examples\n    --------\n    &gt;&gt;&gt; pm = PathManager()\n    &gt;&gt;&gt; m2m_path = pm.path(\"m2m\", subject_id=\"001\")\n    &gt;&gt;&gt; sim_path = pm.path(\"simulation\", subject_id=\"001\", simulation_name=\"montage1\")\n    &gt;&gt;&gt; mesh_path = pm.path(\"ti_mesh\", subject_id=\"001\", simulation_name=\"montage1\")\n\n    Notes\n    -----\n    - Results are cached for performance (8192 entry LRU cache)\n    - All paths are resolved relative to project_dir\n    - Template entities are type-checked and missing entities raise ValueError\n    \"\"\"\n    if not self.project_dir:\n        raise RuntimeError(\"Project directory not resolved. Set PROJECT_DIR_NAME or PROJECT_DIR in Docker.\")\n    tpl = _COMPILED_TEMPLATES.get(key)\n    if tpl is None:\n        raise KeyError(f\"Unknown path key: {key}\")\n    missing = [e for e in tpl.required_entities if e not in kwargs]\n    if missing:\n        raise ValueError(f\"Missing required path entities for {key!r}: {', '.join(missing)}\")\n    return _cached_render(self.project_dir, key, _freeze_kwargs(kwargs))\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.path_optional","title":"path_optional","text":"<pre><code>path_optional(key: str, /, **kwargs) -&gt; Optional[str]\n</code></pre> <p>Resolve a path without raising exceptions.</p> <p>Similar to path() but returns None instead of raising exceptions. Useful for checking if paths exist or can be resolved without error handling.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Path template key</p> required <code>**kwargs</code> <p>Template entities (e.g., subject_id='001')</p> <code>{}</code> <p>Returns:</p> Type Description <code>str or None</code> <p>Resolved path if successful, None otherwise</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pm = PathManager()\n&gt;&gt;&gt; m2m_path = pm.path_optional(\"m2m\", subject_id=\"001\")\n&gt;&gt;&gt; if m2m_path:\n...     print(f\"m2m exists at {m2m_path}\")\n</code></pre> Notes <p>Returns None if: - project_dir is not resolved - key is unknown - required entities are missing</p> Source code in <code>tit/core/paths.py</code> <pre><code>def path_optional(self, key: str, /, **kwargs) -&gt; Optional[str]:\n    \"\"\"\n    Resolve a path without raising exceptions.\n\n    Similar to path() but returns None instead of raising exceptions.\n    Useful for checking if paths exist or can be resolved without error handling.\n\n    Parameters\n    ----------\n    key : str\n        Path template key\n    **kwargs\n        Template entities (e.g., subject_id='001')\n\n    Returns\n    -------\n    str or None\n        Resolved path if successful, None otherwise\n\n    Examples\n    --------\n    &gt;&gt;&gt; pm = PathManager()\n    &gt;&gt;&gt; m2m_path = pm.path_optional(\"m2m\", subject_id=\"001\")\n    &gt;&gt;&gt; if m2m_path:\n    ...     print(f\"m2m exists at {m2m_path}\")\n\n    Notes\n    -----\n    Returns None if:\n    - project_dir is not resolved\n    - key is unknown\n    - required entities are missing\n    \"\"\"\n    if not self.project_dir:\n        return None\n    tpl = _COMPILED_TEMPLATES.get(key)\n    if tpl is None:\n        return None\n    # Optional resolver: if required entities are missing, return None.\n    for e in tpl.required_entities:\n        if e not in kwargs:\n            return None\n    try:\n        return _cached_render(self.project_dir, key, _freeze_kwargs(kwargs))\n    except KeyError:\n        return None\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.spherical_analysis_name","title":"spherical_analysis_name  <code>staticmethod</code>","text":"<pre><code>spherical_analysis_name(x: float, y: float, z: float, radius: float, coordinate_space: str) -&gt; str\n</code></pre> <p>Match GUI/CLI naming: sphere_x..y.._z.._r..{_MNI|_subject}.</p> Source code in <code>tit/core/paths.py</code> <pre><code>@staticmethod\ndef spherical_analysis_name(x: float, y: float, z: float, radius: float, coordinate_space: str) -&gt; str:\n    \"\"\"Match GUI/CLI naming: sphere_x.._y.._z.._r.._{_MNI|_subject}.\"\"\"\n    coord_space_suffix = \"_MNI\" if str(coordinate_space).upper() == \"MNI\" else \"_subject\"\n    return f\"sphere_x{x:.2f}_y{y:.2f}_z{z:.2f}_r{float(radius)}{coord_space_suffix}\"\n</code></pre>"},{"location":"reference/core/#tit.core.paths.PathManager.validate_subject_structure","title":"validate_subject_structure","text":"<pre><code>validate_subject_structure(subject_id: str) -&gt; Dict[str, any]\n</code></pre> <p>Validate that a subject has the required directory structure.</p> <p>Args:     subject_id: Subject ID</p> <p>Returns:     Dictionary with validation results:     - 'valid': bool indicating if structure is valid     - 'missing': list of missing required components     - 'warnings': list of optional missing components</p> Source code in <code>tit/core/paths.py</code> <pre><code>def validate_subject_structure(self, subject_id: str) -&gt; Dict[str, any]:\n    \"\"\"\n    Validate that a subject has the required directory structure.\n\n    Args:\n        subject_id: Subject ID\n\n    Returns:\n        Dictionary with validation results:\n        - 'valid': bool indicating if structure is valid\n        - 'missing': list of missing required components\n        - 'warnings': list of optional missing components\n    \"\"\"\n    results = {\n        'valid': True,\n        'missing': [],\n        'warnings': []\n    }\n\n    # Check subject directory\n    subject_dir = self.path_optional(\"simnibs_subject\", subject_id=subject_id)\n    if not subject_dir or not os.path.isdir(subject_dir):\n        results['valid'] = False\n        results['missing'].append(f\"Subject directory: {const.PREFIX_SUBJECT}{subject_id}\")\n        return results\n\n    # Check m2m directory\n    if not self.is_dir(\"m2m\", subject_id=subject_id):\n        results['valid'] = False\n        results['missing'].append(f\"m2m directory: {const.DIR_M2M_PREFIX}{subject_id}\")\n\n    # Check for EEG positions (optional warning)\n    if not self.is_dir(\"eeg_positions\", subject_id=subject_id):\n        results['warnings'].append(const.WARNING_NO_EEG_POSITIONS)\n\n    return results\n</code></pre>"},{"location":"reference/core/#tit.core.paths.get_path_manager","title":"get_path_manager","text":"<pre><code>get_path_manager() -&gt; PathManager\n</code></pre> <p>Get the global PathManager singleton instance.</p> <p>Returns:     The global path manager instance</p> Source code in <code>tit/core/paths.py</code> <pre><code>def get_path_manager() -&gt; PathManager:\n    \"\"\"\n    Get the global PathManager singleton instance.\n\n    Returns:\n        The global path manager instance\n    \"\"\"\n    global _path_manager_instance\n    if _path_manager_instance is None:\n        _path_manager_instance = PathManager()\n    return _path_manager_instance\n</code></pre>"},{"location":"reference/core/#tit.core.paths.reset_path_manager","title":"reset_path_manager","text":"<pre><code>reset_path_manager()\n</code></pre> <p>Reset the global PathManager singleton instance. Useful for testing or when project directory changes.</p> Source code in <code>tit/core/paths.py</code> <pre><code>def reset_path_manager():\n    \"\"\"\n    Reset the global PathManager singleton instance.\n    Useful for testing or when project directory changes.\n    \"\"\"\n    global _path_manager_instance\n    _path_manager_instance = None\n    try:\n        _cached_render.cache_clear()\n    except Exception:\n        pass\n</code></pre>"},{"location":"reference/core/#constants-titcoreconstants","title":"Constants (<code>tit.core.constants</code>)","text":"<p>TI-Toolbox Constants Centralized constants for the entire TI-Toolbox codebase. This module contains all hardcoded values, magic numbers, and configuration constants.</p>"},{"location":"reference/core/#nifti-titcorenifti","title":"NIfTI (<code>tit.core.nifti</code>)","text":"<p>TI-Toolbox NIfTI Module</p> <p>TI-Toolbox specific NIfTI file operations. Provides functions for loading subject and group data from TI-Toolbox BIDS structure.</p>"},{"location":"reference/core/#tit.core.nifti.load_group_data_ti_toolbox","title":"load_group_data_ti_toolbox","text":"<pre><code>load_group_data_ti_toolbox(subject_configs: List[Dict], nifti_file_pattern: str = 'grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz', dtype=np.float32) -&gt; Tuple[np.ndarray, nib.Nifti1Image, List[str]]\n</code></pre> <p>Load multiple subjects from TI-Toolbox BIDS structure</p> Parameters: <p>subject_configs : list of dict     List of subject configurations with keys:     - 'subject_id': Subject ID (e.g., '070')     - 'simulation_name': Simulation name (e.g., 'ICP_RHIPPO') nifti_file_pattern : str, optional     Pattern for NIfTI files dtype : numpy dtype, optional     Data type to load (default: float32)</p> Returns: <p>data_4d : ndarray (x, y, z, n_subjects)     4D array with all loaded data template_img : nibabel Nifti1Image     Template image from first subject subject_ids : list of str     List of successfully loaded subject IDs</p> Source code in <code>tit/core/nifti.py</code> <pre><code>def load_group_data_ti_toolbox(\n    subject_configs: List[Dict],\n    nifti_file_pattern: str = \"grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz\",\n    dtype=np.float32\n) -&gt; Tuple[np.ndarray, nib.Nifti1Image, List[str]]:\n    \"\"\"\n    Load multiple subjects from TI-Toolbox BIDS structure\n\n    Parameters:\n    -----------\n    subject_configs : list of dict\n        List of subject configurations with keys:\n        - 'subject_id': Subject ID (e.g., '070')\n        - 'simulation_name': Simulation name (e.g., 'ICP_RHIPPO')\n    nifti_file_pattern : str, optional\n        Pattern for NIfTI files\n    dtype : numpy dtype, optional\n        Data type to load (default: float32)\n\n    Returns:\n    --------\n    data_4d : ndarray (x, y, z, n_subjects)\n        4D array with all loaded data\n    template_img : nibabel Nifti1Image\n        Template image from first subject\n    subject_ids : list of str\n        List of successfully loaded subject IDs\n    \"\"\"\n    data_list = []\n    subject_ids = []\n    template_img = None\n    template_affine = None\n    template_header = None\n\n    for config in subject_configs:\n        subject_id = config['subject_id']\n        simulation_name = config['simulation_name']\n\n        try:\n            data, img, filepath = load_subject_nifti_ti_toolbox(\n                subject_id,\n                simulation_name,\n                nifti_file_pattern,\n                dtype=dtype\n            )\n\n            # Store template image from first subject\n            if template_img is None:\n                template_img = img\n                template_affine = img.affine.copy()\n                template_header = img.header.copy()\n\n            data_list.append(data)\n            subject_ids.append(subject_id)\n\n            # Clear the image object to free memory\n            del img\n\n        except FileNotFoundError as e:\n            print(f\"Warning: File not found for subject {subject_id} - {e}\")\n            continue\n        except Exception as e:\n            print(f\"Warning: Error loading subject {subject_id} - {e}\")\n            continue\n\n    if len(data_list) == 0:\n        raise ValueError(\"No subjects could be loaded successfully\")\n\n    # Stack into 4D array\n    data_4d = np.stack(data_list, axis=-1).astype(dtype)\n\n    # Recreate minimal template image\n    template_img = nib.Nifti1Image(data_4d[..., 0], template_affine, template_header)\n\n    # Clean up\n    del data_list\n    gc.collect()\n\n    return data_4d, template_img, subject_ids\n</code></pre>"},{"location":"reference/core/#tit.core.nifti.load_grouped_subjects_ti_toolbox","title":"load_grouped_subjects_ti_toolbox","text":"<pre><code>load_grouped_subjects_ti_toolbox(subject_configs: List[Dict], nifti_file_pattern: str = 'grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz', dtype=np.float32) -&gt; Tuple[Dict[str, np.ndarray], nib.Nifti1Image, Dict[str, List[str]]]\n</code></pre> <p>Load subjects organized by groups from TI-Toolbox BIDS structure</p> Parameters: <p>subject_configs : list of dict     List of subject configurations with keys:     - 'subject_id': Subject ID (e.g., '070')     - 'simulation_name': Simulation name (e.g., 'ICP_RHIPPO')     - 'group': Group name (e.g., 'group1', 'Responders', etc.) nifti_file_pattern : str, optional     Pattern for NIfTI files dtype : numpy dtype, optional     Data type to load (default: float32)</p> Returns: <p>groups_data : dict of str -&gt; ndarray     Dictionary mapping group names to 4D arrays (x, y, z, n_subjects) template_img : nibabel Nifti1Image     Template image from first subject groups_ids : dict of str -&gt; list of str     Dictionary mapping group names to lists of subject IDs</p> Source code in <code>tit/core/nifti.py</code> <pre><code>def load_grouped_subjects_ti_toolbox(\n    subject_configs: List[Dict],\n    nifti_file_pattern: str = \"grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz\",\n    dtype=np.float32\n) -&gt; Tuple[Dict[str, np.ndarray], nib.Nifti1Image, Dict[str, List[str]]]:\n    \"\"\"\n    Load subjects organized by groups from TI-Toolbox BIDS structure\n\n    Parameters:\n    -----------\n    subject_configs : list of dict\n        List of subject configurations with keys:\n        - 'subject_id': Subject ID (e.g., '070')\n        - 'simulation_name': Simulation name (e.g., 'ICP_RHIPPO')\n        - 'group': Group name (e.g., 'group1', 'Responders', etc.)\n    nifti_file_pattern : str, optional\n        Pattern for NIfTI files\n    dtype : numpy dtype, optional\n        Data type to load (default: float32)\n\n    Returns:\n    --------\n    groups_data : dict of str -&gt; ndarray\n        Dictionary mapping group names to 4D arrays (x, y, z, n_subjects)\n    template_img : nibabel Nifti1Image\n        Template image from first subject\n    groups_ids : dict of str -&gt; list of str\n        Dictionary mapping group names to lists of subject IDs\n    \"\"\"\n    # Organize configs by group\n    groups = {}\n    for config in subject_configs:\n        group_name = config.get('group', 'default')\n        if group_name not in groups:\n            groups[group_name] = []\n        groups[group_name].append(config)\n\n    # Load each group\n    groups_data = {}\n    groups_ids = {}\n    template_img = None\n\n    for group_name, group_configs in groups.items():\n        data_4d, img, subject_ids = load_group_data_ti_toolbox(\n            group_configs,\n            nifti_file_pattern,\n            dtype=dtype\n        )\n\n        groups_data[group_name] = data_4d\n        groups_ids[group_name] = subject_ids\n\n        # Use first group's image as template\n        if template_img is None:\n            template_img = img\n\n    return groups_data, template_img, groups_ids\n</code></pre>"},{"location":"reference/core/#tit.core.nifti.load_subject_nifti_ti_toolbox","title":"load_subject_nifti_ti_toolbox","text":"<pre><code>load_subject_nifti_ti_toolbox(subject_id: str, simulation_name: str, nifti_file_pattern: str = 'grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz', dtype=np.float32) -&gt; Tuple[np.ndarray, nib.Nifti1Image, str]\n</code></pre> <p>Load a NIfTI file from TI-Toolbox BIDS structure</p> Parameters: <p>subject_id : str     Subject ID (e.g., '070') simulation_name : str     Simulation name (e.g., 'ICP_RHIPPO') nifti_file_pattern : str, optional     Pattern for NIfTI files. Default: 'grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz'     Available variables: {subject_id}, {simulation_name} dtype : numpy dtype, optional     Data type to load (default: float32)</p> Returns: <p>data : ndarray     NIfTI data img : nibabel Nifti1Image     NIfTI image object filepath : str     Full path to the loaded file</p> Source code in <code>tit/core/nifti.py</code> <pre><code>def load_subject_nifti_ti_toolbox(\n    subject_id: str,\n    simulation_name: str,\n    nifti_file_pattern: str = \"grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz\",\n    dtype=np.float32\n) -&gt; Tuple[np.ndarray, nib.Nifti1Image, str]:\n    \"\"\"\n    Load a NIfTI file from TI-Toolbox BIDS structure\n\n    Parameters:\n    -----------\n    subject_id : str\n        Subject ID (e.g., '070')\n    simulation_name : str\n        Simulation name (e.g., 'ICP_RHIPPO')\n    nifti_file_pattern : str, optional\n        Pattern for NIfTI files. Default: 'grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz'\n        Available variables: {subject_id}, {simulation_name}\n    dtype : numpy dtype, optional\n        Data type to load (default: float32)\n\n    Returns:\n    --------\n    data : ndarray\n        NIfTI data\n    img : nibabel Nifti1Image\n        NIfTI image object\n    filepath : str\n        Full path to the loaded file\n    \"\"\"\n    pm = get_path_manager() if get_path_manager else None\n\n    # Construct file path using TI-Toolbox path structure\n\n    project_dir = pm.project_dir\n    if not project_dir:\n        raise ValueError(\"Project directory not found. Is PROJECT_DIR_NAME set?\")\n\n    nifti_dir = os.path.join(\n        project_dir,\n        const.DIR_DERIVATIVES,\n        const.DIR_SIMNIBS,\n        f\"{const.PREFIX_SUBJECT}{subject_id}\",\n        \"Simulations\",\n        simulation_name,\n        \"TI\",\n        \"niftis\"\n    )\n\n    # Format the filename pattern\n    filename = nifti_file_pattern.format(\n        subject_id=subject_id,\n        simulation_name=simulation_name\n    )\n    filepath = os.path.join(nifti_dir, filename)\n\n    # Load the file (inline basic loading)\n    if not os.path.exists(filepath):\n        # Provide extra context to make debugging path/layout issues easier\n        if os.path.isdir(nifti_dir):\n            try:\n                existing = sorted(os.listdir(nifti_dir))\n            except Exception:\n                existing = []\n            preview = existing[:20]\n            suffix = \"\"\n            if len(existing) &gt; len(preview):\n                suffix = f\" (showing first {len(preview)} of {len(existing)})\"\n            raise FileNotFoundError(\n                f\"NIfTI file not found: {filepath}. \"\n                f\"Directory exists: {nifti_dir}. \"\n                f\"Files in directory: {preview}{suffix}\"\n            )\n        raise FileNotFoundError(f\"NIfTI file not found: {filepath}\")\n\n    img = nib.load(filepath)\n    data = img.get_fdata(dtype=dtype)\n\n    # Ensure 3D data (squeeze out extra dimensions if present)\n    while data.ndim &gt; 3:\n        data = np.squeeze(data, axis=-1)\n\n    return data, img, filepath\n</code></pre>"},{"location":"reference/core/#mesh-titcoremesh","title":"Mesh (<code>tit.core.mesh</code>)","text":"<p>Core mesh utilities for TI-toolbox.</p> <p>This module contains shared mesh-related functionality used across the toolbox.</p>"},{"location":"reference/core/#tit.core.mesh.create_mesh_opt_file","title":"create_mesh_opt_file","text":"<pre><code>create_mesh_opt_file(mesh_path, field_info=None)\n</code></pre> <p>Create a .opt file for Gmsh visualization of mesh fields.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_path</code> <code>str</code> <p>Path to the .msh file (without .opt extension)</p> required <code>field_info</code> <code>dict</code> <p>Dictionary with field information containing: - 'fields': list of field names to visualize - 'max_values': dict mapping field names to their max values (optional) - 'field_type': str, either 'node' or 'element' (default: 'node')</p> <code>None</code> Source code in <code>tit/core/mesh.py</code> <pre><code>def create_mesh_opt_file(mesh_path, field_info=None):\n    \"\"\"\n    Create a .opt file for Gmsh visualization of mesh fields.\n\n    Parameters\n    ----------\n    mesh_path : str\n        Path to the .msh file (without .opt extension)\n    field_info : dict, optional\n        Dictionary with field information containing:\n        - 'fields': list of field names to visualize\n        - 'max_values': dict mapping field names to their max values (optional)\n        - 'field_type': str, either 'node' or 'element' (default: 'node')\n    \"\"\"\n    if field_info is None:\n        field_info = {}\n\n    fields = field_info.get('fields', [])\n    max_values = field_info.get('max_values', {})\n    field_type = field_info.get('field_type', 'node')\n\n    # Default visualization settings\n    opt_content = \"\"\"// Gmsh visualization settings for mesh fields\n                        Mesh.SurfaceFaces = 0;       // Hide surface faces\n                        Mesh.SurfaceEdges = 0;       // Hide surface edges\n                        Mesh.Points = 0;             // Hide mesh points\n                        Mesh.Lines = 0;              // Hide mesh lines\n\n                        \"\"\"\n\n    # Configure each field\n    for i, field_name in enumerate(fields):\n        view_index = i + 1\n        max_value = max_values.get(field_name, 1.0)  # Default max value\n\n        opt_content += f\"\"\"// Make View[{view_index}] ({field_name}) visible\n                            View[{view_index}].Visible = 1;\n                            View[{view_index}].ColormapNumber = {i + 1};  // Use colormap {i + 1}\n                            View[{view_index}].RangeType = 2;       // Custom range\n                            View[{view_index}].CustomMin = 0;       // Minimum value\n                            View[{view_index}].CustomMax = {max_value};  // Maximum value\n                            View[{view_index}].ShowScale = 1;       // Show color scale\n\n                            // Add alpha/transparency based on value\n                            View[{view_index}].ColormapAlpha = 1;\n                            View[{view_index}].ColormapAlphaPower = 0.08;\n\n                            \"\"\"\n\n    # Add field information comments\n    opt_content += \"// Field information:\\n\"\n    for i, field_name in enumerate(fields):\n        max_value = max_values.get(field_name, 1.0)\n        opt_content += f\"// View[{i + 1}]: {field_name} field (max value: {max_value:.6f})\\n\"\n\n    # Write the .opt file\n    opt_path = f\"{mesh_path}.opt\"\n    with open(opt_path, 'w') as f:\n        f.write(opt_content)\n\n    return opt_path\n</code></pre>"},{"location":"reference/core/#calc-titcorecalc","title":"Calc (<code>tit.core.calc</code>)","text":"<p>Shared TI Field Calculation Utilities Used by optimization tools</p>"},{"location":"reference/core/#tit.core.calc.get_TI_vectors","title":"get_TI_vectors","text":"<pre><code>get_TI_vectors(E1_org, E2_org)\n</code></pre> <p>Calculate the temporal interference (TI) modulation amplitude vectors.</p> <p>This function implements the Grossman et al. 2017 algorithm for computing TI vectors that represent both the direction and magnitude of maximum modulation amplitude when two sinusoidal electric fields interfere.</p> <p>PHYSICAL INTERPRETATION: When two electric fields E1(t) = E1cos(2\u03c0f1t) and E2(t) = E2cos(2\u03c0f2t) with slightly different frequencies are applied simultaneously, they create a beating pattern. The TI vector indicates: - DIRECTION: Spatial direction of maximum envelope modulation - MAGNITUDE: Maximum envelope amplitude = 2 * effective_amplitude</p> <p>ALGORITHM (Grossman et al. 2017): 1. Preprocessing: Ensure |E1| \u2265 |E2| and acute angle \u03b1 &lt; \u03c0/2 2. Regime selection based on geometric relationship:    - Regime 1 (parallel): |E2| \u2264 |E1|cos(\u03b1) \u2192 TI = 2E2    - Regime 2 (oblique): |E2| &gt; |E1|cos(\u03b1) \u2192 TI = 2E2_perpendicular_to_h    where h = E1 - E2</p> <p>Parameters:</p> Name Type Description Default <code>E1_org</code> <code>(ndarray, shape(N, 3))</code> <p>Electric field vectors from electrode pair 1 [V/m]</p> required <code>E2_org</code> <code>(ndarray, shape(N, 3))</code> <p>Electric field vectors from electrode pair 2 [V/m]</p> required <p>Returns:</p> Name Type Description <code>TI_vectors</code> <code>(ndarray, shape(N, 3))</code> <p>TI modulation amplitude vectors [V/m] Direction: Maximum modulation direction Magnitude: Maximum envelope amplitude</p> References <p>Grossman, N. et al. (2017). Noninvasive Deep Brain Stimulation via Temporally Interfering Electric Fields. Cell, 169(6), 1029-1041.</p> Source code in <code>tit/core/calc.py</code> <pre><code>def get_TI_vectors(E1_org, E2_org):\n    \"\"\"\n    Calculate the temporal interference (TI) modulation amplitude vectors.\n\n    This function implements the Grossman et al. 2017 algorithm for computing\n    TI vectors that represent both the direction and magnitude of maximum\n    modulation amplitude when two sinusoidal electric fields interfere.\n\n    PHYSICAL INTERPRETATION:\n    When two electric fields E1(t) = E1*cos(2\u03c0f1*t) and E2(t) = E2*cos(2\u03c0f2*t)\n    with slightly different frequencies are applied simultaneously, they create\n    a beating pattern. The TI vector indicates:\n    - DIRECTION: Spatial direction of maximum envelope modulation\n    - MAGNITUDE: Maximum envelope amplitude = 2 * effective_amplitude\n\n    ALGORITHM (Grossman et al. 2017):\n    1. Preprocessing: Ensure |E1| \u2265 |E2| and acute angle \u03b1 &lt; \u03c0/2\n    2. Regime selection based on geometric relationship:\n       - Regime 1 (parallel): |E2| \u2264 |E1|cos(\u03b1) \u2192 TI = 2*E2\n       - Regime 2 (oblique): |E2| &gt; |E1|cos(\u03b1) \u2192 TI = 2*E2_perpendicular_to_h\n       where h = E1 - E2\n\n    Parameters\n    ----------\n    E1_org : np.ndarray, shape (N, 3)\n        Electric field vectors from electrode pair 1 [V/m]\n    E2_org : np.ndarray, shape (N, 3)\n        Electric field vectors from electrode pair 2 [V/m]\n\n    Returns\n    -------\n    TI_vectors : np.ndarray, shape (N, 3)\n        TI modulation amplitude vectors [V/m]\n        Direction: Maximum modulation direction\n        Magnitude: Maximum envelope amplitude\n\n    References\n    ----------\n    Grossman, N. et al. (2017). Noninvasive Deep Brain Stimulation via\n    Temporally Interfering Electric Fields. Cell, 169(6), 1029-1041.\n    \"\"\"\n    # Input validation\n    assert E1_org.shape == E2_org.shape, \"E1 and E2 must have same shape\"\n    assert E1_org.shape[1] == 3, \"Vectors must be 3D\"\n\n    # Work with copies to avoid modifying input arrays\n    E1 = E1_org.copy()\n    E2 = E2_org.copy()\n\n    # =================================================================\n    # PREPROCESSING STEP 1: Magnitude ordering |E1| \u2265 |E2|\n    # =================================================================\n    # Ensures consistency by always treating E1 as the \"stronger\" field\n    # This simplifies the subsequent regime analysis\n    idx_swap = np.linalg.norm(E2, axis=1) &gt; np.linalg.norm(E1, axis=1)\n    E1[idx_swap], E2[idx_swap] = E2[idx_swap], E1_org[idx_swap]\n\n    # =================================================================\n    # PREPROCESSING STEP 2: Acute angle constraint \u03b1 &lt; \u03c0/2\n    # =================================================================\n    # Ensures constructive interference by flipping E2 if dot product &lt; 0\n    # This avoids destructive interference scenarios\n    idx_flip = np.sum(E1 * E2, axis=1) &lt; 0\n    E2[idx_flip] = -E2[idx_flip]\n\n    # =================================================================\n    # GEOMETRIC PARAMETERS CALCULATION\n    # =================================================================\n    # Calculate field magnitudes and angle between vectors\n    normE1 = np.linalg.norm(E1, axis=1)\n    normE2 = np.linalg.norm(E2, axis=1)\n\n    # Safe cosine calculation to avoid division by zero and numerical errors\n    denom = normE1 * normE2\n    denom[denom == 0] = 1.0  # Prevent division by zero\n    cosalpha = np.clip(np.sum(E1 * E2, axis=1) / denom, -1.0, 1.0)\n\n    # =================================================================\n    # REGIME SELECTION CRITERION\n    # =================================================================\n    # Critical condition from Grossman 2017: |E2| \u2264 |E1| * cos(\u03b1)\n    # This determines whether E2 is \"small\" relative to E1's projection\n    regime1_mask = normE2 &lt;= normE1 * cosalpha\n\n    # Initialize output array\n    TI_vectors = np.zeros_like(E1)\n\n    # =================================================================\n    # REGIME 1: PARALLEL ALIGNMENT (|E2| \u2264 |E1| cos(\u03b1))\n    # =================================================================\n    # Physical interpretation: E2 is effectively \"contained\" within E1's projection\n    # The TI amplitude is determined entirely by E2's magnitude and direction\n    # Formula: TI = 2 * E2\n    TI_vectors[regime1_mask] = 2.0 * E2[regime1_mask]\n\n    # =================================================================\n    # REGIME 2: OBLIQUE CONFIGURATION (|E2| &gt; |E1| cos(\u03b1))\n    # =================================================================\n    # Physical interpretation: E2 has significant perpendicular component to E1\n    # The TI is determined by the component of E2 perpendicular to h = E1 - E2\n    # Formula: TI = 2 * E2_perpendicular_to_h\n    regime2_mask = ~regime1_mask\n    if np.any(regime2_mask):\n        # Calculate difference vector h = E1 - E2\n        h = E1[regime2_mask] - E2[regime2_mask]\n        h_norm = np.linalg.norm(h, axis=1)\n\n        # Handle degenerate case (h = 0) by setting unit norm\n        h_norm[h_norm == 0] = 1.0\n        e_h = h / h_norm[:, None]  # Unit vector along h\n\n        # Project E2 onto h, then subtract to get perpendicular component\n        # E2_perp = E2 - proj_h(E2) = E2 - (E2\u00b7\u0125)\u0125\n        E2_parallel_component = np.sum(E2[regime2_mask] * e_h, axis=1)[:, None] * e_h\n        E2_perp = E2[regime2_mask] - E2_parallel_component\n\n        # The TI vector in regime 2 is twice the perpendicular component\n        TI_vectors[regime2_mask] = 2.0 * E2_perp\n\n    return TI_vectors\n</code></pre>"},{"location":"reference/core/#tit.core.calc.get_mTI_vectors","title":"get_mTI_vectors","text":"<pre><code>get_mTI_vectors(E1_org, E2_org, E3_org, E4_org)\n</code></pre> <p>Calculate multi-temporal interference (mTI) vectors from four channel E-fields.</p> <p>This computes TI between channels 1 and 2 to get TI_A, TI between channels 3 and 4 to get TI_B, and finally TI between TI_A and TI_B to produce the mTI vector field.</p> <p>Parameters:</p> Name Type Description Default <code>E1_org</code> <code>(ndarray, shape(N, 3))</code> <p>Electric field vectors for channel 1</p> required <code>E2_org</code> <code>(ndarray, shape(N, 3))</code> <p>Electric field vectors for channel 2</p> required <code>E3_org</code> <code>(ndarray, shape(N, 3))</code> <p>Electric field vectors for channel 3</p> required <code>E4_org</code> <code>(ndarray, shape(N, 3))</code> <p>Electric field vectors for channel 4</p> required <p>Returns:</p> Name Type Description <code>mTI_vectors</code> <code>(ndarray, shape(N, 3))</code> <p>Multi-TI modulation amplitude vectors</p> Source code in <code>tit/core/calc.py</code> <pre><code>def get_mTI_vectors(E1_org, E2_org, E3_org, E4_org):\n    \"\"\"\n    Calculate multi-temporal interference (mTI) vectors from four channel E-fields.\n\n    This computes TI between channels 1 and 2 to get TI_A, TI between channels 3 and 4\n    to get TI_B, and finally TI between TI_A and TI_B to produce the mTI vector field.\n\n    Parameters\n    ----------\n    E1_org : np.ndarray, shape (N, 3)\n        Electric field vectors for channel 1\n    E2_org : np.ndarray, shape (N, 3)\n        Electric field vectors for channel 2\n    E3_org : np.ndarray, shape (N, 3)\n        Electric field vectors for channel 3\n    E4_org : np.ndarray, shape (N, 3)\n        Electric field vectors for channel 4\n\n    Returns\n    -------\n    mTI_vectors : np.ndarray, shape (N, 3)\n        Multi-TI modulation amplitude vectors\n    \"\"\"\n    # Validate shapes\n    for i, arr in enumerate([E1_org, E2_org, E3_org, E4_org], start=1):\n        if arr.ndim != 2 or arr.shape[1] != 3:\n            raise ValueError(f\"E{i}_org must have shape (N, 3), got {arr.shape}\")\n\n    if not (E1_org.shape == E2_org.shape == E3_org.shape == E4_org.shape):\n        raise ValueError(\n            \"All input arrays must have identical shapes. \"\n            f\"Got: {[E1_org.shape, E2_org.shape, E3_org.shape, E4_org.shape]}\"\n        )\n\n    # Step 1: TI between (E1, E2)\n    TI_A = get_TI_vectors(E1_org, E2_org)\n\n    # Step 2: TI between (E3, E4)\n    TI_B = get_TI_vectors(E3_org, E4_org)\n\n    # Step 3: TI between (TI_A, TI_B) \u2192 mTI\n    mTI_vectors = get_TI_vectors(TI_A, TI_B)\n\n    return mTI_vectors\n</code></pre>"},{"location":"reference/opt/","title":"Optimization (<code>tit.opt</code>)","text":"<p>TI-Toolbox optimization package</p>"},{"location":"reference/opt/#leadfield-titoptleadfield","title":"Leadfield (<code>tit.opt.leadfield</code>)","text":"<p>Leadfield matrix generator for optimization and other applications Integrates with SimNIBS to create leadfield matrices</p>"},{"location":"reference/opt/#tit.opt.leadfield.LeadfieldGenerator","title":"LeadfieldGenerator","text":"<pre><code>LeadfieldGenerator(subject_dir, electrode_cap='EEG10-10', progress_callback=None, termination_flag=None)\n</code></pre> <p>Generate and load leadfield matrices for TI optimization</p> <p>This class provides a unified interface for leadfield generation and management, supporting both HDF5 and NPY formats with consistent naming conventions.</p> <p>Initialize leadfield generator</p> <p>Args:     subject_dir: Path to subject directory (m2m folder) or subject_id     electrode_cap: Electrode cap type (e.g., 'EEG10-10', 'GSN-256')     progress_callback: Optional callback function(message, type) for progress updates     termination_flag: Optional callable that returns True if generation should be terminated</p> Source code in <code>tit/opt/leadfield.py</code> <pre><code>def __init__(self, subject_dir, electrode_cap='EEG10-10', progress_callback=None, termination_flag=None):\n    \"\"\"\n    Initialize leadfield generator\n\n    Args:\n        subject_dir: Path to subject directory (m2m folder) or subject_id\n        electrode_cap: Electrode cap type (e.g., 'EEG10-10', 'GSN-256')\n        progress_callback: Optional callback function(message, type) for progress updates\n        termination_flag: Optional callable that returns True if generation should be terminated\n    \"\"\"\n    self.subject_dir = Path(subject_dir)\n    self.electrode_cap = electrode_cap\n    self._progress_callback = progress_callback\n    self._termination_flag = termination_flag\n    self._simnibs_process = None\n\n    # Initialize PathManager\n    self.pm = get_path_manager()\n\n    # Extract subject_id from subject_dir path\n    self.subject_id = self.subject_dir.name.replace('m2m_', '')\n\n    # Initialize leadfield data attributes\n    self.lfm = None  # Leadfield matrix\n    self.positions = None  # Electrode positions\n\n    # Setup logger\n    if progress_callback is None:\n        pm = self.pm\n        # Prefer a project-scoped log file when the project dir is resolved.\n        # In unit tests, PathManager is frequently mocked, so avoid filesystem writes.\n        log_file: Optional[str] = None\n        project_dir = getattr(pm, \"project_dir\", None)\n        if isinstance(project_dir, str) and project_dir and os.path.isdir(project_dir):\n            logs_dir = os.path.join(project_dir, \"derivatives\", \"ti-toolbox\", \"logs\")\n            os.makedirs(logs_dir, exist_ok=True)\n            log_file = os.path.join(logs_dir, \"leadfield_generator.log\")\n\n        self.logger = logging_util.get_logger(\"LeadfieldGenerator\", log_file=log_file, overwrite=False, console=True)\n        logging_util.configure_external_loggers(['simnibs', 'mesh_io'], self.logger)\n    else:\n        self.logger = None\n</code></pre>"},{"location":"reference/opt/#tit.opt.leadfield.LeadfieldGenerator.cleanup_old_simulations","title":"cleanup_old_simulations","text":"<pre><code>cleanup_old_simulations()\n</code></pre> <p>Clean up old SimNIBS simulation files, temporary directories, and ROI mesh files.</p> Source code in <code>tit/opt/leadfield.py</code> <pre><code>def cleanup_old_simulations(self):\n    \"\"\"Clean up old SimNIBS simulation files, temporary directories, and ROI mesh files.\"\"\"\n    import glob\n    import shutil\n\n    self._log(\"Checking for old simulation files...\", 'info')\n\n    # Remove old simulation .mat files\n    old_sim_files = glob.glob(str(self.subject_dir / \"simnibs_simulation*.mat\"))\n    if old_sim_files:\n        self._log(f\"  Found {len(old_sim_files)} old simulation file(s), cleaning up...\", 'info')\n        for sim_file in old_sim_files:\n            try:\n                os.remove(sim_file)\n                self._log(f\"  Removed: {os.path.basename(sim_file)}\", 'info')\n            except Exception as e:\n                self._log(f\"  Warning: Could not remove {os.path.basename(sim_file)}: {e}\", 'warning')\n\n    # Remove temporary leadfield directory\n    temp_leadfield_dir = self.subject_dir / 'leadfield'\n    if temp_leadfield_dir.exists():\n        self._log(\"  Removing old temporary leadfield directory...\", 'info')\n        try:\n            shutil.rmtree(temp_leadfield_dir)\n            self._log(\"  Removed: leadfield/\", 'info')\n        except Exception as e:\n            self._log(f\"  Warning: Could not remove leadfield directory: {e}\", 'warning')\n\n    # Remove ROI mesh file\n    subject_id = self.subject_dir.name.replace('m2m_', '')\n    roi_file = self.subject_dir / f\"{subject_id}_ROI.msh\"\n    if roi_file.exists():\n        self._log(\"  Removing old ROI mesh file...\", 'info')\n        try:\n            os.remove(roi_file)\n            self._log(f\"  Removed: {roi_file.name}\", 'info')\n        except Exception as e:\n            self._log(f\"  Warning: Could not remove {roi_file.name}: {e}\", 'warning')\n</code></pre>"},{"location":"reference/opt/#tit.opt.leadfield.LeadfieldGenerator.generate_leadfield","title":"generate_leadfield","text":"<pre><code>generate_leadfield(output_dir=None, tissues=[1, 2], eeg_cap_path=None, cleanup=True)\n</code></pre> <p>Generate leadfield matrix using SimNIBS</p> <p>Args:     output_dir: Output directory for leadfield (default: subject_dir)     tissues: Tissue types to include [1=GM, 2=WM]     eeg_cap_path: Path to EEG cap CSV file (optional, will look in eeg_positions if not provided)     cleanup: Whether to clean up old simulation files before running (default: True)</p> <p>Returns:     dict: Dictionary with path {'hdf5': hdf5_path}</p> Source code in <code>tit/opt/leadfield.py</code> <pre><code>def generate_leadfield(self, output_dir=None, tissues=[1, 2], eeg_cap_path=None, cleanup=True):\n    \"\"\"\n    Generate leadfield matrix using SimNIBS\n\n    Args:\n        output_dir: Output directory for leadfield (default: subject_dir)\n        tissues: Tissue types to include [1=GM, 2=WM]\n        eeg_cap_path: Path to EEG cap CSV file (optional, will look in eeg_positions if not provided)\n        cleanup: Whether to clean up old simulation files before running (default: True)\n\n    Returns:\n        dict: Dictionary with path {'hdf5': hdf5_path}\n    \"\"\"\n    from simnibs import sim_struct\n    import simnibs\n\n    if output_dir is None:\n        # Use PathManager to get leadfield directory\n        output_dir = self.pm.path_optional(\"leadfields\", subject_id=self.subject_id)\n        if output_dir is None:\n            # Fallback to manual construction if PathManager doesn't find it\n            output_dir = self.subject_dir.parent / \"leadfields\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Clean up old simulation files in output directory\n    if cleanup:\n        self._cleanup_output_dir(output_dir)\n\n    # Setup SimNIBS leadfield calculation\n    tdcs_lf = sim_struct.TDCSLEADFIELD()\n\n    # Find mesh file - try multiple naming conventions\n    # Common patterns: {subject_id}.msh, m2m_{subject_id}.msh, {subject_dir_name}.msh\n    subject_id = self.subject_dir.name.replace('m2m_', '')  # Extract subject ID\n\n    possible_mesh_names = [\n        f\"{subject_id}.msh\",  # Most common: 101.msh\n        f\"{self.subject_dir.name}.msh\",  # m2m_101.msh\n        \"final.msh\",  # Sometimes used\n    ]\n\n    mesh_file = None\n    for mesh_name in possible_mesh_names:\n        candidate = self.subject_dir / mesh_name\n        if candidate.exists():\n            mesh_file = candidate\n            self._log(f\"Found mesh file: {mesh_file}\", 'info')\n            break\n\n    if mesh_file is None:\n        # List available .msh files to help debug\n        msh_files = list(self.subject_dir.glob(\"*.msh\"))\n        error_msg = f\"Mesh file not found in {self.subject_dir}\\n\"\n        error_msg += f\"Tried: {', '.join(possible_mesh_names)}\\n\"\n        if msh_files:\n            error_msg += f\"Available .msh files: {', '.join([f.name for f in msh_files])}\"\n        else:\n            error_msg += \"No .msh files found in directory\"\n        raise FileNotFoundError(error_msg)\n\n    tdcs_lf.fnamehead = str(mesh_file)\n    tdcs_lf.subpath = str(self.subject_dir)\n    tdcs_lf.pathfem = str(output_dir)\n    tdcs_lf.interpolation = None\n    tdcs_lf.map_to_surf = False\n    tdcs_lf.tissues = tissues\n\n    # Set EEG cap path if provided\n    if eeg_cap_path:\n        if not Path(eeg_cap_path).exists():\n            raise FileNotFoundError(f\"EEG cap file not found: {eeg_cap_path}\")\n        tdcs_lf.eeg_cap = str(eeg_cap_path)\n        self._log(f\"Using EEG cap: {Path(eeg_cap_path).name}\", 'info')\n    elif self.electrode_cap and self.electrode_cap != 'EEG10-10':\n        # Try to find in eeg_positions directory using PathManager\n        eeg_positions_dir = self.pm.path_optional(\"eeg_positions\", subject_id=self.subject_id)\n        if eeg_positions_dir and os.path.exists(eeg_positions_dir):\n            cap_file = Path(eeg_positions_dir) / f\"{self.electrode_cap}.csv\"\n            if cap_file.exists():\n                tdcs_lf.eeg_cap = str(cap_file)\n                self._log(f\"Found EEG cap: {cap_file.name}\", 'info')\n\n    # Clean up old files if requested\n    if cleanup:\n        self.cleanup_old_simulations()\n\n    self._log(f\"Generating leadfield matrix for {self.subject_dir.name}...\", 'info')\n    self._log(f\"Electrode cap: {self.electrode_cap if self.electrode_cap else 'Default'}\", 'info')\n    self._log(f\"Tissues: {tissues} (1=GM, 2=WM)\", 'info')\n    self._log(f\"Mesh file: {mesh_file.name}\", 'info')\n    self._log(\"Setting up SimNIBS leadfield calculation...\", 'info')\n\n    # Redirect SimNIBS output to GUI console via callback (not terminal)\n    import sys\n    from io import StringIO\n    import logging\n\n    # Suppress SimNIBS console output by redirecting stdout/stderr to StringIO\n    old_stdout = sys.stdout\n    old_stderr = sys.stderr\n    stdout_capture = StringIO()\n    stderr_capture = StringIO()\n    sys.stdout = stdout_capture\n    sys.stderr = stderr_capture\n\n    # Configure SimNIBS logger to use callback if available\n    simnibs_logger = logging.getLogger('simnibs')\n    old_simnibs_handlers = simnibs_logger.handlers[:]\n\n    if self._progress_callback:\n        # Remove console handlers from SimNIBS logger\n        logging_util.suppress_console_output(simnibs_logger)\n\n        # Add callback handler to redirect to GUI\n        logging_util.add_callback_handler(simnibs_logger, self._progress_callback, logging.INFO)\n\n    # Run SimNIBS with termination checks\n    simnibs_error = None\n    try:\n        # Check for termination before starting\n        if self._termination_flag and self._termination_flag():\n            self._log(\"Leadfield generation cancelled before starting\", 'warning')\n            raise InterruptedError(\"Leadfield generation was cancelled before starting\")\n\n        # Note: SimNIBS runs MPI processes that cannot be interrupted mid-execution\n        # The termination check will take effect after SimNIBS completes\n        self._log(\"Running SimNIBS (this cannot be interrupted mid-execution)...\", 'info')\n        simnibs.run_simnibs(tdcs_lf)\n\n        # Check for termination after SimNIBS finishes\n        if self._termination_flag and self._termination_flag():\n            self._log(\"Leadfield generation cancelled after SimNIBS execution\", 'warning')\n            raise InterruptedError(\"Leadfield generation was cancelled\")\n\n    except Exception as e:\n        simnibs_error = e\n    finally:\n        # Restore stdout/stderr\n        sys.stdout = old_stdout\n        sys.stderr = old_stderr\n\n        # Restore SimNIBS logger handlers\n        if self._progress_callback:\n            simnibs_logger.handlers = old_simnibs_handlers\n\n        # Send any captured stdout/stderr to callback (fallback for print statements)\n        if self._progress_callback:\n            stdout_text = stdout_capture.getvalue()\n            stderr_text = stderr_capture.getvalue()\n            if stdout_text.strip():\n                for line in stdout_text.strip().split('\\n'):\n                    if line.strip():\n                        self._log(line, 'info')\n            if stderr_text.strip():\n                for line in stderr_text.strip().split('\\n'):\n                    if line.strip():\n                        self._log(line, 'warning')\n\n    # Re-raise any error that occurred during SimNIBS run\n    if simnibs_error:\n        raise simnibs_error\n\n    self._log(\"SimNIBS leadfield computation completed\", 'info')\n\n    # Find generated HDF5 file\n    self._log(\"Processing generated leadfield files...\", 'info')\n    hdf5_files = list(output_dir.glob('*.hdf5'))\n    if not hdf5_files:\n        raise FileNotFoundError(f\"No HDF5 leadfield file found in {output_dir}\")\n\n    hdf5_path = hdf5_files[0]\n\n    # Use the filename that SimNIBS generated (simplified naming - no renaming)\n    self._log(f\"Leadfield generated: {hdf5_path}\", 'success')\n\n    result = {'hdf5': str(hdf5_path)}\n\n    return result\n</code></pre>"},{"location":"reference/opt/#tit.opt.leadfield.LeadfieldGenerator.get_electrode_names_from_cap","title":"get_electrode_names_from_cap","text":"<pre><code>get_electrode_names_from_cap(cap_name=None)\n</code></pre> <p>Extract electrode names from an EEG cap CSV file using simnibs csv_reader.</p> <p>Args:     cap_name: Name of EEG cap (will look in subject_dir/eeg_positions/).              If None, uses self.electrode_cap.</p> <p>Returns:     list: List of electrode names</p> Source code in <code>tit/opt/leadfield.py</code> <pre><code>def get_electrode_names_from_cap(self, cap_name=None):\n    \"\"\"\n    Extract electrode names from an EEG cap CSV file using simnibs csv_reader.\n\n    Args:\n        cap_name: Name of EEG cap (will look in subject_dir/eeg_positions/).\n                 If None, uses self.electrode_cap.\n\n    Returns:\n        list: List of electrode names\n    \"\"\"\n    if cap_name is None:\n        cap_name = self.electrode_cap\n\n    # Prefer SimNIBS reader when available; fall back to a lightweight CSV parse for tests.\n    try:\n        from simnibs.utils.csv_reader import eeg_positions  # type: ignore[import-not-found]\n\n        eeg_pos = eeg_positions(str(self.subject_dir), cap_name=cap_name)\n        electrodes = list(eeg_pos.keys())\n        return sorted(electrodes)\n    except Exception:\n        cap = str(cap_name)\n        if not cap.lower().endswith(\".csv\"):\n            cap = f\"{cap}.csv\"\n        cap_path = self.subject_dir / \"eeg_positions\" / cap\n        if not cap_path.exists():\n            raise OSError(f\"Could not find EEG cap file: {cap_path}\")\n\n        labels: List[str] = []\n        with cap_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                parts = [p.strip() for p in line.split(\",\")]\n                # expected SimNIBS format: Type,x,y,z,label\n                if len(parts) &lt; 2:\n                    continue\n                if parts[0] not in (\"Electrode\", \"ReferenceElectrode\"):\n                    continue\n                label = parts[-1].strip()\n                if label:\n                    labels.append(label)\n        # stable unique\n        return sorted(list(dict.fromkeys(labels)))\n</code></pre>"},{"location":"reference/opt/#tit.opt.leadfield.LeadfieldGenerator.list_available_leadfields","title":"list_available_leadfields","text":"<pre><code>list_available_leadfields(subject_id=None)\n</code></pre> <p>List available leadfield HDF5 files for a subject.</p> <p>Args:     subject_id: Subject ID (optional, will use self.subject_id if not provided)</p> <p>Returns:     list: List of tuples (net_name, hdf5_path, file_size_gb)</p> Source code in <code>tit/opt/leadfield.py</code> <pre><code>def list_available_leadfields(self, subject_id=None):\n    \"\"\"\n    List available leadfield HDF5 files for a subject.\n\n    Args:\n        subject_id: Subject ID (optional, will use self.subject_id if not provided)\n\n    Returns:\n        list: List of tuples (net_name, hdf5_path, file_size_gb)\n    \"\"\"\n    if subject_id is None:\n        subject_id = self.subject_id\n\n    # Use PathManager to get leadfield directory\n    leadfields_dir = self.pm.path_optional(\"leadfields\", subject_id=subject_id)\n\n    leadfields = []\n    if leadfields_dir and os.path.exists(leadfields_dir):\n        leadfields_dir = Path(leadfields_dir)\n        for item in leadfields_dir.iterdir():\n            # Look for files matching pattern: {net_name}_leadfield.hdf5\n            if item.is_file() and item.name.endswith(\"_leadfield.hdf5\"):\n                hdf5_file = item\n                if hdf5_file.exists():\n                    # Extract net name from filename pattern: {net_name}_leadfield.hdf5\n                    net_name = item.name.replace(\"_leadfield.hdf5\", \"\")\n                    # Get file size in GB\n                    try:\n                        file_size = hdf5_file.stat().st_size / (1024**3)  # GB\n                    except OSError:\n                        file_size = 0.0\n                    leadfields.append((net_name, str(hdf5_file), file_size))\n\n    return sorted(leadfields, key=lambda x: x[0])\n</code></pre>"},{"location":"reference/opt/#tit.opt.leadfield.LeadfieldGenerator.list_available_leadfields_hdf5","title":"list_available_leadfields_hdf5","text":"<pre><code>list_available_leadfields_hdf5(subject_id=None)\n</code></pre> <p>List available leadfield HDF5 files for a subject.</p> <p>Args:     subject_id: Subject ID (optional, will use self.subject_id if not provided)</p> <p>Returns:     list: List of tuples (net_name, hdf5_path, file_size_gb)</p> Source code in <code>tit/opt/leadfield.py</code> <pre><code>def list_available_leadfields_hdf5(self, subject_id=None):\n    \"\"\"\n    List available leadfield HDF5 files for a subject.\n\n    Args:\n        subject_id: Subject ID (optional, will use self.subject_id if not provided)\n\n    Returns:\n        list: List of tuples (net_name, hdf5_path, file_size_gb)\n    \"\"\"\n    if subject_id is None:\n        subject_id = self.subject_id\n\n    # Use PathManager to get leadfield directory\n    leadfields_dir = self.pm.path_optional(\"leadfields\", subject_id=subject_id)\n\n    leadfields = []\n    if leadfields_dir and os.path.exists(leadfields_dir):\n        leadfields_dir = Path(leadfields_dir)\n        for item in leadfields_dir.iterdir():\n            # Look for HDF5 files that contain \"leadfield\" in the name (flexible naming)\n            if item.is_file() and item.name.endswith(\".hdf5\") and \"leadfield\" in item.name.lower():\n                hdf5_file = item\n                if hdf5_file.exists():\n                    # Extract net name more flexibly - try different SimNIBS naming patterns\n                    filename = item.name\n\n                    # Try to extract net_name from various SimNIBS naming patterns\n                    if \"_leadfield_\" in filename:\n                        # Split by \"_leadfield_\" and take the part after it, remove .hdf5\n                        parts = filename.split(\"_leadfield_\")\n                        if len(parts) == 2:\n                            net_name = parts[1].replace(\".hdf5\", \"\")\n                        else:\n                            net_name = filename.replace(\"_leadfield_\", \"\").replace(\".hdf5\", \"\")\n                    elif filename.endswith(\"_leadfield.hdf5\"):\n                        # Standard pattern: {net_name}_leadfield.hdf5\n                        net_name = filename.replace(\"_leadfield.hdf5\", \"\")\n                    else:\n                        # Fallback: remove .hdf5 and try to clean up\n                        net_name = filename.replace(\".hdf5\", \"\")\n\n                    # Clean up the net_name (remove subject_id prefix if present)\n                    if net_name.startswith(f\"{subject_id}_\"):\n                        net_name = net_name.replace(f\"{subject_id}_\", \"\", 1)\n                    elif net_name.startswith(f\"{subject_id}\"):\n                        net_name = net_name.replace(f\"{subject_id}\", \"\", 1)\n\n                    # Clean up extra underscores and empty parts\n                    net_name = net_name.strip(\"_\")\n                    if not net_name:\n                        net_name = \"unknown\"\n\n                    # Get file size in GB\n                    try:\n                        file_size = hdf5_file.stat().st_size / (1024**3)  # GB\n                    except OSError:\n                        file_size = 0.0\n                    leadfields.append((net_name, str(hdf5_file), file_size))\n\n    return sorted(leadfields, key=lambda x: x[0])\n</code></pre>"},{"location":"reference/opt/#flex-search-titoptflex","title":"Flex search (<code>tit.opt.flex</code>)","text":"<p>Flex-search optimization package for TI stimulation.</p> <p>This package provides flexible optimization for temporal interference (TI) stimulation with support for different ROI definitions and optimization goals.</p> <p>Modules:     flex_config: Configuration and optimization setup     flex_log: Logging utilities and progress tracking     multi_start: Multi-start optimization logic and result management     flex: Main optimization orchestration script</p> <p>Note:     roi module is now located at core.roi (shared across optimization approaches)</p>"},{"location":"reference/opt/#main-api-titoptflexflex","title":"Main API (<code>tit.opt.flex.flex</code>)","text":"<p>Main flex-search script for TI stimulation optimization.</p> <p>This script orchestrates the flexible search optimization process for temporal interference (TI) stimulation. It supports: - Multiple ROI methods (spherical, atlas-based, subcortical) - Multiple optimization goals (mean, max, focality) - Multi-start optimization for robust results - Optional electrode mapping to EEG cap positions</p>"},{"location":"reference/opt/#tit.opt.flex.flex.main","title":"main","text":"<pre><code>main() -&gt; int\n</code></pre> <p>Main entry point for flex-search optimization.</p> <p>Returns:     Exit code (0 for success, 1 for failure)</p> Source code in <code>tit/opt/flex/flex.py</code> <pre><code>def main() -&gt; int:\n    \"\"\"Main entry point for flex-search optimization.\n\n    Returns:\n        Exit code (0 for success, 1 for failure)\n    \"\"\"\n    # Parse arguments\n    args = flex_config.parse_arguments()\n\n    # Track total session time\n    start_time = time.time()\n\n    # Multi-start optimization setup\n    n_multistart = args.n_multistart\n    optim_funvalue_list = np.zeros(n_multistart)\n\n    # Build base optimization to get the output folder structure\n\n    opt_base = flex_config.build_optimization(args)\n    base_output_folder = opt_base.output_folder\n\n    # Setup logger\n    logger = flex_log.setup_logger(base_output_folder, args.subject)\n\n    # Configure SimNIBS related loggers\n    configure_external_loggers(['simnibs', 'mesh_io', 'sim_struct', 'opt_struct'], logger)\n\n    logger.debug(f\"Base output directory: {base_output_folder}\")\n    logger.debug(f\"Command: {' '.join(sys.argv)}\")\n\n    if n_multistart &gt; 1:\n        logger.debug(f\"Running multi-start optimization with {n_multistart} runs\")\n    else:\n        logger.debug(\"Running single optimization\")\n\n    # Create output folder list for each run\n    output_folder_list = [\n        os.path.join(base_output_folder, f\"{i_opt:02d}\") \n        for i_opt in range(n_multistart)\n    ]\n\n    # Log optimization start\n    flex_log.log_optimization_start(\n        args.subject, args.goal, args.postproc, \n        args.roi_method, n_multistart, logger\n    )\n\n    # Run multiple optimizations\n    for i_opt in range(n_multistart):\n        run_start_time = time.time()\n\n        try:\n            # Build optimization for this specific run\n            opt = flex_config.build_optimization(args)\n            opt.output_folder = output_folder_list[i_opt]\n            os.makedirs(opt.output_folder, exist_ok=True)\n\n            # Log optimization parameters (only for first run)\n            if i_opt == 0:\n                flex_log.log_optimization_config(args, n_multistart, logger)\n\n            # Configure optimizer options\n            flex_config.configure_optimizer_options(opt, args, logger)\n\n            # Log run start\n            cpus_to_pass = args.cpus if args.cpus is not None else None\n            if n_multistart &gt; 1:\n                logger.debug(f\"Starting optimization run {i_opt + 1}/{n_multistart}...\")\n            else:\n                logger.debug(\"Starting optimization...\")\n\n            # Define step name for logging\n            step_name = f\"Optimization run {i_opt + 1}/{n_multistart}\" if n_multistart &gt; 1 else \"Optimization\"\n            flex_log.log_optimization_step_start(step_name, logger)\n\n            # Run optimization\n            optimization_start_time = time.time()\n            optim_funvalue_list[i_opt] = multi_start.run_single_optimization(\n                opt, cpus_to_pass, logger\n            )\n            optimization_end_time = time.time()\n\n            # Log results\n            if optim_funvalue_list[i_opt] != float('inf'):\n                optimization_duration = optimization_end_time - optimization_start_time\n                run_duration = time.time() - run_start_time\n                flex_log.log_run_details(\n                    i_opt, n_multistart, opt.output_folder, opt,\n                    optimization_duration, run_duration, logger\n                )\n                flex_log.log_optimization_step_complete(step_name, \"\", logger)\n\n                # Log final electrode simulation if enabled\n                run_final_sim = (\n                    args.run_final_electrode_simulation and \n                    not args.skip_final_electrode_simulation\n                )\n                if run_final_sim:\n                    flex_log.log_optimization_step_start(\"Final electrode simulation\", logger)\n                    flex_log.log_optimization_step_complete(\"Final electrode simulation\", \"\", logger)\n            else:\n                # Optimization failed\n                flex_log.log_optimization_step_failed(step_name, \"See logs for details\", logger)\n\n        except Exception as exc:\n            # Unexpected error\n            run_duration = time.time() - run_start_time\n            logger.error(f\"Unexpected error in run {i_opt + 1} after {run_duration:.1f} seconds: {exc}\")\n            optim_funvalue_list[i_opt] = float('inf')\n            flex_log.log_optimization_step_failed(\n                step_name, f\"Unexpected error: {type(exc).__name__}\", logger\n            )\n\n    # Post-processing\n    if n_multistart &gt; 1:\n        logger.debug(\"=\" * 80)\n        logger.debug(\"MULTI-START OPTIMIZATION POST-PROCESSING\")\n        logger.debug(\"=\" * 80)\n\n        flex_log.log_optimization_step_start(\"Post-processing\", logger)\n\n        # Select best solution\n        best_opt_idx, valid_runs, failed_runs = multi_start.select_best_solution(\n            optim_funvalue_list, n_multistart, logger\n        )\n\n        if best_opt_idx == -1:\n            flex_log.log_optimization_step_failed(\n                \"Post-processing\", \"No valid optimization results found\", logger\n            )\n            flex_log.log_optimization_complete(\n                args.subject, success=False, n_multistart=n_multistart, logger=logger\n            )\n            return 1\n\n        best_run_number = best_opt_idx + 1\n        best_folder = output_folder_list[best_opt_idx]\n\n        # Copy best solution\n        if not multi_start.copy_best_solution(best_folder, base_output_folder, logger):\n            flex_log.log_optimization_step_failed(\n                \"Post-processing\", \"Failed to copy best solution\", logger\n            )\n            flex_log.log_optimization_complete(\n                args.subject, success=False, n_multistart=n_multistart, logger=logger\n            )\n            return 1\n\n        # Create summary file\n        multistart_summary_file = os.path.join(base_output_folder, \"multistart_optimization_summary.txt\")\n        try:\n            multi_start.create_multistart_summary_file(\n                multistart_summary_file, args, n_multistart, \n                optim_funvalue_list, best_opt_idx, valid_runs, \n                failed_runs, start_time\n            )\n            logger.debug(f\"Multi-start summary saved to: {multistart_summary_file}\")\n        except Exception as e:\n            logger.warning(f\"Failed to create multi-start summary file: {e}\")\n\n        # Clean up\n        multi_start.cleanup_temporary_directories(\n            output_folder_list, n_multistart, logger\n        )\n\n        logger.debug(\"MULTI-START OPTIMIZATION COMPLETED SUCCESSFULLY\")\n        logger.debug(f\"Final results available in: {base_output_folder}\")\n\n        flex_log.log_optimization_step_complete(\n            \"Post-processing\", \n            f\"{len(valid_runs)}/{n_multistart} runs successful\", \n            logger\n        )\n\n        flex_log.log_optimization_complete(\n            args.subject, success=True, output_path=base_output_folder,\n            n_multistart=n_multistart, best_run=best_run_number, logger=logger\n        )\n\n    else:\n        # Single optimization run\n        if optim_funvalue_list[0] == float('inf'):\n            logger.error(\"Single optimization run failed\")\n            flex_log.log_optimization_complete(\n                args.subject, success=False, logger=logger\n            )\n            return 1\n        else:\n            logger.debug(\"SINGLE OPTIMIZATION COMPLETED SUCCESSFULLY\")\n            logger.debug(f\"Final function value: {optim_funvalue_list[0]:.6f}\")\n\n            single_run_folder = output_folder_list[0]\n\n            logger.debug(\"FINALIZING RESULTS:\")\n            logger.debug(f\"Moving results from: {single_run_folder}\")\n            logger.debug(f\"Moving results to: {base_output_folder}\")\n\n            # Copy results\n            if not multi_start.copy_best_solution(single_run_folder, base_output_folder, logger):\n                flex_log.log_optimization_complete(\n                    args.subject, success=False, logger=logger\n                )\n                return 1\n\n            # Create summary file\n            single_summary_file = os.path.join(base_output_folder, \"optimization_summary.txt\")\n            try:\n                multi_start.create_single_optimization_summary_file(\n                    single_summary_file, args, optim_funvalue_list[0], start_time\n                )\n                logger.debug(f\"Optimization summary saved to: {single_summary_file}\")\n            except Exception as e:\n                logger.warning(f\"Failed to create optimization summary file: {e}\")\n\n            # Clean up\n            time.sleep(0.1)\n            logger.debug(\"CLEANING UP TEMPORARY DIRECTORY:\")\n\n            for attempt in range(2):\n                try:\n                    if os.path.exists(single_run_folder):\n                        shutil.rmtree(single_run_folder)\n                    logger.debug(\"\u2713 Removed temporary directory\")\n                    break\n                except Exception as exc:\n                    if attempt == 0:\n                        time.sleep(0.2)\n                        continue\n                    else:\n                        logger.warning(f\"\u2717 Failed to remove temporary directory: {single_run_folder} - {exc}\")\n                        logger.warning(\"\u26a0 Temporary directory could not be removed (results still valid)\")\n\n            logger.debug(f\"Results available in: {base_output_folder}\")\n\n            flex_log.log_optimization_complete(\n                args.subject, success=True, output_path=base_output_folder,\n                n_multistart=n_multistart, logger=logger\n            )\n\n    # Log session footer\n    total_duration = time.time() - start_time\n    flex_log.log_session_footer(args.subject, n_multistart, total_duration, logger)\n\n    return 0\n</code></pre>"},{"location":"reference/opt/#config-titoptflexflex_config","title":"Config (<code>tit.opt.flex.flex_config</code>)","text":"<p>Configuration and optimization setup for flex-search.</p> <p>This module handles: - Argument parsing - Optimization object configuration - Electrode setup - Output directory structure</p>"},{"location":"reference/opt/#tit.opt.flex.flex_config.build_optimization","title":"build_optimization","text":"<pre><code>build_optimization(args: argparse.Namespace) -&gt; opt_struct.TesFlexOptimization\n</code></pre> <p>Set up optimization object with all parameters.</p> <p>Args:     args: Parsed command line arguments</p> <p>Returns:     Configured SimNIBS optimization object</p> <p>Raises:     SystemExit: If required environment variables or files are missing</p> Source code in <code>tit/opt/flex/flex_config.py</code> <pre><code>def build_optimization(args: argparse.Namespace) -&gt; opt_struct.TesFlexOptimization:\n    \"\"\"Set up optimization object with all parameters.\n\n    Args:\n        args: Parsed command line arguments\n\n    Returns:\n        Configured SimNIBS optimization object\n\n    Raises:\n        SystemExit: If required environment variables or files are missing\n    \"\"\"\n    opt = opt_struct.TesFlexOptimization()\n\n    # Docker-first: resolve paths via PathManager templates (centralized conventions).\n    from tit.core import get_path_manager\n\n    pm = get_path_manager()\n    opt.subpath = pm.path(\"m2m\", subject_id=args.subject)\n    opt.output_folder = pm.path(\"flex_search_run\", subject_id=args.subject, search_name=utils.roi_dirname(args))\n    os.makedirs(opt.output_folder, exist_ok=True)\n\n    # Configure goals and thresholds\n    opt.goal = args.goal\n    if args.goal == \"focality\":\n        # Allow \"dynamic\" focality thresholds:\n        # - If --thresholds is omitted (None/empty) or explicitly set to \"dynamic\"/\"auto\",\n        #   do NOT set opt.threshold and let SimNIBS handle threshold adaptation.\n        thr_raw = (args.thresholds or \"\").strip()\n        if thr_raw and thr_raw.lower() not in {\"dynamic\", \"auto\"}:\n            try:\n                vals = [float(v) for v in thr_raw.split(\",\")]\n            except Exception as exc:\n                raise SystemExit(f\"Invalid --thresholds value: {args.thresholds!r}. Expected float(s) or 'dynamic'.\") from exc\n            opt.threshold = vals if len(vals) &gt; 1 else vals[0]\n        if not args.non_roi_method:\n            raise SystemExit(\"--non-roi-method required for focality goal\")\n\n    opt.e_postproc = args.postproc\n    opt.open_in_gmsh = False  # Never auto-launch GUI\n\n    # Final electrode simulation control\n    opt.run_final_electrode_simulation = (\n        args.run_final_electrode_simulation and\n        not args.skip_final_electrode_simulation\n    )\n\n    # Detailed results control\n    if hasattr(args, 'detailed_results') and args.detailed_results:\n        opt.detailed_results = True\n\n    # Skin visualization control\n    if hasattr(args, 'visualize_valid_skin_region') and args.visualize_valid_skin_region:\n        opt.visualize_valid_skin_region = True\n\n    # Configure mapping\n    if args.enable_mapping:\n        opt.map_to_net_electrodes = True\n        eeg_dir = pm.path(\"eeg_positions\", subject_id=args.subject)\n        opt.net_electrode_file = os.path.join(eeg_dir, f\"{args.eeg_net}.csv\")\n        if not os.path.isfile(opt.net_electrode_file):\n            raise SystemExit(f\"EEG net file not found: {opt.net_electrode_file}\")\n        if hasattr(opt, \"run_mapped_electrodes_simulation\") and not args.disable_mapping_simulation:\n            opt.run_mapped_electrodes_simulation = True\n    else:\n        # Initialize electrode_mapping to None when mapping is disabled\n        # This prevents AttributeError in SimNIBS logging code\n        opt.electrode_mapping = None\n\n    # Configure skin visualization net file (separate from mapping)\n    if hasattr(args, 'skin_visualization_net') and args.skin_visualization_net:\n        opt.net_electrode_file = args.skin_visualization_net\n        if not os.path.isfile(opt.net_electrode_file):\n            raise SystemExit(f\"Skin visualization EEG net file not found: {opt.net_electrode_file}\")\n\n    # Configure electrodes\n    c_A = args.current / 1000.0  # mA \u2192 A\n    electrode_shape = args.electrode_shape\n    dimensions = [float(x) for x in args.dimensions.split(',')]\n    thickness = args.thickness\n\n    # Calculate effective radius from dimensions for ElectrodeArrayPair layout\n    # For circular electrodes, use average of dimensions; for rectangular, use max dimension\n    if electrode_shape == \"ellipse\":\n        effective_radius = (dimensions[0] + dimensions[1]) / 4.0  # Average dimension / 2\n    else:  # rectangle\n        effective_radius = max(dimensions) / 2.0  # Max dimension / 2\n\n    # Create electrode pairs for TI stimulation (2 pairs)\n    electrode_pairs = []\n    for _ in range(2):  # Two pairs for TI\n        electrode_pair = ElectrodeArrayPair()\n\n        # Set electrode shape and dimensions for plotting\n        if electrode_shape == \"ellipse\":\n            electrode_pair.radius = [effective_radius]\n            electrode_pair.dimensions = [dimensions[0], dimensions[1]]\n        else:  # rectangle\n            electrode_pair.radius = [0]  # No radius for rectangular\n            electrode_pair.length_x = [dimensions[0]]\n            electrode_pair.length_y = [dimensions[1]]\n\n        electrode_pair.current = [c_A, -c_A]\n        electrode_pairs.append(electrode_pair)\n\n    # Add to optimization\n    opt.electrode = electrode_pairs\n\n    # Configure ROI\n    utils.configure_roi(opt, args)\n\n    return opt\n</code></pre>"},{"location":"reference/opt/#tit.opt.flex.flex_config.configure_optimizer_options","title":"configure_optimizer_options","text":"<pre><code>configure_optimizer_options(opt: opt_struct.TesFlexOptimization, args: argparse.Namespace, logger) -&gt; None\n</code></pre> <p>Configure optimizer options for the optimization object.</p> <p>Args:     opt: SimNIBS optimization object     args: Parsed command line arguments     logger: Logger instance</p> Source code in <code>tit/opt/flex/flex_config.py</code> <pre><code>def configure_optimizer_options(\n    opt: opt_struct.TesFlexOptimization,\n    args: argparse.Namespace,\n    logger\n) -&gt; None:\n    \"\"\"Configure optimizer options for the optimization object.\n\n    Args:\n        opt: SimNIBS optimization object\n        args: Parsed command line arguments\n        logger: Logger instance\n    \"\"\"\n    # Check if optimizer options exist\n    if not hasattr(opt, '_optimizer_options_std') or not isinstance(opt._optimizer_options_std, dict):\n        logger.warning(\"opt._optimizer_options_std not found or not a dict, cannot configure optimizer options.\")\n        return\n\n    # Apply max_iterations if provided\n    if args.max_iterations is not None:\n        opt._optimizer_options_std[\"maxiter\"] = args.max_iterations\n        logger.debug(f\"Set max iterations to {args.max_iterations}\")\n\n    # Apply population_size if provided\n    if args.population_size is not None:\n        opt._optimizer_options_std[\"popsize\"] = args.population_size\n        logger.debug(f\"Set population size to {args.population_size}\")\n\n    # Apply tolerance if provided\n    if hasattr(args, 'tolerance') and args.tolerance is not None:\n        opt._optimizer_options_std[\"tol\"] = args.tolerance\n        logger.debug(f\"Set tolerance to {args.tolerance}\")\n\n    # Apply mutation if provided\n    if hasattr(args, 'mutation') and args.mutation is not None:\n        # Parse mutation parameter - can be single value or min,max range\n        mutation_str = args.mutation.strip()\n        if ',' in mutation_str:\n            # Parse as [min, max] range\n            try:\n                mutation_parts = [float(x.strip()) for x in mutation_str.split(',')]\n                if len(mutation_parts) == 2:\n                    opt._optimizer_options_std[\"mutation\"] = mutation_parts\n                    logger.debug(f\"Set mutation to {mutation_parts}\")\n                else:\n                    logger.warning(f\"Invalid mutation format: {mutation_str}. Expected single value or 'min,max'\")\n            except ValueError as e:\n                logger.warning(f\"Failed to parse mutation parameter '{mutation_str}': {e}\")\n        else:\n            # Parse as single value\n            try:\n                mutation_val = float(mutation_str)\n                opt._optimizer_options_std[\"mutation\"] = mutation_val\n                logger.debug(f\"Set mutation to {mutation_val}\")\n            except ValueError as e:\n                logger.warning(f\"Failed to parse mutation parameter '{mutation_str}': {e}\")\n\n    # Apply recombination if provided\n    if hasattr(args, 'recombination') and args.recombination is not None:\n        opt._optimizer_options_std[\"recombination\"] = args.recombination\n        logger.debug(f\"Set recombination to {args.recombination}\")\n</code></pre>"},{"location":"reference/opt/#tit.opt.flex.flex_config.parse_arguments","title":"parse_arguments","text":"<pre><code>parse_arguments() -&gt; argparse.Namespace\n</code></pre> <p>Parse command line arguments.</p> <p>Returns:     Parsed arguments namespace</p> Source code in <code>tit/opt/flex/flex_config.py</code> <pre><code>def parse_arguments() -&gt; argparse.Namespace:\n    \"\"\"Parse command line arguments.\n\n    Returns:\n        Parsed arguments namespace\n    \"\"\"\n    p = argparse.ArgumentParser(\n        prog=\"flex-search\",\n        description=\"Optimise TI stimulation and (optionally) map final \"\n                    \"electrodes to the nearest EEG-net nodes.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    # Core parameters\n    p.add_argument(\"--subject\", \"-sub\", required=True, help=\"Subject ID\")\n    p.add_argument(\"--goal\", choices=[\"mean\", \"max\", \"focality\"], required=True,\n                   help=\"Optimization goal\")\n    p.add_argument(\"--postproc\", choices=[\"max_TI\", \"dir_TI_normal\", \"dir_TI_tangential\"],\n                   required=True, help=\"Post-processing method\")\n    p.add_argument(\"--eeg-net\", \"-eeg\",\n                   help=\"CSV filename in eeg_positions (without .csv). Required when --enable-mapping is used.\")\n    p.add_argument(\"--current\", type=float, required=True,\n                   help=\"Electrode current in mA\")\n    p.add_argument(\"--electrode-shape\", choices=[\"rect\", \"ellipse\"], required=True,\n                   help=\"Electrode shape (rect or ellipse)\")\n    p.add_argument(\"--dimensions\", type=str, required=True,\n                   help=\"Electrode dimensions in mm (x,y format, e.g., '8,8')\")\n    p.add_argument(\"--thickness\", type=float, required=True,\n                   help=\"Electrode thickness in mm\")\n    p.add_argument(\"--roi-method\", choices=[\"spherical\", \"atlas\", \"subcortical\"],\n                   required=True, help=\"ROI definition method\")\n\n    # Focality-specific arguments\n    p.add_argument(\"--thresholds\",\n                   help=\"Focality threshold(s). Provide a single value or two comma-separated values. \"\n                        \"If omitted (or set to 'dynamic'), SimNIBS will use its dynamic thresholding/adaptation.\")\n    p.add_argument(\"--non-roi-method\", choices=[\"everything_else\", \"specific\"],\n                   help=\"Non-ROI definition method (required for focality goal)\")\n\n    # Mapping (disabled by default)\n    p.add_argument(\"--enable-mapping\", action=\"store_true\",\n                   help=\"Map optimal electrodes to nearest EEG-net nodes\")\n    p.add_argument(\"--disable-mapping-simulation\", action=\"store_true\",\n                   help=\"Skip extra simulation with mapped electrodes\")\n\n    # Output control\n    p.add_argument(\"--run-final-electrode-simulation\", action=\"store_true\", default=False,\n                   help=\"Run final simulation with optimal electrodes (default: False)\")\n    p.add_argument(\"--skip-final-electrode-simulation\", action=\"store_true\",\n                   help=\"Skip final simulation with optimal electrodes\")\n\n    # Stability and performance arguments\n    p.add_argument(\"--n-multistart\", type=int, default=1,\n                   help=\"Number of optimization runs (multi-start). Best result will be kept.\")\n    p.add_argument(\"--max-iterations\", type=int,\n                   help=\"Maximum optimization iterations for differential_evolution\")\n    p.add_argument(\"--population-size\", type=int,\n                   help=\"Population size for differential_evolution\")\n    p.add_argument(\"--cpus\", type=int,\n                   help=\"Number of CPU cores to utilize\")\n\n    # Differential evolution optimizer parameters\n    p.add_argument(\"--tolerance\", type=float,\n                   help=\"Tolerance for differential_evolution convergence (tol parameter)\")\n    p.add_argument(\"--mutation\", type=str,\n                   help=\"Mutation parameter for differential_evolution (single value or 'min,max' range)\")\n    p.add_argument(\"--recombination\", type=float,\n                   help=\"Recombination parameter for differential_evolution\")\n\n    # Output control\n    p.add_argument(\"--detailed-results\", action=\"store_true\",\n                   help=\"Enable detailed results output (creates additional visualization and debug files)\")\n    p.add_argument(\"--visualize-valid-skin-region\", action=\"store_true\",\n                   help=\"Create visualizations of valid skin region for electrode placement (requires --detailed-results)\")\n    p.add_argument(\"--skin-visualization-net\",\n                   help=\"EEG net CSV file to use for skin visualization (shows electrode positions on valid/invalid skin regions)\")\n\n    return p.parse_args()\n</code></pre>"},{"location":"reference/opt/#ex-search-titoptex","title":"Ex search (<code>tit.opt.ex</code>)","text":"<p>TI Exhaustive Search Module</p> <p>A streamlined implementation for TI exhaustive search simulations.</p>"},{"location":"reference/opt/#tit.opt.ex.calculate_total_combinations","title":"calculate_total_combinations","text":"<pre><code>calculate_total_combinations(e1_plus, e1_minus, e2_plus, e2_minus, current_ratios, all_combinations)\n</code></pre> <p>Calculate total number of montage combinations to be tested.</p> <p>Args:     e1_plus (list): E1+ electrode names     e1_minus (list): E1- electrode names     e2_plus (list): E2+ electrode names     e2_minus (list): E2- electrode names     current_ratios (list): List of (ch1_current, ch2_current) tuples     all_combinations (bool): If True, test all valid electrode combinations</p> <p>Returns:     int: Total number of combinations to test</p> Source code in <code>tit/opt/ex/logic.py</code> <pre><code>def calculate_total_combinations(e1_plus, e1_minus, e2_plus, e2_minus, current_ratios, all_combinations):\n    \"\"\"Calculate total number of montage combinations to be tested.\n\n    Args:\n        e1_plus (list): E1+ electrode names\n        e1_minus (list): E1- electrode names\n        e2_plus (list): E2+ electrode names\n        e2_minus (list): E2- electrode names\n        current_ratios (list): List of (ch1_current, ch2_current) tuples\n        all_combinations (bool): If True, test all valid electrode combinations\n\n    Returns:\n        int: Total number of combinations to test\n    \"\"\"\n    if all_combinations:\n        electrode_combinations = [(e1p, e1m, e2p, e2m) for e1p, e1m, e2p, e2m in product(e1_plus, repeat=4)\n                                 if len(set([e1p, e1m, e2p, e2m])) == 4]\n        return len(electrode_combinations) * len(current_ratios)\n    return len(e1_plus) * len(e1_minus) * len(e2_plus) * len(e2_minus) * len(current_ratios)\n</code></pre>"},{"location":"reference/opt/#tit.opt.ex.create_roi_from_coordinates","title":"create_roi_from_coordinates","text":"<pre><code>create_roi_from_coordinates(subject_id: str, roi_name: str, x: float, y: float, z: float) -&gt; Tuple[bool, str]\n</code></pre> <p>Create an ROI from custom coordinates.</p> <p>Args:     subject_id: Subject identifier     roi_name: Name for the ROI file (without .csv extension)     x, y, z: Coordinates in subject space (RAS)</p> <p>Returns:     Tuple of (success: bool, message: str)</p> Source code in <code>tit/opt/ex/roi_utils.py</code> <pre><code>def create_roi_from_coordinates(\n    subject_id: str,\n    roi_name: str,\n    x: float,\n    y: float,\n    z: float\n) -&gt; Tuple[bool, str]:\n    \"\"\"\n    Create an ROI from custom coordinates.\n\n    Args:\n        subject_id: Subject identifier\n        roi_name: Name for the ROI file (without .csv extension)\n        x, y, z: Coordinates in subject space (RAS)\n\n    Returns:\n        Tuple of (success: bool, message: str)\n    \"\"\"\n    try:\n        pm = get_path_manager()\n        m2m_dir = pm.path(\"m2m\", subject_id=subject_id)\n        roi_dir = pm.path(\"m2m_rois\", subject_id=subject_id)\n        os.makedirs(roi_dir, exist_ok=True)\n\n        # Ensure .csv extension\n        if not roi_name.endswith('.csv'):\n            roi_name += '.csv'\n\n        roi_file = Path(roi_dir) / roi_name\n\n        # Save ROI file\n        ROICoordinateHelper.save_roi_to_csv([x, y, z], str(roi_file))\n\n        # Update roi_list.txt\n        _update_roi_list_file(roi_dir, roi_name)\n\n        return True, f\"ROI '{roi_name}' created successfully at ({x:.2f}, {y:.2f}, {z:.2f})\"\n\n    except Exception as e:\n        return False, f\"Failed to create ROI: {str(e)}\"\n</code></pre>"},{"location":"reference/opt/#tit.opt.ex.create_roi_from_preset","title":"create_roi_from_preset","text":"<pre><code>create_roi_from_preset(subject_id: str, roi_name: str, preset_key: str, presets: Optional[Dict[str, Dict]] = None) -&gt; Tuple[bool, str]\n</code></pre> <p>Create an ROI from a preset.</p> <p>Args:     subject_id: Subject identifier     roi_name: Name for the ROI file (without .csv extension)     preset_key: Key of the preset to use     presets: Optional preset dictionary (will load if not provided)</p> <p>Returns:     Tuple of (success: bool, message: str)</p> Source code in <code>tit/opt/ex/roi_utils.py</code> <pre><code>def create_roi_from_preset(\n    subject_id: str,\n    roi_name: str,\n    preset_key: str,\n    presets: Optional[Dict[str, Dict]] = None\n) -&gt; Tuple[bool, str]:\n    \"\"\"\n    Create an ROI from a preset.\n\n    Args:\n        subject_id: Subject identifier\n        roi_name: Name for the ROI file (without .csv extension)\n        preset_key: Key of the preset to use\n        presets: Optional preset dictionary (will load if not provided)\n\n    Returns:\n        Tuple of (success: bool, message: str)\n    \"\"\"\n    if presets is None:\n        presets = load_roi_presets()\n\n    if preset_key not in presets:\n        return False, f\"Preset '{preset_key}' not found\"\n\n    try:\n        pm = get_path_manager()\n        m2m_dir = pm.path(\"m2m\", subject_id=subject_id)\n        roi_dir = pm.path(\"m2m_rois\", subject_id=subject_id)\n        os.makedirs(roi_dir, exist_ok=True)\n\n        # Ensure .csv extension\n        if not roi_name.endswith('.csv'):\n            roi_name += '.csv'\n\n        roi_file = Path(roi_dir) / roi_name\n\n        # Get MNI coordinates from preset\n        mni_coords = presets[preset_key]['mni']\n\n        # Transform to subject space\n        subject_coords = ROICoordinateHelper.transform_mni_to_subject(mni_coords, m2m_dir)\n        x, y, z = subject_coords[0], subject_coords[1], subject_coords[2]\n\n        # Save ROI file\n        ROICoordinateHelper.save_roi_to_csv([x, y, z], str(roi_file))\n\n        # Update roi_list.txt\n        _update_roi_list_file(roi_dir, roi_name)\n\n        return True, f\"ROI '{roi_name}' created successfully at ({x:.2f}, {y:.2f}, {z:.2f})\"\n\n    except Exception as e:\n        return False, f\"Failed to create ROI: {str(e)}\"\n</code></pre>"},{"location":"reference/opt/#tit.opt.ex.delete_roi","title":"delete_roi","text":"<pre><code>delete_roi(subject_id: str, roi_name: str) -&gt; Tuple[bool, str]\n</code></pre> <p>Delete an ROI file.</p> <p>Args:     subject_id: Subject identifier     roi_name: Name of the ROI file to delete</p> <p>Returns:     Tuple of (success: bool, message: str)</p> Source code in <code>tit/opt/ex/roi_utils.py</code> <pre><code>def delete_roi(subject_id: str, roi_name: str) -&gt; Tuple[bool, str]:\n    \"\"\"\n    Delete an ROI file.\n\n    Args:\n        subject_id: Subject identifier\n        roi_name: Name of the ROI file to delete\n\n    Returns:\n        Tuple of (success: bool, message: str)\n    \"\"\"\n    try:\n        pm = get_path_manager()\n        m2m_dir = pm.path(\"m2m\", subject_id=subject_id)\n        roi_dir = pm.path(\"m2m_rois\", subject_id=subject_id)\n\n        # Ensure .csv extension\n        if not roi_name.endswith('.csv'):\n            roi_name += '.csv'\n\n        roi_file = Path(roi_dir) / roi_name\n        roi_list_file = Path(roi_dir) / \"roi_list.txt\"\n\n        # Remove the ROI file\n        if roi_file.exists():\n            roi_file.unlink()\n\n        # Update roi_list.txt\n        if roi_list_file.exists():\n            with open(roi_list_file, 'r') as f:\n                existing_rois = [line.strip() for line in f.readlines()]\n\n            if roi_name in existing_rois:\n                existing_rois.remove(roi_name)\n\n                with open(roi_list_file, 'w') as f:\n                    for roi in existing_rois:\n                        f.write(f\"{roi}\\n\")\n\n        return True, f\"ROI '{roi_name}' deleted successfully\"\n\n    except Exception as e:\n        return False, f\"Failed to delete ROI: {str(e)}\"\n</code></pre>"},{"location":"reference/opt/#tit.opt.ex.generate_current_ratios","title":"generate_current_ratios","text":"<pre><code>generate_current_ratios(total_current, current_step, channel_limit)\n</code></pre> <p>Generate valid current ratio combinations for TI stimulation.</p> <p>Args:     total_current (float): Total current in milliamps     current_step (float): Step size for current increments in milliamps     channel_limit (float): Maximum current per channel in milliamps</p> <p>Returns:     tuple: (ratios, channel_limit_exceeded)         - ratios: List of (ch1_current, ch2_current) tuples         - channel_limit_exceeded: Boolean indicating if limit was exceeded</p> Source code in <code>tit/opt/ex/logic.py</code> <pre><code>def generate_current_ratios(total_current, current_step, channel_limit):\n    \"\"\"Generate valid current ratio combinations for TI stimulation.\n\n    Args:\n        total_current (float): Total current in milliamps\n        current_step (float): Step size for current increments in milliamps\n        channel_limit (float): Maximum current per channel in milliamps\n\n    Returns:\n        tuple: (ratios, channel_limit_exceeded)\n            - ratios: List of (ch1_current, ch2_current) tuples\n            - channel_limit_exceeded: Boolean indicating if limit was exceeded\n    \"\"\"\n    ratios, epsilon = [], current_step * 0.01\n    min_current = max(total_current - channel_limit, current_step)\n    if min_current &lt; current_step - epsilon:\n        min_current = current_step\n        channel_limit_exceeded = True\n    else:\n        channel_limit_exceeded = False\n\n    current_ch1 = channel_limit\n    while current_ch1 &gt;= min_current - epsilon:\n        current_ch2 = total_current - current_ch1\n        if (current_ch1 &lt;= channel_limit + epsilon and\n            current_ch2 &lt;= channel_limit + epsilon and\n            current_ch1 &gt;= current_step - epsilon and\n            current_ch2 &gt;= current_step - epsilon):\n            ratios.append((current_ch1, current_ch2))\n        current_ch1 -= current_step\n\n    return ratios, channel_limit_exceeded\n</code></pre>"},{"location":"reference/opt/#tit.opt.ex.generate_montage_combinations","title":"generate_montage_combinations","text":"<pre><code>generate_montage_combinations(e1_plus, e1_minus, e2_plus, e2_minus, current_ratios, all_combinations)\n</code></pre> <p>Generate electrode montage combinations for testing.</p> <p>Args:     e1_plus (list): E1+ electrode names     e1_minus (list): E1- electrode names     e2_plus (list): E2+ electrode names     e2_minus (list): E2- electrode names     current_ratios (list): List of (ch1_current, ch2_current) tuples     all_combinations (bool): If True, generate all valid electrode combinations</p> <p>Yields:     tuple: (e1_plus, e1_minus, e2_plus, e2_minus, current_ch1, current_ch2)</p> Source code in <code>tit/opt/ex/logic.py</code> <pre><code>def generate_montage_combinations(e1_plus, e1_minus, e2_plus, e2_minus, current_ratios, all_combinations):\n    \"\"\"Generate electrode montage combinations for testing.\n\n    Args:\n        e1_plus (list): E1+ electrode names\n        e1_minus (list): E1- electrode names\n        e2_plus (list): E2+ electrode names\n        e2_minus (list): E2- electrode names\n        current_ratios (list): List of (ch1_current, ch2_current) tuples\n        all_combinations (bool): If True, generate all valid electrode combinations\n\n    Yields:\n        tuple: (e1_plus, e1_minus, e2_plus, e2_minus, current_ch1, current_ch2)\n    \"\"\"\n    if all_combinations:\n        electrode_combinations = [(e1p, e1m, e2p, e2m) for e1p, e1m, e2p, e2m in product(e1_plus, repeat=4)\n                                 if len(set([e1p, e1m, e2p, e2m])) == 4]\n        for electrode_combo in electrode_combinations:\n            for current_ratio in current_ratios:\n                yield (*electrode_combo, current_ratio)\n    else:\n        for combo in product(e1_plus, e1_minus, e2_plus, e2_minus, current_ratios):\n            yield combo\n</code></pre>"},{"location":"reference/opt/#tit.opt.ex.get_available_rois","title":"get_available_rois","text":"<pre><code>get_available_rois(subject_id: str) -&gt; List[str]\n</code></pre> <p>Get list of available ROIs for a subject.</p> Source code in <code>tit/opt/ex/roi_utils.py</code> <pre><code>def get_available_rois(subject_id: str) -&gt; List[str]:\n    \"\"\"Get list of available ROIs for a subject.\"\"\"\n    pm = get_path_manager()\n    roi_dir = pm.path_optional(\"m2m_rois\", subject_id=subject_id) or \"\"\n\n    roi_files = []\n    for p in Path(roi_dir).glob(\"*.csv\"):\n        roi_files.append(p.name)\n\n    return sorted(roi_files)\n</code></pre>"},{"location":"reference/opt/#tit.opt.ex.get_roi_coordinates","title":"get_roi_coordinates","text":"<pre><code>get_roi_coordinates(subject_id: str, roi_name: str) -&gt; Optional[Tuple[float, float, float]]\n</code></pre> <p>Get coordinates for an ROI.</p> <p>Args:     subject_id: Subject identifier     roi_name: Name of the ROI file</p> <p>Returns:     Tuple of (x, y, z) coordinates or None if not found</p> Source code in <code>tit/opt/ex/roi_utils.py</code> <pre><code>def get_roi_coordinates(subject_id: str, roi_name: str) -&gt; Optional[Tuple[float, float, float]]:\n    \"\"\"\n    Get coordinates for an ROI.\n\n    Args:\n        subject_id: Subject identifier\n        roi_name: Name of the ROI file\n\n    Returns:\n        Tuple of (x, y, z) coordinates or None if not found\n    \"\"\"\n    try:\n        pm = get_path_manager()\n        m2m_dir = pm.path(\"m2m\", subject_id=subject_id)\n        roi_dir = pm.path(\"m2m_rois\", subject_id=subject_id)\n\n        # Ensure .csv extension\n        if not roi_name.endswith('.csv'):\n            roi_name += '.csv'\n\n        roi_file = Path(roi_dir) / roi_name\n\n        coords = ROICoordinateHelper.load_roi_from_csv(str(roi_file))\n        if coords is not None:\n            return (float(coords[0]), float(coords[1]), float(coords[2]))\n\n        return None\n\n    except Exception:\n        return None\n</code></pre>"},{"location":"reference/opt/#tit.opt.ex.load_roi_presets","title":"load_roi_presets","text":"<pre><code>load_roi_presets() -&gt; Dict[str, Dict]\n</code></pre> <p>Load ROI presets from the roi_presets.json file.</p> Source code in <code>tit/opt/ex/roi_utils.py</code> <pre><code>def load_roi_presets() -&gt; Dict[str, Dict]:\n    \"\"\"Load ROI presets from the roi_presets.json file.\"\"\"\n    # Try multiple possible locations for the presets file\n    possible_paths = [\n        Path(__file__).parent.parent / \"roi_presets.json\",  # tit/opt/roi_presets.json\n        Path(__file__).parent.parent.parent / \"resources\" / \"roi_presets.json\",  # resources/roi_presets.json\n    ]\n\n    for preset_path in possible_paths:\n        if preset_path.exists():\n            try:\n                with open(preset_path, 'r') as f:\n                    data = json.load(f)\n                    return data.get('regions', {})\n            except (json.JSONDecodeError, KeyError):\n                continue\n\n    # Return empty dict if no presets found\n    return {}\n</code></pre>"},{"location":"reference/opt/#main-titoptexmain","title":"Main (<code>tit.opt.ex.main</code>)","text":""},{"location":"reference/opt/#runner-titoptexrunner","title":"Runner (<code>tit.opt.ex.runner</code>)","text":""},{"location":"reference/sim/","title":"Simulation (<code>tit.sim</code>)","text":"<p>Temporal Interference (TI) Simulation Module</p> <p>This module provides a unified interface for running TI and mTI simulations. It automatically detects the simulation type based on montage configuration.</p>"},{"location":"reference/sim/#tit.sim.ConductivityType","title":"ConductivityType","text":"<p>               Bases: <code>Enum</code></p> <p>SimNIBS conductivity type enumeration.</p>"},{"location":"reference/sim/#tit.sim.ElectrodeConfig","title":"ElectrodeConfig  <code>dataclass</code>","text":"<pre><code>ElectrodeConfig(shape: str = 'ellipse', dimensions: List[float] = (lambda: [8.0, 8.0])(), thickness: float = 4.0, sponge_thickness: float = 2.0)\n</code></pre> <p>Configuration for electrode properties.</p>"},{"location":"reference/sim/#tit.sim.IntensityConfig","title":"IntensityConfig  <code>dataclass</code>","text":"<pre><code>IntensityConfig(pair1: float = 1.0, pair2: float = 1.0, pair3: float = 1.0, pair4: float = 1.0)\n</code></pre> <p>Configuration for current intensities in TI simulations.</p> <p>Each pair requires one intensity value (in mA). SimNIBS automatically applies equal and opposite currents to the two electrodes in each pair. For example: pair1=2.0 means electrode1=+2.0mA and electrode2=-2.0mA</p> <p>TI mode (2 pairs): Uses pair1 and pair2 mTI mode (4 pairs): Uses pair1, pair2, pair3, and pair4</p>"},{"location":"reference/sim/#tit.sim.IntensityConfig.from_string","title":"from_string  <code>classmethod</code>","text":"<pre><code>from_string(intensity_str: str) -&gt; IntensityConfig\n</code></pre> <p>Parse intensity from string format.</p> <p>Formats: - \"2.0\" -&gt; all pairs: 2.0 mA - \"2.0,1.5\" -&gt; pair1: 2.0, pair2: 1.5 (both set to 1.0 for pair3/pair4) - \"2.0,1.5,1.0,0.5\" -&gt; pair1: 2.0, pair2: 1.5, pair3: 1.0, pair4: 0.5</p> <p>Args:     intensity_str: Comma-separated intensity values</p> <p>Returns:     IntensityConfig object</p> Source code in <code>tit/sim/config.py</code> <pre><code>@classmethod\ndef from_string(cls, intensity_str: str) -&gt; 'IntensityConfig':\n    \"\"\"\n    Parse intensity from string format.\n\n    Formats:\n    - \"2.0\" -&gt; all pairs: 2.0 mA\n    - \"2.0,1.5\" -&gt; pair1: 2.0, pair2: 1.5 (both set to 1.0 for pair3/pair4)\n    - \"2.0,1.5,1.0,0.5\" -&gt; pair1: 2.0, pair2: 1.5, pair3: 1.0, pair4: 0.5\n\n    Args:\n        intensity_str: Comma-separated intensity values\n\n    Returns:\n        IntensityConfig object\n    \"\"\"\n    intensities = [float(x.strip()) for x in intensity_str.split(',')]\n\n    if len(intensities) == 1:\n        # Single value: use for all pairs\n        val = intensities[0]\n        return cls(val, val, val, val)\n    elif len(intensities) == 2:\n        # Two values: pair1, pair2 (TI mode)\n        return cls(intensities[0], intensities[1], 1.0, 1.0)\n    elif len(intensities) == 4:\n        # Four values: all pairs specified (mTI mode)\n        return cls(*intensities)\n    else:\n        raise ValueError(\n            f\"Invalid intensity format: {intensity_str}. \"\n            f\"Expected 1, 2, or 4 comma-separated values.\"\n        )\n</code></pre>"},{"location":"reference/sim/#tit.sim.MontageConfig","title":"MontageConfig  <code>dataclass</code>","text":"<pre><code>MontageConfig(name: str, electrode_pairs: List[Tuple[Union[str, List[float]], Union[str, List[float]]]], is_xyz: bool = False, eeg_net: Optional[str] = None)\n</code></pre> <p>Configuration for a single montage.</p>"},{"location":"reference/sim/#tit.sim.MontageConfig.num_pairs","title":"num_pairs  <code>property</code>","text":"<pre><code>num_pairs: int\n</code></pre> <p>Get number of electrode pairs.</p>"},{"location":"reference/sim/#tit.sim.MontageConfig.simulation_mode","title":"simulation_mode  <code>property</code>","text":"<pre><code>simulation_mode: SimulationMode\n</code></pre> <p>Determine simulation mode based on number of electrode pairs.</p>"},{"location":"reference/sim/#tit.sim.ParallelConfig","title":"ParallelConfig  <code>dataclass</code>","text":"<pre><code>ParallelConfig(enabled: bool = False, max_workers: int = 0)\n</code></pre> <p>Configuration for parallel simulation execution.</p>"},{"location":"reference/sim/#tit.sim.ParallelConfig.effective_workers","title":"effective_workers  <code>property</code>","text":"<pre><code>effective_workers: int\n</code></pre> <p>Get the effective number of workers.</p>"},{"location":"reference/sim/#tit.sim.ParallelConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Set max_workers to sensible default if auto-detect.</p> Source code in <code>tit/sim/config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Set max_workers to sensible default if auto-detect.\"\"\"\n    if self.max_workers &lt;= 0:\n        # Use half of available CPUs (simulations are memory-intensive)\n        cpu_count = os.cpu_count() or 4\n        # Limit to max 4 workers by default (memory constraint)\n        self.max_workers = min(4, max(1, cpu_count // 2))\n</code></pre>"},{"location":"reference/sim/#tit.sim.ParallelConfig.get_memory_warning","title":"get_memory_warning","text":"<pre><code>get_memory_warning() -&gt; Optional[str]\n</code></pre> <p>Return memory warning if parallel execution may cause issues.</p> Source code in <code>tit/sim/config.py</code> <pre><code>def get_memory_warning(self) -&gt; Optional[str]:\n    \"\"\"Return memory warning if parallel execution may cause issues.\"\"\"\n    if not self.enabled:\n        return None\n    if self.max_workers &gt; 2:\n        return (\n            f\"\u26a0\ufe0f Running {self.max_workers} parallel simulations may require \"\n            f\"significant memory (~4-8 GB per simulation). Consider reducing \"\n            f\"workers if you experience memory issues.\"\n        )\n    return None\n</code></pre>"},{"location":"reference/sim/#tit.sim.PostProcessor","title":"PostProcessor","text":"<pre><code>PostProcessor(subject_id: str, conductivity_type: str, m2m_dir: str, logger)\n</code></pre> <p>Post-processor for TI simulation results.</p> <p>Initialize post-processor.</p> <p>Args:     subject_id: Subject identifier     conductivity_type: Conductivity type string     m2m_dir: Path to m2m directory     logger: Logger instance</p> Source code in <code>tit/sim/post_processor.py</code> <pre><code>def __init__(\n    self,\n    subject_id: str,\n    conductivity_type: str,\n    m2m_dir: str,\n    logger\n):\n    \"\"\"\n    Initialize post-processor.\n\n    Args:\n        subject_id: Subject identifier\n        conductivity_type: Conductivity type string\n        m2m_dir: Path to m2m directory\n        logger: Logger instance\n    \"\"\"\n    self.subject_id = subject_id\n    self.conductivity_type = conductivity_type\n    self.m2m_dir = m2m_dir\n    self.logger = logger\n\n    # Path to tools directory\n    self.tools_dir = os.path.join(os.path.dirname(__file__), '..', 'tools')\n</code></pre>"},{"location":"reference/sim/#tit.sim.PostProcessor.process_mti_results","title":"process_mti_results","text":"<pre><code>process_mti_results(hf_dir: str, ti_dir: str, mti_dir: str, mti_nifti_dir: str, hf_mesh_dir: str, hf_analysis_dir: str, documentation_dir: str, montage_name: str) -&gt; str\n</code></pre> <p>Process 4-pair mTI simulation results with full pipeline.</p> <p>Args:     hf_dir: High-frequency output directory     ti_dir: TI intermediate output directory     mti_dir: mTI final output directory     mti_nifti_dir: mTI NIfTI output directory     hf_mesh_dir: High-frequency mesh output directory     hf_analysis_dir: High-frequency analysis output directory     documentation_dir: Documentation output directory     montage_name: Montage name</p> <p>Returns:     Path to output mTI mesh file</p> Source code in <code>tit/sim/post_processor.py</code> <pre><code>def process_mti_results(\n    self,\n    hf_dir: str,\n    ti_dir: str,\n    mti_dir: str,\n    mti_nifti_dir: str,\n    hf_mesh_dir: str,\n    hf_analysis_dir: str,\n    documentation_dir: str,\n    montage_name: str\n) -&gt; str:\n    \"\"\"\n    Process 4-pair mTI simulation results with full pipeline.\n\n    Args:\n        hf_dir: High-frequency output directory\n        ti_dir: TI intermediate output directory\n        mti_dir: mTI final output directory\n        mti_nifti_dir: mTI NIfTI output directory\n        hf_mesh_dir: High-frequency mesh output directory\n        hf_analysis_dir: High-frequency analysis output directory\n        documentation_dir: Documentation output directory\n        montage_name: Montage name\n\n    Returns:\n        Path to output mTI mesh file\n    \"\"\"\n    self.logger.info(f\"Processing mTI results for {montage_name}\")\n\n    # Step 1: Load 4 HF meshes\n    hf_meshes = []\n    for i in range(1, 5):\n        mesh_file = os.path.join(hf_dir, f\"{self.subject_id}_TDCS_{i}_{self.conductivity_type}.msh\")\n\n        if not os.path.exists(mesh_file):\n            raise FileNotFoundError(f\"Mesh file not found: {mesh_file}\")\n\n        m = mesh_io.read_msh(mesh_file)\n        tags_keep = np.hstack((np.arange(1, 100), np.arange(1001, 1100)))\n        m = m.crop_mesh(tags=tags_keep)\n        hf_meshes.append(m)\n\n    # Step 2: Calculate TI pairs (AB and CD)\n    ti_ab_vectors = get_TI_vectors(hf_meshes[0].field[\"E\"].value, hf_meshes[1].field[\"E\"].value)\n    ti_cd_vectors = get_TI_vectors(hf_meshes[2].field[\"E\"].value, hf_meshes[3].field[\"E\"].value)\n\n    # Step 3: Save intermediate TI meshes\n    self._save_ti_intermediate(hf_meshes[0], ti_ab_vectors, ti_dir, f\"{montage_name}_TI_AB.msh\")\n    self._save_ti_intermediate(hf_meshes[0], ti_cd_vectors, ti_dir, f\"{montage_name}_TI_CD.msh\")\n\n    # Step 4: Calculate and save final mTI\n    mti_field = TI.get_maxTI(ti_ab_vectors, ti_cd_vectors)\n    mout = deepcopy(hf_meshes[0])\n    mout.elmdata = []\n    mout.add_element_field(mti_field, \"TI_Max\")\n\n    mti_path = os.path.join(mti_dir, f\"{montage_name}_mTI.msh\")\n    mesh_io.write_msh(mout, mti_path)\n    mout.view(visible_tags=[1002, 1006], visible_fields=\"TI_Max\").write_opt(mti_path)\n\n    # Step 5: Extract GM/WM fields for mTI\n    self.logger.info(\"Field extraction: Started\")\n    self._extract_fields(mti_path, mti_dir, f\"{montage_name}_mTI\")\n    self.logger.info(\"Field extraction: \u2713 Complete\")\n\n    # Step 6: Extract GM/WM fields for intermediate TI meshes\n    ti_ab_path = os.path.join(ti_dir, f\"{montage_name}_TI_AB.msh\")\n    ti_cd_path = os.path.join(ti_dir, f\"{montage_name}_TI_CD.msh\")\n    if os.path.exists(ti_ab_path):\n        self._extract_fields(ti_ab_path, ti_dir, f\"{montage_name}_TI_AB\")\n    if os.path.exists(ti_cd_path):\n        self._extract_fields(ti_cd_path, ti_dir, f\"{montage_name}_TI_CD\")\n\n    # Step 7: Convert mTI meshes to NIfTI\n    self.logger.info(\"NIfTI transformation: Started\")\n    self._transform_to_nifti(mti_dir, mti_nifti_dir)\n    self.logger.info(\"NIfTI transformation: \u2713 Complete\")\n\n    # Step 8: Organize HF files with mTI naming\n    self._organize_mti_files(\n        hf_dir=hf_dir,\n        hf_mesh_dir=hf_mesh_dir,\n        hf_analysis_dir=hf_analysis_dir,\n        documentation_dir=documentation_dir\n    )\n\n    # Step 9: Convert T1 to MNI space\n    self._convert_t1_to_mni()\n\n    self.logger.info(f\"Saved mTI mesh: {mti_path}\")\n    return mti_path\n</code></pre>"},{"location":"reference/sim/#tit.sim.PostProcessor.process_ti_results","title":"process_ti_results","text":"<pre><code>process_ti_results(hf_dir: str, output_dir: str, nifti_dir: str, surface_overlays_dir: str, hf_mesh_dir: str, hf_nifti_dir: str, hf_analysis_dir: str, documentation_dir: str, montage_name: str) -&gt; str\n</code></pre> <p>Process 2-pair TI simulation results with full pipeline.</p> <p>Args:     hf_dir: High-frequency output directory (SimNIBS writes here)     output_dir: TI mesh output directory     nifti_dir: TI NIfTI output directory     surface_overlays_dir: Surface overlays output directory     hf_mesh_dir: High-frequency mesh output directory     hf_nifti_dir: High-frequency NIfTI output directory     hf_analysis_dir: High-frequency analysis output directory     documentation_dir: Documentation output directory     montage_name: Montage name</p> <p>Returns:     Path to output TI mesh file</p> Source code in <code>tit/sim/post_processor.py</code> <pre><code>def process_ti_results(\n    self,\n    hf_dir: str,\n    output_dir: str,\n    nifti_dir: str,\n    surface_overlays_dir: str,\n    hf_mesh_dir: str,\n    hf_nifti_dir: str,\n    hf_analysis_dir: str,\n    documentation_dir: str,\n    montage_name: str\n) -&gt; str:\n    \"\"\"\n    Process 2-pair TI simulation results with full pipeline.\n\n    Args:\n        hf_dir: High-frequency output directory (SimNIBS writes here)\n        output_dir: TI mesh output directory\n        nifti_dir: TI NIfTI output directory\n        surface_overlays_dir: Surface overlays output directory\n        hf_mesh_dir: High-frequency mesh output directory\n        hf_nifti_dir: High-frequency NIfTI output directory\n        hf_analysis_dir: High-frequency analysis output directory\n        documentation_dir: Documentation output directory\n        montage_name: Montage name\n\n    Returns:\n        Path to output TI mesh file\n    \"\"\"\n    self.logger.info(f\"Processing TI results for {montage_name}\")\n\n    # Step 1: Calculate TI field\n    ti_path = self._calculate_ti_field(hf_dir, output_dir, montage_name)\n\n    # Step 2: Calculate TI normal (cortical surface)\n    self._process_ti_normal(hf_dir, output_dir, montage_name)\n\n    # Step 3: Extract GM/WM fields\n    self.logger.info(\"Field extraction: Started\")\n    self._extract_fields(ti_path, output_dir, f\"{montage_name}_TI\")\n    self.logger.info(\"Field extraction: \u2713 Complete\")\n\n    # Step 4: Convert to NIfTI\n    self.logger.info(\"NIfTI transformation: Started\")\n    self._transform_to_nifti(output_dir, nifti_dir)\n    self.logger.info(\"NIfTI transformation: \u2713 Complete\")\n\n    # Step 5: Organize files\n    self._organize_ti_files(\n        hf_dir=hf_dir,\n        hf_mesh_dir=hf_mesh_dir,\n        hf_nifti_dir=hf_nifti_dir,\n        hf_analysis_dir=hf_analysis_dir,\n        surface_overlays_dir=surface_overlays_dir,\n        documentation_dir=documentation_dir\n    )\n\n    # Step 6: Convert T1 to MNI space\n    self._convert_t1_to_mni()\n\n    self.logger.info(f\"Saved TI mesh: {ti_path}\")\n    return ti_path\n</code></pre>"},{"location":"reference/sim/#tit.sim.SessionBuilder","title":"SessionBuilder","text":"<pre><code>SessionBuilder(config: SimulationConfig)\n</code></pre> <p>Builder class for constructing SimNIBS SESSION objects.</p> <p>Initialize session builder.</p> <p>Args:     config: Simulation configuration</p> Source code in <code>tit/sim/session_builder.py</code> <pre><code>def __init__(self, config: SimulationConfig):\n    \"\"\"\n    Initialize session builder.\n\n    Args:\n        config: Simulation configuration\n    \"\"\"\n    self.config = config\n    self.pm = get_path_manager()\n\n    # Setup paths\n    self.m2m_dir = self.pm.path(\"m2m\", subject_id=config.subject_id)\n    self.mesh_file = os.path.join(self.m2m_dir, f\"{config.subject_id}.msh\")\n    self.tensor_file = os.path.join(self.m2m_dir, \"DTI_coregT1_tensor.nii.gz\")\n</code></pre>"},{"location":"reference/sim/#tit.sim.SessionBuilder.build_session","title":"build_session","text":"<pre><code>build_session(montage: MontageConfig, output_dir: str) -&gt; sim_struct.SESSION\n</code></pre> <p>Build SimNIBS SESSION object for a montage.</p> <p>Args:     montage: Montage configuration     output_dir: Output directory for simulation results</p> <p>Returns:     Configured SESSION object</p> Source code in <code>tit/sim/session_builder.py</code> <pre><code>def build_session(\n    self,\n    montage: MontageConfig,\n    output_dir: str\n) -&gt; sim_struct.SESSION:\n    \"\"\"\n    Build SimNIBS SESSION object for a montage.\n\n    Args:\n        montage: Montage configuration\n        output_dir: Output directory for simulation results\n\n    Returns:\n        Configured SESSION object\n    \"\"\"\n    # Create base session\n    S = sim_struct.SESSION()\n    S.subpath = self.m2m_dir\n    S.fnamehead = self.mesh_file\n    S.anisotropy_type = self.config.conductivity_type.value\n    S.pathfem = output_dir\n\n    # Set EEG cap if using electrode names (not XYZ)\n    if not montage.is_xyz:\n        eeg_net = montage.eeg_net or self.config.eeg_net\n        S.eeg_cap = os.path.join(self.pm.path(\"eeg_positions\", subject_id=self.config.subject_id), eeg_net)\n\n    # Mapping options\n    S.map_to_surf = self.config.map_to_surf\n    S.map_to_vol = self.config.map_to_vol\n    S.map_to_mni = self.config.map_to_mni\n    S.map_to_fsavg = self.config.map_to_fsavg\n    S.open_in_gmsh = self.config.open_in_gmsh\n    S.tissues_in_niftis = self.config.tissues_in_niftis\n\n    # DTI tensor for anisotropic conductivity\n    if os.path.exists(self.tensor_file):\n        S.dti_nii = self.tensor_file\n\n    # Add electrode pairs based on simulation mode\n    if montage.simulation_mode == SimulationMode.TI:\n        self._add_ti_pairs(S, montage)\n    elif montage.simulation_mode == SimulationMode.MTI:\n        self._add_mti_pairs(S, montage)\n\n    return S\n</code></pre>"},{"location":"reference/sim/#tit.sim.SimulationConfig","title":"SimulationConfig  <code>dataclass</code>","text":"<pre><code>SimulationConfig(subject_id: str, project_dir: str, conductivity_type: ConductivityType, intensities: IntensityConfig, electrode: ElectrodeConfig, eeg_net: str = 'GSN-HydroCel-185.csv', map_to_surf: bool = True, map_to_vol: bool = True, map_to_mni: bool = True, map_to_fsavg: bool = False, tissues_in_niftis: str = 'all', open_in_gmsh: bool = False, parallel: ParallelConfig = ParallelConfig())\n</code></pre> <p>Main configuration for TI simulation.</p>"},{"location":"reference/sim/#tit.sim.SimulationConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Convert string conductivity type to enum if needed.</p> Source code in <code>tit/sim/config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Convert string conductivity type to enum if needed.\"\"\"\n    if isinstance(self.conductivity_type, str):\n        self.conductivity_type = ConductivityType(self.conductivity_type)\n    if isinstance(self.parallel, dict):\n        self.parallel = ParallelConfig(**self.parallel)\n</code></pre>"},{"location":"reference/sim/#tit.sim.SimulationMode","title":"SimulationMode","text":"<p>               Bases: <code>Enum</code></p> <p>Simulation mode enumeration.</p>"},{"location":"reference/sim/#tit.sim.load_montages","title":"load_montages","text":"<pre><code>load_montages(montage_names: List[str], project_dir: str, eeg_net: str, include_flex: bool = True) -&gt; List[MontageConfig]\n</code></pre> <p>Load all montages (regular + flex).</p> <p>Args:     montage_names: List of montage names to load     project_dir: Project directory path     eeg_net: EEG net name     include_flex: Whether to include flex montages</p> <p>Returns:     List of MontageConfig objects</p> Source code in <code>tit/sim/montage_loader.py</code> <pre><code>def load_montages(\n    montage_names: List[str],\n    project_dir: str,\n    eeg_net: str,\n    include_flex: bool = True\n) -&gt; List[MontageConfig]:\n    \"\"\"\n    Load all montages (regular + flex).\n\n    Args:\n        montage_names: List of montage names to load\n        project_dir: Project directory path\n        eeg_net: EEG net name\n        include_flex: Whether to include flex montages\n\n    Returns:\n        List of MontageConfig objects\n    \"\"\"\n    montages = []\n\n    # Load regular montages\n    net_montages = load_montage_file(project_dir, eeg_net)\n\n    for name in montage_names:\n        # Try multi_polar first, then uni_polar\n        montage_data = net_montages.get('multi_polar_montages', {}).get(name)\n        if not montage_data:\n            montage_data = net_montages.get('uni_polar_montages', {}).get(name)\n\n        if montage_data:\n            # Determine if freehand mode (XYZ coordinates)\n            is_xyz = eeg_net in [\"freehand\", \"flex_mode\"]\n\n            montages.append(MontageConfig(\n                name=name,\n                electrode_pairs=montage_data,\n                is_xyz=is_xyz,\n                eeg_net=eeg_net\n            ))\n\n    # Load flex montages\n    if include_flex:\n        flex_montages = load_flex_montages()\n        for flex_data in flex_montages:\n            try:\n                montages.append(parse_flex_montage(flex_data))\n            except Exception as e:\n                print(f\"Warning: Failed to parse flex montage: {e}\")\n\n    return montages\n</code></pre>"},{"location":"reference/sim/#tit.sim.run_montage_visualization","title":"run_montage_visualization","text":"<pre><code>run_montage_visualization(montage_name: str, simulation_mode: SimulationMode, eeg_net: str, output_dir: str, project_dir: str, logger, electrode_pairs: Optional[List] = None) -&gt; bool\n</code></pre> <p>Run montage visualization using visualize-montage.sh.</p> <p>Args:     montage_name: Name of the montage     simulation_mode: TI or MTI mode     eeg_net: EEG net name     output_dir: Output directory for montage images     project_dir: Project directory path     logger: Logger instance     electrode_pairs: Optional list of electrode pairs for direct visualization</p> <p>Returns:     True if successful, False otherwise</p> Source code in <code>tit/sim/simulator.py</code> <pre><code>def run_montage_visualization(\n    montage_name: str,\n    simulation_mode: SimulationMode,\n    eeg_net: str,\n    output_dir: str,\n    project_dir: str,\n    logger,\n    electrode_pairs: Optional[List] = None\n) -&gt; bool:\n    \"\"\"\n    Run montage visualization using visualize-montage.sh.\n\n    Args:\n        montage_name: Name of the montage\n        simulation_mode: TI or MTI mode\n        eeg_net: EEG net name\n        output_dir: Output directory for montage images\n        project_dir: Project directory path\n        logger: Logger instance\n        electrode_pairs: Optional list of electrode pairs for direct visualization\n\n    Returns:\n        True if successful, False otherwise\n    \"\"\"\n    # Skip visualization for freehand/flex modes with XYZ coordinates (no electrode_pairs provided)\n    if eeg_net in [\"freehand\", \"flex_mode\"] and not electrode_pairs:\n        logger.info(f\"Skipping montage visualization for {eeg_net} mode\")\n        return True\n\n    # Determine sim_mode string for visualize-montage.sh\n    sim_mode_str = \"U\" if simulation_mode == SimulationMode.TI else \"M\"\n\n    # Path to visualize-montage.sh\n    script_dir = os.path.dirname(__file__)\n    visualize_script = os.path.join(script_dir, \"visualize-montage.sh\")\n\n    if not os.path.exists(visualize_script):\n        logger.warning(f\"Visualize montage script not found: {visualize_script}\")\n        return True  # Non-fatal, continue without visualization\n\n    logger.info(f\"Visualizing montage: {montage_name}\")\n\n    try:\n        # Set PROJECT_DIR_NAME environment variable for the script\n        env = os.environ.copy()\n        project_dir_name = os.path.basename(project_dir)\n        env['PROJECT_DIR_NAME'] = project_dir_name\n\n        # Build command\n        cmd = ['bash', visualize_script, sim_mode_str, eeg_net, output_dir]\n\n        if electrode_pairs:\n            # For flex montages with known electrode pairs, pass --pairs\n            pairs_str = \",\".join([f\"{pair[0]}-{pair[1]}\" for pair in electrode_pairs])\n            pairs_arg = f\"{montage_name}:{pairs_str}\"\n            cmd.extend(['--pairs', pairs_arg])\n        else:\n            # For standard montages, pass montage name\n            cmd.insert(2, montage_name)  # Insert after script path and sim_mode\n\n        result = subprocess.run(\n            cmd,\n            env=env,\n            capture_output=True,\n            text=True,\n            timeout=120\n        )\n\n        if result.returncode != 0:\n            logger.warning(f\"Montage visualization returned non-zero: {result.stderr}\")\n\n        return result.returncode == 0\n\n    except subprocess.TimeoutExpired:\n        logger.warning(f\"Montage visualization timed out for {montage_name}\")\n        return False\n    except Exception as e:\n        logger.warning(f\"Montage visualization failed: {e}\")\n        return False\n</code></pre>"},{"location":"reference/sim/#tit.sim.run_simulation","title":"run_simulation","text":"<pre><code>run_simulation(config: SimulationConfig, montages: List[MontageConfig], logger=None, progress_callback: Optional[Callable[[int, int, str], None]] = None) -&gt; List[dict]\n</code></pre> <p>Run TI/mTI simulations for given montages.</p> <p>Supports both sequential and parallel execution based on config.parallel settings.</p> <p>Args:     config: Simulation configuration     montages: List of montage configurations     logger: Optional logger instance     progress_callback: Optional callback for progress updates (current, total, montage_name)</p> <p>Returns:     List of dictionaries with simulation results</p> Source code in <code>tit/sim/simulator.py</code> <pre><code>def run_simulation(\n    config: SimulationConfig,\n    montages: List[MontageConfig],\n    logger=None,\n    progress_callback: Optional[Callable[[int, int, str], None]] = None\n) -&gt; List[dict]:\n    \"\"\"\n    Run TI/mTI simulations for given montages.\n\n    Supports both sequential and parallel execution based on config.parallel settings.\n\n    Args:\n        config: Simulation configuration\n        montages: List of montage configurations\n        logger: Optional logger instance\n        progress_callback: Optional callback for progress updates (current, total, montage_name)\n\n    Returns:\n        List of dictionaries with simulation results\n    \"\"\"\n    # Initialize logger if not provided\n    if logger is None:\n        import logging\n        pm = get_path_manager()\n        log_dir = pm.path(\"ti_logs\", subject_id=config.subject_id)\n        os.makedirs(log_dir, exist_ok=True)\n        log_file = os.path.join(log_dir, f'Simulator_{time.strftime(\"%Y%m%d_%H%M%S\")}.log')\n\n        # Use file-only logger by default (no console output)\n        # Console output should be controlled by the caller (GUI or CLI)\n        logger = logging_util.get_file_only_logger('TI-Simulator', log_file, level=logging.DEBUG)\n\n        # Configure external loggers to also use file-only logging\n        for ext_name in ['simnibs', 'mesh_io', 'sim_struct', 'TI']:\n            ext_logger = logging.getLogger(ext_name)\n            ext_logger.setLevel(logging.DEBUG)\n            ext_logger.propagate = False\n            for handler in list(ext_logger.handlers):\n                ext_logger.removeHandler(handler)\n            file_handler = logging.FileHandler(log_file, mode='a')\n            file_handler.setLevel(logging.DEBUG)\n            file_handler.setFormatter(logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s'))\n            ext_logger.addHandler(file_handler)\n\n    # Get simulation directory\n    pm = get_path_manager()\n    simulation_dir = pm.path(\"simulations\", subject_id=config.subject_id)\n\n    # Check if parallel execution is enabled and we have multiple montages\n    use_parallel = (\n        config.parallel.enabled and \n        len(montages) &gt; 1 and \n        config.parallel.effective_workers &gt; 1\n    )\n\n    if use_parallel:\n        return _run_parallel(config, montages, simulation_dir, logger, progress_callback)\n    else:\n        return _run_sequential(config, montages, simulation_dir, logger, progress_callback)\n</code></pre>"},{"location":"reference/sim/#tit.sim.setup_montage_directories","title":"setup_montage_directories","text":"<pre><code>setup_montage_directories(montage_dir: str, simulation_mode: SimulationMode) -&gt; dict\n</code></pre> <p>Create the complete directory structure for a montage simulation.</p> <p>Args:     montage_dir: Base montage directory     simulation_mode: TI or MTI simulation mode</p> <p>Returns:     Dictionary of created directory paths</p> Source code in <code>tit/sim/simulator.py</code> <pre><code>def setup_montage_directories(montage_dir: str, simulation_mode: SimulationMode) -&gt; dict:\n    \"\"\"\n    Create the complete directory structure for a montage simulation.\n\n    Args:\n        montage_dir: Base montage directory\n        simulation_mode: TI or MTI simulation mode\n\n    Returns:\n        Dictionary of created directory paths\n    \"\"\"\n    dirs = {\n        'montage_dir': montage_dir,\n        'hf_dir': os.path.join(montage_dir, \"high_Frequency\"),\n        'hf_mesh': os.path.join(montage_dir, \"high_Frequency\", \"mesh\"),\n        'hf_niftis': os.path.join(montage_dir, \"high_Frequency\", \"niftis\"),\n        'hf_analysis': os.path.join(montage_dir, \"high_Frequency\", \"analysis\"),\n        'ti_mesh': os.path.join(montage_dir, \"TI\", \"mesh\"),\n        'ti_niftis': os.path.join(montage_dir, \"TI\", \"niftis\"),\n        'ti_surface_overlays': os.path.join(montage_dir, \"TI\", \"surface_overlays\"),\n        'ti_montage_imgs': os.path.join(montage_dir, \"TI\", \"montage_imgs\"),\n        'documentation': os.path.join(montage_dir, \"documentation\"),\n    }\n\n    # Add mTI directories for multipolar mode\n    if simulation_mode == SimulationMode.MTI:\n        dirs['mti_mesh'] = os.path.join(montage_dir, \"mTI\", \"mesh\")\n        dirs['mti_niftis'] = os.path.join(montage_dir, \"mTI\", \"niftis\")\n        dirs['mti_montage_imgs'] = os.path.join(montage_dir, \"mTI\", \"montage_imgs\")\n\n    # Create all directories\n    for path in dirs.values():\n        os.makedirs(path, exist_ok=True)\n\n    return dirs\n</code></pre>"},{"location":"reference/sim/#config-titsimconfig","title":"Config (<code>tit.sim.config</code>)","text":"<p>Configuration dataclasses for TI simulations.</p>"},{"location":"reference/sim/#tit.sim.config.ConductivityType","title":"ConductivityType","text":"<p>               Bases: <code>Enum</code></p> <p>SimNIBS conductivity type enumeration.</p>"},{"location":"reference/sim/#tit.sim.config.ElectrodeConfig","title":"ElectrodeConfig  <code>dataclass</code>","text":"<pre><code>ElectrodeConfig(shape: str = 'ellipse', dimensions: List[float] = (lambda: [8.0, 8.0])(), thickness: float = 4.0, sponge_thickness: float = 2.0)\n</code></pre> <p>Configuration for electrode properties.</p>"},{"location":"reference/sim/#tit.sim.config.IntensityConfig","title":"IntensityConfig  <code>dataclass</code>","text":"<pre><code>IntensityConfig(pair1: float = 1.0, pair2: float = 1.0, pair3: float = 1.0, pair4: float = 1.0)\n</code></pre> <p>Configuration for current intensities in TI simulations.</p> <p>Each pair requires one intensity value (in mA). SimNIBS automatically applies equal and opposite currents to the two electrodes in each pair. For example: pair1=2.0 means electrode1=+2.0mA and electrode2=-2.0mA</p> <p>TI mode (2 pairs): Uses pair1 and pair2 mTI mode (4 pairs): Uses pair1, pair2, pair3, and pair4</p>"},{"location":"reference/sim/#tit.sim.config.IntensityConfig.from_string","title":"from_string  <code>classmethod</code>","text":"<pre><code>from_string(intensity_str: str) -&gt; IntensityConfig\n</code></pre> <p>Parse intensity from string format.</p> <p>Formats: - \"2.0\" -&gt; all pairs: 2.0 mA - \"2.0,1.5\" -&gt; pair1: 2.0, pair2: 1.5 (both set to 1.0 for pair3/pair4) - \"2.0,1.5,1.0,0.5\" -&gt; pair1: 2.0, pair2: 1.5, pair3: 1.0, pair4: 0.5</p> <p>Args:     intensity_str: Comma-separated intensity values</p> <p>Returns:     IntensityConfig object</p> Source code in <code>tit/sim/config.py</code> <pre><code>@classmethod\ndef from_string(cls, intensity_str: str) -&gt; 'IntensityConfig':\n    \"\"\"\n    Parse intensity from string format.\n\n    Formats:\n    - \"2.0\" -&gt; all pairs: 2.0 mA\n    - \"2.0,1.5\" -&gt; pair1: 2.0, pair2: 1.5 (both set to 1.0 for pair3/pair4)\n    - \"2.0,1.5,1.0,0.5\" -&gt; pair1: 2.0, pair2: 1.5, pair3: 1.0, pair4: 0.5\n\n    Args:\n        intensity_str: Comma-separated intensity values\n\n    Returns:\n        IntensityConfig object\n    \"\"\"\n    intensities = [float(x.strip()) for x in intensity_str.split(',')]\n\n    if len(intensities) == 1:\n        # Single value: use for all pairs\n        val = intensities[0]\n        return cls(val, val, val, val)\n    elif len(intensities) == 2:\n        # Two values: pair1, pair2 (TI mode)\n        return cls(intensities[0], intensities[1], 1.0, 1.0)\n    elif len(intensities) == 4:\n        # Four values: all pairs specified (mTI mode)\n        return cls(*intensities)\n    else:\n        raise ValueError(\n            f\"Invalid intensity format: {intensity_str}. \"\n            f\"Expected 1, 2, or 4 comma-separated values.\"\n        )\n</code></pre>"},{"location":"reference/sim/#tit.sim.config.MontageConfig","title":"MontageConfig  <code>dataclass</code>","text":"<pre><code>MontageConfig(name: str, electrode_pairs: List[Tuple[Union[str, List[float]], Union[str, List[float]]]], is_xyz: bool = False, eeg_net: Optional[str] = None)\n</code></pre> <p>Configuration for a single montage.</p>"},{"location":"reference/sim/#tit.sim.config.MontageConfig.num_pairs","title":"num_pairs  <code>property</code>","text":"<pre><code>num_pairs: int\n</code></pre> <p>Get number of electrode pairs.</p>"},{"location":"reference/sim/#tit.sim.config.MontageConfig.simulation_mode","title":"simulation_mode  <code>property</code>","text":"<pre><code>simulation_mode: SimulationMode\n</code></pre> <p>Determine simulation mode based on number of electrode pairs.</p>"},{"location":"reference/sim/#tit.sim.config.ParallelConfig","title":"ParallelConfig  <code>dataclass</code>","text":"<pre><code>ParallelConfig(enabled: bool = False, max_workers: int = 0)\n</code></pre> <p>Configuration for parallel simulation execution.</p>"},{"location":"reference/sim/#tit.sim.config.ParallelConfig.effective_workers","title":"effective_workers  <code>property</code>","text":"<pre><code>effective_workers: int\n</code></pre> <p>Get the effective number of workers.</p>"},{"location":"reference/sim/#tit.sim.config.ParallelConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Set max_workers to sensible default if auto-detect.</p> Source code in <code>tit/sim/config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Set max_workers to sensible default if auto-detect.\"\"\"\n    if self.max_workers &lt;= 0:\n        # Use half of available CPUs (simulations are memory-intensive)\n        cpu_count = os.cpu_count() or 4\n        # Limit to max 4 workers by default (memory constraint)\n        self.max_workers = min(4, max(1, cpu_count // 2))\n</code></pre>"},{"location":"reference/sim/#tit.sim.config.ParallelConfig.get_memory_warning","title":"get_memory_warning","text":"<pre><code>get_memory_warning() -&gt; Optional[str]\n</code></pre> <p>Return memory warning if parallel execution may cause issues.</p> Source code in <code>tit/sim/config.py</code> <pre><code>def get_memory_warning(self) -&gt; Optional[str]:\n    \"\"\"Return memory warning if parallel execution may cause issues.\"\"\"\n    if not self.enabled:\n        return None\n    if self.max_workers &gt; 2:\n        return (\n            f\"\u26a0\ufe0f Running {self.max_workers} parallel simulations may require \"\n            f\"significant memory (~4-8 GB per simulation). Consider reducing \"\n            f\"workers if you experience memory issues.\"\n        )\n    return None\n</code></pre>"},{"location":"reference/sim/#tit.sim.config.SimulationConfig","title":"SimulationConfig  <code>dataclass</code>","text":"<pre><code>SimulationConfig(subject_id: str, project_dir: str, conductivity_type: ConductivityType, intensities: IntensityConfig, electrode: ElectrodeConfig, eeg_net: str = 'GSN-HydroCel-185.csv', map_to_surf: bool = True, map_to_vol: bool = True, map_to_mni: bool = True, map_to_fsavg: bool = False, tissues_in_niftis: str = 'all', open_in_gmsh: bool = False, parallel: ParallelConfig = ParallelConfig())\n</code></pre> <p>Main configuration for TI simulation.</p>"},{"location":"reference/sim/#tit.sim.config.SimulationConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Convert string conductivity type to enum if needed.</p> Source code in <code>tit/sim/config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Convert string conductivity type to enum if needed.\"\"\"\n    if isinstance(self.conductivity_type, str):\n        self.conductivity_type = ConductivityType(self.conductivity_type)\n    if isinstance(self.parallel, dict):\n        self.parallel = ParallelConfig(**self.parallel)\n</code></pre>"},{"location":"reference/sim/#tit.sim.config.SimulationMode","title":"SimulationMode","text":"<p>               Bases: <code>Enum</code></p> <p>Simulation mode enumeration.</p>"},{"location":"reference/sim/#simulator-titsimsimulator","title":"Simulator (<code>tit.sim.simulator</code>)","text":"<p>Unified TI/mTI simulation runner.</p> <p>This module provides a single entry point for running both TI (2-pair) and mTI (4-pair) simulations. The simulation type is automatically detected based on montage configuration.</p> <p>This refactored module includes all features from the original pipeline: - Montage visualization - SimNIBS simulation - Field extraction (GM/WM) - NIfTI transformation - T1 to MNI conversion - File organization</p> <p>Parallel execution support: - Multiple montages can be simulated in parallel - Worker count is configurable (default: half of CPU cores)</p>"},{"location":"reference/sim/#tit.sim.simulator.create_simulation_config_file","title":"create_simulation_config_file","text":"<pre><code>create_simulation_config_file(config: SimulationConfig, montage: MontageConfig, documentation_dir: str, logger) -&gt; str\n</code></pre> <p>Create a config.json file with all simulation parameters.</p> <p>This file is used by visualization tools to auto-populate parameters without requiring manual input.</p> <p>Args:     config: Simulation configuration     montage: Montage configuration     documentation_dir: Documentation directory path     logger: Logger instance</p> <p>Returns:     Path to created config.json file</p> Source code in <code>tit/sim/simulator.py</code> <pre><code>def create_simulation_config_file(\n    config: SimulationConfig,\n    montage: MontageConfig,\n    documentation_dir: str,\n    logger\n) -&gt; str:\n    \"\"\"\n    Create a config.json file with all simulation parameters.\n\n    This file is used by visualization tools to auto-populate parameters\n    without requiring manual input.\n\n    Args:\n        config: Simulation configuration\n        montage: Montage configuration\n        documentation_dir: Documentation directory path\n        logger: Logger instance\n\n    Returns:\n        Path to created config.json file\n    \"\"\"\n    config_file = os.path.join(documentation_dir, \"config.json\")\n\n    # Build configuration dictionary\n    sim_config = {\n        \"subject_id\": config.subject_id,\n        \"simulation_name\": montage.name,\n        \"simulation_mode\": montage.simulation_mode.value,\n        \"eeg_net\": montage.eeg_net or config.eeg_net,\n        \"conductivity_type\": config.conductivity_type.value,\n        \"electrode_pairs\": montage.electrode_pairs,\n        \"is_xyz_montage\": montage.is_xyz,\n        \"intensities\": {\n            \"pair1\": config.intensities.pair1,\n            \"pair2\": config.intensities.pair2,\n            \"pair3\": config.intensities.pair3,\n            \"pair4\": config.intensities.pair4\n        },\n        \"electrode_geometry\": {\n            \"shape\": config.electrode.shape,\n            \"dimensions\": config.electrode.dimensions,\n            \"gel_thickness\": config.electrode.thickness,\n            \"sponge_thickness\": config.electrode.sponge_thickness\n        },\n        \"mapping_options\": {\n            \"map_to_surf\": config.map_to_surf,\n            \"map_to_vol\": config.map_to_vol,\n            \"map_to_mni\": config.map_to_mni,\n            \"map_to_fsavg\": config.map_to_fsavg\n        },\n        \"created_at\": datetime.now().isoformat(),\n        \"ti_toolbox_version\": \"2.0.0\"  # TODO: Get from version.py\n    }\n\n    # Write config file\n    try:\n        with open(config_file, 'w') as f:\n            json.dump(sim_config, f, indent=2)\n        logger.info(f\"Created simulation config: {config_file}\")\n    except Exception as e:\n        logger.warning(f\"Failed to create config file: {e}\")\n\n    return config_file\n</code></pre>"},{"location":"reference/sim/#tit.sim.simulator.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Command-line entry point for the simulator.</p> <p>Parses command-line arguments, loads montages, runs simulations, and generates a completion report. This function is called when the module is executed directly or via the CLI wrapper script.</p> Command-line Arguments <ol> <li>subject_id : str     Subject identifier (e.g., '001')</li> <li>conductivity_str : str     Conductivity type ('scalar', 'vn', 'dir', 'mc')</li> <li>project_dir : str     Project directory path</li> <li>simulation_dir : str     Simulation output directory (legacy, not used)</li> <li>mode : str     Simulation mode (legacy, not used - auto-detected)</li> <li>intensity_str : str     Current intensity string (e.g., '2.0' or '2.0,1.5' or '2.0,1.5,1.0,0.5')</li> <li>electrode_shape : str     Electrode shape ('ellipse' or 'rect')</li> <li>dimensions : str     Electrode dimensions as 'x,y' in mm (e.g., '8.0,8.0')</li> <li>thickness : float     Gel thickness in mm</li> <li>eeg_net : str     EEG cap name (e.g., 'EGI_template.csv') 11+. montage_names : str     One or more montage names to simulate</li> </ol> Exit Codes <p>0 : All simulations completed successfully 1 : One or more simulations failed</p> Output <p>Creates simulation_completion__.json in derivatives/temp/ with detailed results of all simulations. <p>Examples:</p> <pre><code>&gt;&gt;&gt; python simulator.py 001 scalar /path/to/project /tmp TI 2.0 ellipse 8.0,8.0 4.0 EGI_template.csv montage1 montage2\n</code></pre> Source code in <code>tit/sim/simulator.py</code> <pre><code>def main():\n    \"\"\"\n    Command-line entry point for the simulator.\n\n    Parses command-line arguments, loads montages, runs simulations, and generates\n    a completion report. This function is called when the module is executed directly\n    or via the CLI wrapper script.\n\n    Command-line Arguments\n    ----------------------\n    1. subject_id : str\n        Subject identifier (e.g., '001')\n    2. conductivity_str : str\n        Conductivity type ('scalar', 'vn', 'dir', 'mc')\n    3. project_dir : str\n        Project directory path\n    4. simulation_dir : str\n        Simulation output directory (legacy, not used)\n    5. mode : str\n        Simulation mode (legacy, not used - auto-detected)\n    6. intensity_str : str\n        Current intensity string (e.g., '2.0' or '2.0,1.5' or '2.0,1.5,1.0,0.5')\n    7. electrode_shape : str\n        Electrode shape ('ellipse' or 'rect')\n    8. dimensions : str\n        Electrode dimensions as 'x,y' in mm (e.g., '8.0,8.0')\n    9. thickness : float\n        Gel thickness in mm\n    10. eeg_net : str\n        EEG cap name (e.g., 'EGI_template.csv')\n    11+. montage_names : str\n        One or more montage names to simulate\n\n    Exit Codes\n    ----------\n    0 : All simulations completed successfully\n    1 : One or more simulations failed\n\n    Output\n    ------\n    Creates simulation_completion_&lt;subject_id&gt;_&lt;timestamp&gt;.json in derivatives/temp/\n    with detailed results of all simulations.\n\n    Examples\n    --------\n    &gt;&gt;&gt; python simulator.py 001 scalar /path/to/project /tmp TI 2.0 ellipse 8.0,8.0 4.0 EGI_template.csv montage1 montage2\n    \"\"\"\n    if len(sys.argv) &lt; 11:\n        print(\"Usage: simulator.py SUBJECT_ID CONDUCTIVITY PROJECT_DIR SIMULATION_DIR MODE \"\n              \"INTENSITY ELECTRODE_SHAPE DIMENSIONS THICKNESS EEG_NET MONTAGE_NAMES...\")\n        sys.exit(1)\n\n    # Parse command-line arguments\n    subject_id = sys.argv[1]\n    conductivity_str = sys.argv[2]\n    project_dir = sys.argv[3]\n    simulation_dir = sys.argv[4]\n    mode = sys.argv[5]  # Kept for backward compatibility but not used\n    intensity_str = sys.argv[6]\n    electrode_shape = sys.argv[7]\n    dimensions = [float(x) for x in sys.argv[8].split(',')]\n    thickness = float(sys.argv[9])\n    eeg_net = sys.argv[10]\n    montage_names = sys.argv[11:]\n\n    # Filter out flags\n    montage_names = [name for name in montage_names if not name.startswith('--')]\n\n    # Build configuration\n    config = SimulationConfig(\n        subject_id=subject_id,\n        project_dir=project_dir,\n        conductivity_type=ConductivityType(conductivity_str),\n        intensities=IntensityConfig.from_string(intensity_str),\n        electrode=ElectrodeConfig(\n            shape=electrode_shape,\n            dimensions=dimensions,\n            thickness=thickness\n        ),\n        eeg_net=eeg_net\n    )\n\n    # Load montages\n    montages = load_montages(\n        montage_names=montage_names,\n        project_dir=project_dir,\n        eeg_net=eeg_net,\n        include_flex=True\n    )\n\n    # Run simulations\n    results = run_simulation(config, montages)\n\n    # Generate completion report\n    pm = get_path_manager()\n    derivatives_dir = pm.path(\"derivatives\")\n\n    report = {\n        'session_id': os.environ.get('SIMULATION_SESSION_ID', 'unknown'),\n        'subject_id': subject_id,\n        'project_dir': project_dir,\n        'simulation_dir': simulation_dir,\n        'completed_simulations': [r for r in results if r['status'] == 'completed'],\n        'failed_simulations': [r for r in results if r['status'] == 'failed'],\n        'timestamp': datetime.now().isoformat(),\n        'total_simulations': len(results),\n        'success_count': len([r for r in results if r['status'] == 'completed']),\n        'error_count': len([r for r in results if r['status'] == 'failed'])\n    }\n\n    report_file = os.path.join(derivatives_dir, 'temp', f'simulation_completion_{subject_id}_{int(time.time())}.json')\n    os.makedirs(os.path.dirname(report_file), exist_ok=True)\n    with open(report_file, 'w') as f:\n        json.dump(report, f, indent=2)\n\n    print(f\"Completed {report['success_count']}/{report['total_simulations']} simulations\")\n\n    # Exit with error if any simulations failed\n    if report['error_count'] &gt; 0:\n        sys.exit(1)\n</code></pre>"},{"location":"reference/sim/#tit.sim.simulator.run_montage_visualization","title":"run_montage_visualization","text":"<pre><code>run_montage_visualization(montage_name: str, simulation_mode: SimulationMode, eeg_net: str, output_dir: str, project_dir: str, logger, electrode_pairs: Optional[List] = None) -&gt; bool\n</code></pre> <p>Run montage visualization using visualize-montage.sh.</p> <p>Args:     montage_name: Name of the montage     simulation_mode: TI or MTI mode     eeg_net: EEG net name     output_dir: Output directory for montage images     project_dir: Project directory path     logger: Logger instance     electrode_pairs: Optional list of electrode pairs for direct visualization</p> <p>Returns:     True if successful, False otherwise</p> Source code in <code>tit/sim/simulator.py</code> <pre><code>def run_montage_visualization(\n    montage_name: str,\n    simulation_mode: SimulationMode,\n    eeg_net: str,\n    output_dir: str,\n    project_dir: str,\n    logger,\n    electrode_pairs: Optional[List] = None\n) -&gt; bool:\n    \"\"\"\n    Run montage visualization using visualize-montage.sh.\n\n    Args:\n        montage_name: Name of the montage\n        simulation_mode: TI or MTI mode\n        eeg_net: EEG net name\n        output_dir: Output directory for montage images\n        project_dir: Project directory path\n        logger: Logger instance\n        electrode_pairs: Optional list of electrode pairs for direct visualization\n\n    Returns:\n        True if successful, False otherwise\n    \"\"\"\n    # Skip visualization for freehand/flex modes with XYZ coordinates (no electrode_pairs provided)\n    if eeg_net in [\"freehand\", \"flex_mode\"] and not electrode_pairs:\n        logger.info(f\"Skipping montage visualization for {eeg_net} mode\")\n        return True\n\n    # Determine sim_mode string for visualize-montage.sh\n    sim_mode_str = \"U\" if simulation_mode == SimulationMode.TI else \"M\"\n\n    # Path to visualize-montage.sh\n    script_dir = os.path.dirname(__file__)\n    visualize_script = os.path.join(script_dir, \"visualize-montage.sh\")\n\n    if not os.path.exists(visualize_script):\n        logger.warning(f\"Visualize montage script not found: {visualize_script}\")\n        return True  # Non-fatal, continue without visualization\n\n    logger.info(f\"Visualizing montage: {montage_name}\")\n\n    try:\n        # Set PROJECT_DIR_NAME environment variable for the script\n        env = os.environ.copy()\n        project_dir_name = os.path.basename(project_dir)\n        env['PROJECT_DIR_NAME'] = project_dir_name\n\n        # Build command\n        cmd = ['bash', visualize_script, sim_mode_str, eeg_net, output_dir]\n\n        if electrode_pairs:\n            # For flex montages with known electrode pairs, pass --pairs\n            pairs_str = \",\".join([f\"{pair[0]}-{pair[1]}\" for pair in electrode_pairs])\n            pairs_arg = f\"{montage_name}:{pairs_str}\"\n            cmd.extend(['--pairs', pairs_arg])\n        else:\n            # For standard montages, pass montage name\n            cmd.insert(2, montage_name)  # Insert after script path and sim_mode\n\n        result = subprocess.run(\n            cmd,\n            env=env,\n            capture_output=True,\n            text=True,\n            timeout=120\n        )\n\n        if result.returncode != 0:\n            logger.warning(f\"Montage visualization returned non-zero: {result.stderr}\")\n\n        return result.returncode == 0\n\n    except subprocess.TimeoutExpired:\n        logger.warning(f\"Montage visualization timed out for {montage_name}\")\n        return False\n    except Exception as e:\n        logger.warning(f\"Montage visualization failed: {e}\")\n        return False\n</code></pre>"},{"location":"reference/sim/#tit.sim.simulator.run_simulation","title":"run_simulation","text":"<pre><code>run_simulation(config: SimulationConfig, montages: List[MontageConfig], logger=None, progress_callback: Optional[Callable[[int, int, str], None]] = None) -&gt; List[dict]\n</code></pre> <p>Run TI/mTI simulations for given montages.</p> <p>Supports both sequential and parallel execution based on config.parallel settings.</p> <p>Args:     config: Simulation configuration     montages: List of montage configurations     logger: Optional logger instance     progress_callback: Optional callback for progress updates (current, total, montage_name)</p> <p>Returns:     List of dictionaries with simulation results</p> Source code in <code>tit/sim/simulator.py</code> <pre><code>def run_simulation(\n    config: SimulationConfig,\n    montages: List[MontageConfig],\n    logger=None,\n    progress_callback: Optional[Callable[[int, int, str], None]] = None\n) -&gt; List[dict]:\n    \"\"\"\n    Run TI/mTI simulations for given montages.\n\n    Supports both sequential and parallel execution based on config.parallel settings.\n\n    Args:\n        config: Simulation configuration\n        montages: List of montage configurations\n        logger: Optional logger instance\n        progress_callback: Optional callback for progress updates (current, total, montage_name)\n\n    Returns:\n        List of dictionaries with simulation results\n    \"\"\"\n    # Initialize logger if not provided\n    if logger is None:\n        import logging\n        pm = get_path_manager()\n        log_dir = pm.path(\"ti_logs\", subject_id=config.subject_id)\n        os.makedirs(log_dir, exist_ok=True)\n        log_file = os.path.join(log_dir, f'Simulator_{time.strftime(\"%Y%m%d_%H%M%S\")}.log')\n\n        # Use file-only logger by default (no console output)\n        # Console output should be controlled by the caller (GUI or CLI)\n        logger = logging_util.get_file_only_logger('TI-Simulator', log_file, level=logging.DEBUG)\n\n        # Configure external loggers to also use file-only logging\n        for ext_name in ['simnibs', 'mesh_io', 'sim_struct', 'TI']:\n            ext_logger = logging.getLogger(ext_name)\n            ext_logger.setLevel(logging.DEBUG)\n            ext_logger.propagate = False\n            for handler in list(ext_logger.handlers):\n                ext_logger.removeHandler(handler)\n            file_handler = logging.FileHandler(log_file, mode='a')\n            file_handler.setLevel(logging.DEBUG)\n            file_handler.setFormatter(logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s'))\n            ext_logger.addHandler(file_handler)\n\n    # Get simulation directory\n    pm = get_path_manager()\n    simulation_dir = pm.path(\"simulations\", subject_id=config.subject_id)\n\n    # Check if parallel execution is enabled and we have multiple montages\n    use_parallel = (\n        config.parallel.enabled and \n        len(montages) &gt; 1 and \n        config.parallel.effective_workers &gt; 1\n    )\n\n    if use_parallel:\n        return _run_parallel(config, montages, simulation_dir, logger, progress_callback)\n    else:\n        return _run_sequential(config, montages, simulation_dir, logger, progress_callback)\n</code></pre>"},{"location":"reference/sim/#tit.sim.simulator.setup_montage_directories","title":"setup_montage_directories","text":"<pre><code>setup_montage_directories(montage_dir: str, simulation_mode: SimulationMode) -&gt; dict\n</code></pre> <p>Create the complete directory structure for a montage simulation.</p> <p>Args:     montage_dir: Base montage directory     simulation_mode: TI or MTI simulation mode</p> <p>Returns:     Dictionary of created directory paths</p> Source code in <code>tit/sim/simulator.py</code> <pre><code>def setup_montage_directories(montage_dir: str, simulation_mode: SimulationMode) -&gt; dict:\n    \"\"\"\n    Create the complete directory structure for a montage simulation.\n\n    Args:\n        montage_dir: Base montage directory\n        simulation_mode: TI or MTI simulation mode\n\n    Returns:\n        Dictionary of created directory paths\n    \"\"\"\n    dirs = {\n        'montage_dir': montage_dir,\n        'hf_dir': os.path.join(montage_dir, \"high_Frequency\"),\n        'hf_mesh': os.path.join(montage_dir, \"high_Frequency\", \"mesh\"),\n        'hf_niftis': os.path.join(montage_dir, \"high_Frequency\", \"niftis\"),\n        'hf_analysis': os.path.join(montage_dir, \"high_Frequency\", \"analysis\"),\n        'ti_mesh': os.path.join(montage_dir, \"TI\", \"mesh\"),\n        'ti_niftis': os.path.join(montage_dir, \"TI\", \"niftis\"),\n        'ti_surface_overlays': os.path.join(montage_dir, \"TI\", \"surface_overlays\"),\n        'ti_montage_imgs': os.path.join(montage_dir, \"TI\", \"montage_imgs\"),\n        'documentation': os.path.join(montage_dir, \"documentation\"),\n    }\n\n    # Add mTI directories for multipolar mode\n    if simulation_mode == SimulationMode.MTI:\n        dirs['mti_mesh'] = os.path.join(montage_dir, \"mTI\", \"mesh\")\n        dirs['mti_niftis'] = os.path.join(montage_dir, \"mTI\", \"niftis\")\n        dirs['mti_montage_imgs'] = os.path.join(montage_dir, \"mTI\", \"montage_imgs\")\n\n    # Create all directories\n    for path in dirs.values():\n        os.makedirs(path, exist_ok=True)\n\n    return dirs\n</code></pre>"},{"location":"reference/sim/#session-builder-titsimsession_builder","title":"Session builder (<code>tit.sim.session_builder</code>)","text":"<p>SimNIBS session builder for TI simulations.</p>"},{"location":"reference/sim/#tit.sim.session_builder.SessionBuilder","title":"SessionBuilder","text":"<pre><code>SessionBuilder(config: SimulationConfig)\n</code></pre> <p>Builder class for constructing SimNIBS SESSION objects.</p> <p>Initialize session builder.</p> <p>Args:     config: Simulation configuration</p> Source code in <code>tit/sim/session_builder.py</code> <pre><code>def __init__(self, config: SimulationConfig):\n    \"\"\"\n    Initialize session builder.\n\n    Args:\n        config: Simulation configuration\n    \"\"\"\n    self.config = config\n    self.pm = get_path_manager()\n\n    # Setup paths\n    self.m2m_dir = self.pm.path(\"m2m\", subject_id=config.subject_id)\n    self.mesh_file = os.path.join(self.m2m_dir, f\"{config.subject_id}.msh\")\n    self.tensor_file = os.path.join(self.m2m_dir, \"DTI_coregT1_tensor.nii.gz\")\n</code></pre>"},{"location":"reference/sim/#tit.sim.session_builder.SessionBuilder.build_session","title":"build_session","text":"<pre><code>build_session(montage: MontageConfig, output_dir: str) -&gt; sim_struct.SESSION\n</code></pre> <p>Build SimNIBS SESSION object for a montage.</p> <p>Args:     montage: Montage configuration     output_dir: Output directory for simulation results</p> <p>Returns:     Configured SESSION object</p> Source code in <code>tit/sim/session_builder.py</code> <pre><code>def build_session(\n    self,\n    montage: MontageConfig,\n    output_dir: str\n) -&gt; sim_struct.SESSION:\n    \"\"\"\n    Build SimNIBS SESSION object for a montage.\n\n    Args:\n        montage: Montage configuration\n        output_dir: Output directory for simulation results\n\n    Returns:\n        Configured SESSION object\n    \"\"\"\n    # Create base session\n    S = sim_struct.SESSION()\n    S.subpath = self.m2m_dir\n    S.fnamehead = self.mesh_file\n    S.anisotropy_type = self.config.conductivity_type.value\n    S.pathfem = output_dir\n\n    # Set EEG cap if using electrode names (not XYZ)\n    if not montage.is_xyz:\n        eeg_net = montage.eeg_net or self.config.eeg_net\n        S.eeg_cap = os.path.join(self.pm.path(\"eeg_positions\", subject_id=self.config.subject_id), eeg_net)\n\n    # Mapping options\n    S.map_to_surf = self.config.map_to_surf\n    S.map_to_vol = self.config.map_to_vol\n    S.map_to_mni = self.config.map_to_mni\n    S.map_to_fsavg = self.config.map_to_fsavg\n    S.open_in_gmsh = self.config.open_in_gmsh\n    S.tissues_in_niftis = self.config.tissues_in_niftis\n\n    # DTI tensor for anisotropic conductivity\n    if os.path.exists(self.tensor_file):\n        S.dti_nii = self.tensor_file\n\n    # Add electrode pairs based on simulation mode\n    if montage.simulation_mode == SimulationMode.TI:\n        self._add_ti_pairs(S, montage)\n    elif montage.simulation_mode == SimulationMode.MTI:\n        self._add_mti_pairs(S, montage)\n\n    return S\n</code></pre>"},{"location":"reference/sim/#post-processor-titsimpost_processor","title":"Post-processor (<code>tit.sim.post_processor</code>)","text":"<p>Post-processing utilities for TI simulations.</p> <p>This module handles all post-simulation processing including: - TI/mTI field calculation - Field extraction (GM/WM mesh creation) - NIfTI transformation - T1 to MNI conversion - File organization</p>"},{"location":"reference/sim/#tit.sim.post_processor.PostProcessor","title":"PostProcessor","text":"<pre><code>PostProcessor(subject_id: str, conductivity_type: str, m2m_dir: str, logger)\n</code></pre> <p>Post-processor for TI simulation results.</p> <p>Initialize post-processor.</p> <p>Args:     subject_id: Subject identifier     conductivity_type: Conductivity type string     m2m_dir: Path to m2m directory     logger: Logger instance</p> Source code in <code>tit/sim/post_processor.py</code> <pre><code>def __init__(\n    self,\n    subject_id: str,\n    conductivity_type: str,\n    m2m_dir: str,\n    logger\n):\n    \"\"\"\n    Initialize post-processor.\n\n    Args:\n        subject_id: Subject identifier\n        conductivity_type: Conductivity type string\n        m2m_dir: Path to m2m directory\n        logger: Logger instance\n    \"\"\"\n    self.subject_id = subject_id\n    self.conductivity_type = conductivity_type\n    self.m2m_dir = m2m_dir\n    self.logger = logger\n\n    # Path to tools directory\n    self.tools_dir = os.path.join(os.path.dirname(__file__), '..', 'tools')\n</code></pre>"},{"location":"reference/sim/#tit.sim.post_processor.PostProcessor.process_mti_results","title":"process_mti_results","text":"<pre><code>process_mti_results(hf_dir: str, ti_dir: str, mti_dir: str, mti_nifti_dir: str, hf_mesh_dir: str, hf_analysis_dir: str, documentation_dir: str, montage_name: str) -&gt; str\n</code></pre> <p>Process 4-pair mTI simulation results with full pipeline.</p> <p>Args:     hf_dir: High-frequency output directory     ti_dir: TI intermediate output directory     mti_dir: mTI final output directory     mti_nifti_dir: mTI NIfTI output directory     hf_mesh_dir: High-frequency mesh output directory     hf_analysis_dir: High-frequency analysis output directory     documentation_dir: Documentation output directory     montage_name: Montage name</p> <p>Returns:     Path to output mTI mesh file</p> Source code in <code>tit/sim/post_processor.py</code> <pre><code>def process_mti_results(\n    self,\n    hf_dir: str,\n    ti_dir: str,\n    mti_dir: str,\n    mti_nifti_dir: str,\n    hf_mesh_dir: str,\n    hf_analysis_dir: str,\n    documentation_dir: str,\n    montage_name: str\n) -&gt; str:\n    \"\"\"\n    Process 4-pair mTI simulation results with full pipeline.\n\n    Args:\n        hf_dir: High-frequency output directory\n        ti_dir: TI intermediate output directory\n        mti_dir: mTI final output directory\n        mti_nifti_dir: mTI NIfTI output directory\n        hf_mesh_dir: High-frequency mesh output directory\n        hf_analysis_dir: High-frequency analysis output directory\n        documentation_dir: Documentation output directory\n        montage_name: Montage name\n\n    Returns:\n        Path to output mTI mesh file\n    \"\"\"\n    self.logger.info(f\"Processing mTI results for {montage_name}\")\n\n    # Step 1: Load 4 HF meshes\n    hf_meshes = []\n    for i in range(1, 5):\n        mesh_file = os.path.join(hf_dir, f\"{self.subject_id}_TDCS_{i}_{self.conductivity_type}.msh\")\n\n        if not os.path.exists(mesh_file):\n            raise FileNotFoundError(f\"Mesh file not found: {mesh_file}\")\n\n        m = mesh_io.read_msh(mesh_file)\n        tags_keep = np.hstack((np.arange(1, 100), np.arange(1001, 1100)))\n        m = m.crop_mesh(tags=tags_keep)\n        hf_meshes.append(m)\n\n    # Step 2: Calculate TI pairs (AB and CD)\n    ti_ab_vectors = get_TI_vectors(hf_meshes[0].field[\"E\"].value, hf_meshes[1].field[\"E\"].value)\n    ti_cd_vectors = get_TI_vectors(hf_meshes[2].field[\"E\"].value, hf_meshes[3].field[\"E\"].value)\n\n    # Step 3: Save intermediate TI meshes\n    self._save_ti_intermediate(hf_meshes[0], ti_ab_vectors, ti_dir, f\"{montage_name}_TI_AB.msh\")\n    self._save_ti_intermediate(hf_meshes[0], ti_cd_vectors, ti_dir, f\"{montage_name}_TI_CD.msh\")\n\n    # Step 4: Calculate and save final mTI\n    mti_field = TI.get_maxTI(ti_ab_vectors, ti_cd_vectors)\n    mout = deepcopy(hf_meshes[0])\n    mout.elmdata = []\n    mout.add_element_field(mti_field, \"TI_Max\")\n\n    mti_path = os.path.join(mti_dir, f\"{montage_name}_mTI.msh\")\n    mesh_io.write_msh(mout, mti_path)\n    mout.view(visible_tags=[1002, 1006], visible_fields=\"TI_Max\").write_opt(mti_path)\n\n    # Step 5: Extract GM/WM fields for mTI\n    self.logger.info(\"Field extraction: Started\")\n    self._extract_fields(mti_path, mti_dir, f\"{montage_name}_mTI\")\n    self.logger.info(\"Field extraction: \u2713 Complete\")\n\n    # Step 6: Extract GM/WM fields for intermediate TI meshes\n    ti_ab_path = os.path.join(ti_dir, f\"{montage_name}_TI_AB.msh\")\n    ti_cd_path = os.path.join(ti_dir, f\"{montage_name}_TI_CD.msh\")\n    if os.path.exists(ti_ab_path):\n        self._extract_fields(ti_ab_path, ti_dir, f\"{montage_name}_TI_AB\")\n    if os.path.exists(ti_cd_path):\n        self._extract_fields(ti_cd_path, ti_dir, f\"{montage_name}_TI_CD\")\n\n    # Step 7: Convert mTI meshes to NIfTI\n    self.logger.info(\"NIfTI transformation: Started\")\n    self._transform_to_nifti(mti_dir, mti_nifti_dir)\n    self.logger.info(\"NIfTI transformation: \u2713 Complete\")\n\n    # Step 8: Organize HF files with mTI naming\n    self._organize_mti_files(\n        hf_dir=hf_dir,\n        hf_mesh_dir=hf_mesh_dir,\n        hf_analysis_dir=hf_analysis_dir,\n        documentation_dir=documentation_dir\n    )\n\n    # Step 9: Convert T1 to MNI space\n    self._convert_t1_to_mni()\n\n    self.logger.info(f\"Saved mTI mesh: {mti_path}\")\n    return mti_path\n</code></pre>"},{"location":"reference/sim/#tit.sim.post_processor.PostProcessor.process_ti_results","title":"process_ti_results","text":"<pre><code>process_ti_results(hf_dir: str, output_dir: str, nifti_dir: str, surface_overlays_dir: str, hf_mesh_dir: str, hf_nifti_dir: str, hf_analysis_dir: str, documentation_dir: str, montage_name: str) -&gt; str\n</code></pre> <p>Process 2-pair TI simulation results with full pipeline.</p> <p>Args:     hf_dir: High-frequency output directory (SimNIBS writes here)     output_dir: TI mesh output directory     nifti_dir: TI NIfTI output directory     surface_overlays_dir: Surface overlays output directory     hf_mesh_dir: High-frequency mesh output directory     hf_nifti_dir: High-frequency NIfTI output directory     hf_analysis_dir: High-frequency analysis output directory     documentation_dir: Documentation output directory     montage_name: Montage name</p> <p>Returns:     Path to output TI mesh file</p> Source code in <code>tit/sim/post_processor.py</code> <pre><code>def process_ti_results(\n    self,\n    hf_dir: str,\n    output_dir: str,\n    nifti_dir: str,\n    surface_overlays_dir: str,\n    hf_mesh_dir: str,\n    hf_nifti_dir: str,\n    hf_analysis_dir: str,\n    documentation_dir: str,\n    montage_name: str\n) -&gt; str:\n    \"\"\"\n    Process 2-pair TI simulation results with full pipeline.\n\n    Args:\n        hf_dir: High-frequency output directory (SimNIBS writes here)\n        output_dir: TI mesh output directory\n        nifti_dir: TI NIfTI output directory\n        surface_overlays_dir: Surface overlays output directory\n        hf_mesh_dir: High-frequency mesh output directory\n        hf_nifti_dir: High-frequency NIfTI output directory\n        hf_analysis_dir: High-frequency analysis output directory\n        documentation_dir: Documentation output directory\n        montage_name: Montage name\n\n    Returns:\n        Path to output TI mesh file\n    \"\"\"\n    self.logger.info(f\"Processing TI results for {montage_name}\")\n\n    # Step 1: Calculate TI field\n    ti_path = self._calculate_ti_field(hf_dir, output_dir, montage_name)\n\n    # Step 2: Calculate TI normal (cortical surface)\n    self._process_ti_normal(hf_dir, output_dir, montage_name)\n\n    # Step 3: Extract GM/WM fields\n    self.logger.info(\"Field extraction: Started\")\n    self._extract_fields(ti_path, output_dir, f\"{montage_name}_TI\")\n    self.logger.info(\"Field extraction: \u2713 Complete\")\n\n    # Step 4: Convert to NIfTI\n    self.logger.info(\"NIfTI transformation: Started\")\n    self._transform_to_nifti(output_dir, nifti_dir)\n    self.logger.info(\"NIfTI transformation: \u2713 Complete\")\n\n    # Step 5: Organize files\n    self._organize_ti_files(\n        hf_dir=hf_dir,\n        hf_mesh_dir=hf_mesh_dir,\n        hf_nifti_dir=hf_nifti_dir,\n        hf_analysis_dir=hf_analysis_dir,\n        surface_overlays_dir=surface_overlays_dir,\n        documentation_dir=documentation_dir\n    )\n\n    # Step 6: Convert T1 to MNI space\n    self._convert_t1_to_mni()\n\n    self.logger.info(f\"Saved TI mesh: {ti_path}\")\n    return ti_path\n</code></pre>"},{"location":"reference/stats/","title":"Statistics (<code>tit.stats</code>)","text":"<p>Statistical analysis utilities for TI-Toolbox.</p> <p>This package includes permutation testing, atlas-based posthoc analyses, and reporting/visualization helpers.</p>"},{"location":"reference/stats/#tit.stats.atlas_overlap_analysis","title":"atlas_overlap_analysis","text":"<pre><code>atlas_overlap_analysis(sig_mask, atlas_files, data_dir, reference_img=None, verbose=True)\n</code></pre> <p>Analyze overlap between significant voxels and atlas regions</p> Parameters: <p>sig_mask : ndarray (x, y, z)     Binary mask of significant voxels atlas_files : list of str     List of atlas file names data_dir : str     Directory containing atlas files reference_img : nibabel image, optional     Reference image for resampling verbose : bool     Print progress information</p> Returns: <p>results : dict     Dictionary mapping atlas names to DataFrames of region overlap statistics</p> Source code in <code>tit/stats/atlas_utils.py</code> <pre><code>def atlas_overlap_analysis(sig_mask, atlas_files, data_dir, reference_img=None, verbose=True):\n    \"\"\"\n    Analyze overlap between significant voxels and atlas regions\n\n    Parameters:\n    -----------\n    sig_mask : ndarray (x, y, z)\n        Binary mask of significant voxels\n    atlas_files : list of str\n        List of atlas file names\n    data_dir : str\n        Directory containing atlas files\n    reference_img : nibabel image, optional\n        Reference image for resampling\n    verbose : bool\n        Print progress information\n\n    Returns:\n    --------\n    results : dict\n        Dictionary mapping atlas names to DataFrames of region overlap statistics\n    \"\"\"\n    if verbose:\n        print(\"\\n\" + \"=\"*60)\n        print(\"ATLAS OVERLAP ANALYSIS\")\n        print(\"=\"*60)\n\n    results = {}\n\n    for atlas_file in atlas_files:\n        atlas_path = os.path.join(data_dir, atlas_file)\n        if not os.path.exists(atlas_path):\n            if verbose:\n                print(f\"Warning: Atlas file not found - {atlas_file}\")\n            continue\n\n        if verbose:\n            print(f\"\\n--- {atlas_file} ---\")\n        atlas_img = nib.load(atlas_path)\n\n        # Check dimensions and resample if needed\n        if reference_img is not None:\n            atlas_data = check_and_resample_atlas(atlas_img, reference_img, atlas_file, verbose)\n        else:\n            atlas_data = atlas_img.get_fdata().astype(int)\n\n        # Get unique regions (excluding 0 = background)\n        regions = np.unique(atlas_data[atlas_data &gt; 0])\n\n        region_counts = []\n        for region_id in regions:\n            region_mask = (atlas_data == region_id)\n            overlap = np.sum(sig_mask &amp; region_mask)\n\n            if overlap &gt; 0:\n                region_counts.append({\n                    'region_id': int(region_id),\n                    'overlap_voxels': int(overlap),\n                    'region_size': int(np.sum(region_mask))\n                })\n\n        # Sort by overlap count\n        region_counts = sorted(region_counts, key=lambda x: x['overlap_voxels'], reverse=True)\n\n        if verbose:\n            print(f\"\\nTop regions by significant voxel count:\")\n            for i, r in enumerate(region_counts[:15], 1):\n                pct = 100 * r['overlap_voxels'] / r['region_size']\n                print(f\"{i:2d}. Region {r['region_id']:3d}: {r['overlap_voxels']:4d} sig. voxels \"\n                      f\"({pct:.1f}% of region)\")\n\n        results[atlas_file] = region_counts\n\n    return results\n</code></pre>"},{"location":"reference/stats/#tit.stats.cluster_analysis","title":"cluster_analysis","text":"<pre><code>cluster_analysis(sig_mask, affine, verbose=True)\n</code></pre> <p>Perform cluster analysis on significant voxels</p> Parameters: <p>sig_mask : ndarray (x, y, z)     Binary mask of significant voxels affine : ndarray     Affine transformation matrix verbose : bool     Print progress information</p> Returns: <p>clusters : list of dict     Cluster information including size, center of mass in voxel and MNI coordinates</p> Source code in <code>tit/stats/stats_utils.py</code> <pre><code>def cluster_analysis(sig_mask, affine, verbose=True):\n    \"\"\"\n    Perform cluster analysis on significant voxels\n\n    Parameters:\n    -----------\n    sig_mask : ndarray (x, y, z)\n        Binary mask of significant voxels\n    affine : ndarray\n        Affine transformation matrix\n    verbose : bool\n        Print progress information\n\n    Returns:\n    --------\n    clusters : list of dict\n        Cluster information including size, center of mass in voxel and MNI coordinates\n    \"\"\"\n    import nibabel as nib\n\n    if verbose:\n        print(\"\\nPerforming cluster analysis...\")\n\n    # Find connected clusters\n    labeled_array, num_clusters = label(sig_mask)\n\n    if num_clusters == 0:\n        if verbose:\n            print(\"No clusters found\")\n        return []\n\n    clusters = []\n    for cluster_id in range(1, num_clusters + 1):\n        cluster_mask = (labeled_array == cluster_id)\n        cluster_size = np.sum(cluster_mask)\n\n        # Get center of mass in voxel coordinates\n        coords = np.argwhere(cluster_mask)\n        com_voxel = np.mean(coords, axis=0)\n\n        # Convert to MNI coordinates\n        com_mni = nib.affines.apply_affine(affine, com_voxel)\n\n        clusters.append({\n            'cluster_id': cluster_id,\n            'size': cluster_size,\n            'center_voxel': com_voxel,\n            'center_mni': com_mni\n        })\n\n    # Sort by size\n    clusters = sorted(clusters, key=lambda x: x['size'], reverse=True)\n\n    if verbose:\n        print(f\"Found {num_clusters} clusters\")\n        for c in clusters[:10]:  # Show top 10\n            print(f\"  Cluster {c['cluster_id']}: {c['size']} voxels, \"\n                  f\"MNI center: ({c['center_mni'][0]:.1f}, {c['center_mni'][1]:.1f}, {c['center_mni'][2]:.1f})\")\n\n    return clusters\n</code></pre>"},{"location":"reference/stats/#tit.stats.cluster_based_correction","title":"cluster_based_correction","text":"<pre><code>cluster_based_correction(responders, non_responders, p_values, valid_mask, cluster_threshold=0.01, n_permutations=500, alpha=0.05, test_type='unpaired', alternative='two-sided', cluster_stat='size', t_statistics=None, n_jobs=-1, verbose=True, logger=None, save_permutation_log=False, permutation_log_file=None, subject_ids_resp=None, subject_ids_non_resp=None)\n</code></pre> <p>Apply cluster-based permutation correction for multiple comparisons</p> <p>This implements the cluster-based approach commonly used in neuroimaging. Tests all valid voxels in permutations and uses parallel processing for speed.</p> Parameters: <p>responders : ndarray (x, y, z, n_subjects)     Responder data non_responders : ndarray (x, y, z, n_subjects)     Non-responder data p_values : ndarray (x, y, z)     Uncorrected p-values from initial test valid_mask : ndarray (x, y, z)     Boolean mask of valid voxels cluster_threshold : float     Initial p-value threshold for cluster formation (uncorrected) n_permutations : int     Number of permutations for null distribution (500-1000 recommended) alpha : float     Significance level for cluster-level correction test_type : str     Either 'paired' or 'unpaired' t-test for permutations alternative : {'two-sided', 'greater', 'less'}, optional     Alternative hypothesis (default: 'two-sided') cluster_stat : {'size', 'mass'}, optional     Cluster statistic to use (default: 'size'):     * 'size': count of contiguous significant voxels     * 'mass': sum of t-statistics in contiguous significant voxels t_statistics : ndarray (x, y, z), optional     T-statistics from initial test (required if cluster_stat='mass') n_jobs : int     Number of parallel jobs. -1 uses all available CPU cores. 1 disables parallelization. verbose : bool     Print progress information logger : logging.Logger, optional     Logger instance for output (default: None) save_permutation_log : bool, optional     If True, save detailed permutation information to file (default: False) permutation_log_file : str, optional     Path to save permutation log. If None and save_permutation_log=True,      will use default name subject_ids_resp : list, optional     List of responder subject IDs (for logging) subject_ids_non_resp : list, optional     List of non-responder subject IDs (for logging)</p> Returns: <p>sig_mask : ndarray (x, y, z)     Binary mask of significant voxels cluster_stat_threshold : float     Cluster statistic threshold from permutation distribution sig_clusters : list of dict     Information about significant clusters null_max_cluster_stats : ndarray     Maximum cluster statistics from permutation null distribution cluster_stats : list of dict     All clusters from observed data (for plotting) correlation_data : dict     Dictionary with 'sizes' and 'masses' arrays for correlation analysis</p> Source code in <code>tit/stats/stats_utils.py</code> <pre><code>def cluster_based_correction(responders, non_responders, p_values, valid_mask,\n                            cluster_threshold=0.01, n_permutations=500, alpha=0.05,\n                            test_type='unpaired', alternative='two-sided', cluster_stat='size',\n                            t_statistics=None, n_jobs=-1, verbose=True,\n                            logger=None, save_permutation_log=False, permutation_log_file=None,\n                            subject_ids_resp=None, subject_ids_non_resp=None):\n    \"\"\"\n    Apply cluster-based permutation correction for multiple comparisons\n\n    This implements the cluster-based approach commonly used in neuroimaging.\n    Tests all valid voxels in permutations and uses parallel processing for speed.\n\n    Parameters:\n    -----------\n    responders : ndarray (x, y, z, n_subjects)\n        Responder data\n    non_responders : ndarray (x, y, z, n_subjects)\n        Non-responder data\n    p_values : ndarray (x, y, z)\n        Uncorrected p-values from initial test\n    valid_mask : ndarray (x, y, z)\n        Boolean mask of valid voxels\n    cluster_threshold : float\n        Initial p-value threshold for cluster formation (uncorrected)\n    n_permutations : int\n        Number of permutations for null distribution (500-1000 recommended)\n    alpha : float\n        Significance level for cluster-level correction\n    test_type : str\n        Either 'paired' or 'unpaired' t-test for permutations\n    alternative : {'two-sided', 'greater', 'less'}, optional\n        Alternative hypothesis (default: 'two-sided')\n    cluster_stat : {'size', 'mass'}, optional\n        Cluster statistic to use (default: 'size'):\n        * 'size': count of contiguous significant voxels\n        * 'mass': sum of t-statistics in contiguous significant voxels\n    t_statistics : ndarray (x, y, z), optional\n        T-statistics from initial test (required if cluster_stat='mass')\n    n_jobs : int\n        Number of parallel jobs. -1 uses all available CPU cores. 1 disables parallelization.\n    verbose : bool\n        Print progress information\n    logger : logging.Logger, optional\n        Logger instance for output (default: None)\n    save_permutation_log : bool, optional\n        If True, save detailed permutation information to file (default: False)\n    permutation_log_file : str, optional\n        Path to save permutation log. If None and save_permutation_log=True, \n        will use default name\n    subject_ids_resp : list, optional\n        List of responder subject IDs (for logging)\n    subject_ids_non_resp : list, optional\n        List of non-responder subject IDs (for logging)\n\n    Returns:\n    --------\n    sig_mask : ndarray (x, y, z)\n        Binary mask of significant voxels\n    cluster_stat_threshold : float\n        Cluster statistic threshold from permutation distribution\n    sig_clusters : list of dict\n        Information about significant clusters\n    null_max_cluster_stats : ndarray\n        Maximum cluster statistics from permutation null distribution\n    cluster_stats : list of dict\n        All clusters from observed data (for plotting)\n    correlation_data : dict\n        Dictionary with 'sizes' and 'masses' arrays for correlation analysis\n    \"\"\"\n    try:\n        from .io_utils import save_permutation_details\n    except ImportError:\n        from io_utils import save_permutation_details\n\n    # Validate cluster_stat parameter\n    if cluster_stat not in ['size', 'mass']:\n        raise ValueError(f\"cluster_stat must be 'size' or 'mass', got '{cluster_stat}'\")\n\n    # Check that t_statistics is provided if using cluster mass\n    if cluster_stat == 'mass' and t_statistics is None:\n        raise ValueError(\"t_statistics must be provided when cluster_stat='mass'\")\n\n    if verbose:\n        header = f\"\\n{'='*70}\\nCLUSTER-BASED PERMUTATION CORRECTION\\n{'='*70}\"\n        if logger:\n            logger.info(header)\n        else:\n            print(header)\n        cluster_stat_name = \"Cluster Size\" if cluster_stat == 'size' else \"Cluster Mass\"\n        config_info = f\"Cluster statistic: {cluster_stat_name}\\nCluster-forming threshold: p &lt; {cluster_threshold}\\nNumber of permutations: {n_permutations}\\nCluster-level alpha: {alpha}\"\n        if logger:\n            for line in config_info.split('\\n'):\n                logger.info(line)\n        else:\n            print(config_info)\n\n    # Step 1: Form clusters based on initial threshold\n    initial_mask = (p_values &lt; cluster_threshold) &amp; valid_mask\n    labeled_array, n_clusters = label(initial_mask)\n\n    if verbose:\n        msg = f\"\\nFound {n_clusters} clusters at p &lt; {cluster_threshold} (uncorrected)\"\n        if logger:\n            logger.info(msg)\n        else:\n            print(msg)\n\n    if n_clusters == 0:\n        if verbose:\n            msg = \"No clusters found. Try increasing cluster_threshold (e.g., 0.05)\"\n            if logger:\n                logger.warning(msg)\n            else:\n                print(msg)\n        # Return all 6 expected values with empty/default data\n        empty_correlation_data = {\n            'sizes': np.array([]),\n            'masses': np.array([])\n        }\n        return np.zeros_like(p_values, dtype=int), cluster_threshold, [], np.array([]), [], empty_correlation_data\n\n    # Calculate cluster statistics efficiently without storing all clusters\n    if n_clusters &gt; 1000 and verbose:\n        warning_msg = f\"\\nWARNING: Found {n_clusters} clusters! This is unusually high.\\nConsider:\\n  1. Using a stricter cluster_threshold (e.g., 0.01 instead of 0.05)\\n  2. Checking if your data is properly masked\\n  3. Verifying your p-values are computed correctly\"\n        if logger:\n            logger.warning(warning_msg)\n        else:\n            print(warning_msg)\n\n    # Find top clusters for display without storing all in memory\n    if verbose:\n        msg = f\"\\nFinding largest clusters for summary...\"\n        if logger:\n            logger.info(msg)\n        else:\n            print(msg)\n        top_clusters = []\n\n        # Find top 10 clusters by their statistic\n        for cluster_id in range(1, n_clusters + 1):\n            cluster_mask = (labeled_array == cluster_id)\n            size = np.sum(cluster_mask)\n\n            if size &gt; 1:  # Only multi-voxel clusters\n                if cluster_stat == 'size':\n                    stat_value = float(size)\n                elif cluster_stat == 'mass':\n                    stat_value = float(np.sum(t_statistics[cluster_mask]))\n\n                # Keep only top 10\n                if len(top_clusters) &lt; 10:\n                    top_clusters.append((cluster_id, size, stat_value))\n                    top_clusters.sort(key=lambda x: x[2], reverse=True)\n                elif stat_value &gt; top_clusters[-1][2]:\n                    top_clusters[-1] = (cluster_id, size, stat_value)\n                    top_clusters.sort(key=lambda x: x[2], reverse=True)\n\n        if top_clusters:\n            # Show summary of top clusters\n            if cluster_stat == 'size':\n                msg = f\"\\nTop {len(top_clusters)} clusters (largest: {top_clusters[0][2]:.0f} voxels)\"\n            else:\n                msg = f\"\\nTop {len(top_clusters)} clusters (largest mass: {top_clusters[0][2]:.2f})\"\n\n            if logger:\n                logger.info(msg)\n            else:\n                print(msg)\n\n            for i, (cid, size, stat_value) in enumerate(top_clusters[:5], 1):\n                if cluster_stat == 'size':\n                    msg = f\"  {i}. Cluster {cid}: {size} voxels\"\n                else:\n                    msg = f\"  {i}. Cluster {cid}: {size} voxels, mass = {stat_value:.2f}\"\n                if logger:\n                    logger.info(msg)\n                else:\n                    print(msg)\n        else:\n            msg = f\"\\nNo multi-voxel clusters found\"\n            if logger:\n                logger.info(msg)\n            else:\n                print(msg)\n\n    # Step 2: Test all valid voxels in permutations\n    test_mask = valid_mask\n    test_coords = np.argwhere(test_mask)\n    n_test_voxels = len(test_coords)\n\n    if verbose:\n        msg = f\"\\nTesting all {n_test_voxels} valid voxels in permutations\"\n        if logger:\n            logger.info(msg)\n        else:\n            print(msg)\n\n    # Step 3: Permutation testing\n    if verbose:\n        msg = f\"\\nRunning {n_permutations} permutations...\"\n        if logger:\n            logger.info(msg)\n        else:\n            print(msg)\n\n    # Combine all subjects\n    all_data = np.concatenate([responders, non_responders], axis=-1)\n    n_resp = responders.shape[-1]\n    n_non_resp = non_responders.shape[-1]\n    n_total = n_resp + n_non_resp\n\n    # Pre-extract data for test voxels\n    if verbose:\n        msg = f\"Pre-extracting voxel data for faster permutations...\\n  Test voxels: {n_test_voxels:,}\\n  Total subjects: {n_total}\"\n        if logger:\n            logger.info(msg)\n        else:\n            print(msg)\n\n    # Use float32 to save memory\n    test_data = np.zeros((n_test_voxels, n_total), dtype=np.float32)\n    for idx, coord in enumerate(test_coords):\n        i, j, k = coord[0], coord[1], coord[2]\n        test_data[idx, :] = all_data[i, j, k, :].astype(np.float32)\n\n    # Free the original combined data array\n    del all_data\n    import gc\n    gc.collect()\n\n    # Report memory usage\n    if verbose:\n        test_data_mb = test_data.nbytes / (1024**2)\n        msg = f\"  Test data size: {test_data_mb:.1f} MB\"\n        if logger:\n            logger.info(msg)\n        else:\n            print(msg)\n\n    # Determine number of jobs\n    if n_jobs == -1:\n        n_jobs = multiprocessing.cpu_count()\n        if verbose:\n            print(f\"Auto-detected {n_jobs} CPU cores\")\n\n    if verbose:\n        if n_jobs == 1:\n            print(\"Running permutations sequentially (1 core)...\")\n        else:\n            print(f\"Running permutations in parallel using {n_jobs} cores...\")\n\n    # Run permutations in parallel\n    # Use seeds for reproducibility\n    seeds = np.random.randint(0, 2**31, size=n_permutations)\n\n    # Determine if we need to track permutation indices\n    track_indices = save_permutation_log and subject_ids_resp is not None and subject_ids_non_resp is not None\n\n    if n_jobs == 1:\n        # Sequential execution with progress bar\n        null_max_cluster_stats = []\n        null_max_cluster_sizes = []\n        null_max_cluster_masses = []\n        permutation_indices = [] if track_indices else None\n        iterator = tqdm(range(n_permutations), desc=\"Permutations\") if verbose else range(n_permutations)\n        for perm in iterator:\n            result = _run_single_permutation(\n                test_data, test_coords, n_resp, n_total,\n                cluster_threshold, valid_mask, p_values.shape,\n                test_type=test_type,\n                alternative=alternative,\n                cluster_stat=cluster_stat,\n                seed=seeds[perm],\n                return_indices=track_indices\n            )\n            if track_indices:\n                max_stat, perm_idx, max_size, max_mass = result\n                null_max_cluster_stats.append(max_stat)\n                null_max_cluster_sizes.append(max_size)\n                null_max_cluster_masses.append(max_mass)\n                permutation_indices.append(perm_idx)\n            else:\n                max_stat, max_size, max_mass = result\n                null_max_cluster_stats.append(max_stat)\n                null_max_cluster_sizes.append(max_size)\n                null_max_cluster_masses.append(max_mass)\n    else:\n        # Parallel execution using joblib - same as local implementation\n        if verbose:\n            print(f\"Using joblib with {n_jobs} processes...\")\n\n        # Run permutations with joblib\n        results = Parallel(n_jobs=n_jobs, verbose=10 if verbose else 0)(\n            delayed(_run_single_permutation)(\n                test_data, test_coords, n_resp, n_total,\n                cluster_threshold, valid_mask, p_values.shape,\n                test_type=test_type,\n                alternative=alternative,\n                cluster_stat=cluster_stat,\n                seed=seeds[perm],\n                return_indices=track_indices\n            ) for perm in range(n_permutations)\n        )\n\n        # Clean up parallel processing resources\n        import gc\n        gc.collect()\n\n        if verbose:\n            print(f\"All {n_permutations} permutations completed\")\n\n        if track_indices:\n            null_max_cluster_stats = [r[0] for r in results]\n            permutation_indices = [r[1] for r in results]\n            null_max_cluster_sizes = [r[2] for r in results]\n            null_max_cluster_masses = [r[3] for r in results]\n        else:\n            null_max_cluster_stats = [r[0] for r in results]\n            null_max_cluster_sizes = [r[1] for r in results]\n            null_max_cluster_masses = [r[2] for r in results]\n            permutation_indices = None\n\n    # Explicitly free memory from test_data and other large arrays\n    del test_data\n    del test_coords\n    gc.collect()\n\n    # Step 4: Determine cluster statistic threshold using discrete approach\n    null_max_cluster_stats = np.array(null_max_cluster_stats)\n\n    # For p &lt; alpha, we need (count &gt;= observed) &lt; alpha * n_permutations\n    # The threshold is the value where exactly alpha * n_permutations values exceed it\n    sorted_null = np.sort(null_max_cluster_stats)[::-1]  # Sort descending\n    threshold_index = int(alpha * n_permutations)\n    if threshold_index == 0:\n        threshold_index = 1  # Need at least 1 for edge case\n    if threshold_index &gt; len(sorted_null):\n        threshold_index = len(sorted_null)\n\n    # Discrete threshold: the value where exactly alpha proportion exceeds it\n    cluster_stat_threshold = sorted_null[threshold_index - 1]  # -1 for 0-indexing\n\n    if verbose:\n        stat_unit = \"voxels\" if cluster_stat == 'size' else \"mass units\"\n        msg = f\"\\nDiscrete threshold for significance (p &lt; {alpha}): {cluster_stat_threshold:.2f} {stat_unit}\\n  (This is the {threshold_index}th largest value out of {n_permutations} permutations)\\nNull distribution stats: min={np.min(null_max_cluster_stats):.2f}, mean={np.mean(null_max_cluster_stats):.2f}, max={np.max(null_max_cluster_stats):.2f}\"\n        if logger:\n            logger.info(msg)\n        else:\n            print(msg)\n\n    # Save permutation details if requested\n    if save_permutation_log and permutation_indices is not None:\n        if permutation_log_file is None:\n            permutation_log_file = \"permutation_details.txt\"\n\n        # Prepare permutation info\n        permutation_info = []\n        for perm_num in range(n_permutations):\n            permutation_info.append({\n                'perm_num': perm_num,\n                'perm_idx': permutation_indices[perm_num],\n                'max_cluster_size': null_max_cluster_stats[perm_num]\n            })\n\n        save_permutation_details(permutation_info, permutation_log_file, \n                                subject_ids_resp, subject_ids_non_resp)\n\n        if verbose:\n            print(f\"Saved permutation details to: {permutation_log_file}\")\n\n    # Step 5: Identify significant clusters using per-cluster p-values (MNE-style)\n    # Include observed max in null distribution for proper p-value computation\n    # Reference: Maris &amp; Oostenveld (2007), MNE-Python implementation\n    sig_mask = np.zeros_like(p_values, dtype=int)\n    sig_clusters = []\n    all_cluster_stats = []  # Store all cluster statistics for p-value computation\n\n    # First pass: collect all cluster statistics\n    max_clusters_to_check = min(n_clusters, 10000)  # Safety limit\n\n    if n_clusters &gt; max_clusters_to_check and verbose:\n        print(f\"\\nNote: Checking first {max_clusters_to_check} clusters for significance (out of {n_clusters} total)\")\n\n    cluster_info_list = []\n    for cluster_id in range(1, max_clusters_to_check + 1):\n        # Get cluster coordinates without creating full boolean mask\n        cluster_coords = np.where(labeled_array == cluster_id)\n        size = len(cluster_coords[0])\n\n        if size &gt; 1:  # Only multi-voxel clusters\n            if cluster_stat == 'size':\n                stat_value = float(size)\n            elif cluster_stat == 'mass':\n                stat_value = float(np.sum(t_statistics[cluster_coords]))\n\n            cluster_info_list.append({\n                'id': cluster_id,\n                'size': size,\n                'stat_value': stat_value,\n                'coords': cluster_coords\n            })\n            all_cluster_stats.append(stat_value)\n\n    # Compute per-cluster p-values using MNE-style approach\n    # Add observed max to null distribution for proper family-wise error control\n    if all_cluster_stats:\n        all_cluster_stats = np.array(all_cluster_stats)\n\n        # Determine tail based on alternative hypothesis\n        if alternative == 'greater':\n            tail = 1\n        elif alternative == 'less':\n            tail = -1\n        else:\n            tail = 0\n\n        # Compute p-values for each cluster\n        cluster_pvalues = pval_from_histogram(all_cluster_stats, null_max_cluster_stats, tail=tail)\n\n        # Log top 10 clusters with their p-values for transparency\n        if verbose and len(cluster_info_list) &gt; 0:\n            msg = f\"\\nTop {min(10, len(cluster_info_list))} observed clusters with p-values:\"\n            if logger:\n                logger.info(msg)\n            else:\n                print(msg)\n\n            for i in range(min(10, len(cluster_info_list))):\n                info = cluster_info_list[i]\n                cluster_pv = cluster_pvalues[i]\n                status = \"SIGNIFICANT\" if cluster_pv &lt; alpha else \"not significant\"\n                if cluster_stat == 'size':\n                    msg = (f\"  Cluster {info['id']}: {info['size']} voxels, \"\n                           f\"p={cluster_pv:.4f} ({status})\")\n                else:\n                    msg = (f\"  Cluster {info['id']}: mass={info['stat_value']:.2f}, \"\n                           f\"size={info['size']}, p={cluster_pv:.4f} ({status})\")\n                if logger:\n                    logger.info(msg)\n                else:\n                    print(msg)\n\n        # Collect top observed clusters for visualization (top 10 by cluster statistic)\n        all_observed_clusters = []\n        for i, info in enumerate(cluster_info_list[:min(10, len(cluster_info_list))]):\n            all_observed_clusters.append({\n                'id': info['id'],\n                'size': info['size'],\n                'stat_value': info['stat_value'],\n                'p_value': cluster_pvalues[i]\n            })\n\n        # Second pass: identify significant clusters\n        for i, info in enumerate(cluster_info_list):\n            cluster_pv = cluster_pvalues[i]\n\n            # Check if significant using per-cluster p-value\n            if cluster_pv &lt; alpha:\n                sig_mask[info['coords']] = 1\n                sig_clusters.append({\n                    'id': info['id'],\n                    'size': info['size'],\n                    'stat_value': info['stat_value'],\n                    'p_value': cluster_pv\n                })\n\n    if verbose:\n        msg = f\"\\nSignificant clusters (p &lt; {alpha}): {len(sig_clusters)}\\nTotal significant voxels: {np.sum(sig_mask)}\"\n        if logger:\n            logger.info(msg)\n        else:\n            print(msg)\n\n    # Prepare correlation data\n    correlation_data = {\n        'sizes': np.array(null_max_cluster_sizes),\n        'masses': np.array(null_max_cluster_masses)\n    }\n\n    return sig_mask, cluster_stat_threshold, sig_clusters, null_max_cluster_stats, all_observed_clusters, correlation_data\n</code></pre>"},{"location":"reference/stats/#tit.stats.correlation_cluster_correction","title":"correlation_cluster_correction","text":"<pre><code>correlation_cluster_correction(subject_data, effect_sizes, r_values, t_statistics, p_values, valid_mask, weights=None, correlation_type='pearson', cluster_threshold=0.05, n_permutations=1000, alpha=0.05, cluster_stat='mass', alternative='two-sided', n_jobs=-1, verbose=True, logger=None, save_permutation_log=False, permutation_log_file=None, subject_ids=None)\n</code></pre> <p>Apply cluster-based permutation correction for correlation analysis.</p> <p>This implements the ACES approach where effect sizes are shuffled to  break the association between E-field magnitude and outcome.</p> Parameters: <p>subject_data : ndarray (x, y, z, n_subjects)     Electric field magnitude data effect_sizes : ndarray (n_subjects,)     Continuous outcome measure r_values : ndarray (x, y, z)     Correlation coefficients from initial test t_statistics : ndarray (x, y, z)     T-statistics from initial test p_values : ndarray (x, y, z)     P-values from initial test valid_mask : ndarray (x, y, z)     Boolean mask of valid voxels weights : ndarray (n_subjects,), optional     Subject weights for weighted correlation correlation_type : str     'pearson' or 'spearman' cluster_threshold : float     P-value threshold for cluster formation n_permutations : int     Number of permutations alpha : float     Significance level for cluster-level correction cluster_stat : str     'size' or 'mass' (sum of t-values in cluster) alternative : {'two-sided', 'greater', 'less'}, optional     Defines the alternative hypothesis (default: 'two-sided'):     * 'two-sided': test both positive and negative correlations     * 'greater': test positive correlations only (one-tailed, uses full alpha)     * 'less': test negative correlations only (one-tailed, uses full alpha) n_jobs : int     Number of parallel jobs (-1 = all cores) verbose : bool     Print progress information logger : logging.Logger, optional     Logger instance save_permutation_log : bool     Save permutation details permutation_log_file : str, optional     Path for permutation log subject_ids : list, optional     Subject IDs for logging</p> Returns: <p>sig_mask : ndarray (x, y, z)     Binary mask of significant voxels cluster_stat_threshold : float     Cluster statistic threshold from null distribution sig_clusters : list of dict     Information about significant clusters null_distribution : ndarray     Maximum cluster statistics from permutations all_clusters : list     All observed clusters correlation_data : dict     Cluster size and mass data for analysis</p> Source code in <code>tit/stats/stats_utils.py</code> <pre><code>def correlation_cluster_correction(subject_data, effect_sizes, r_values, t_statistics,\n                                   p_values, valid_mask, weights=None,\n                                   correlation_type='pearson', cluster_threshold=0.05,\n                                   n_permutations=1000, alpha=0.05, cluster_stat='mass',\n                                   alternative='two-sided', n_jobs=-1, verbose=True, logger=None,\n                                   save_permutation_log=False, permutation_log_file=None,\n                                   subject_ids=None):\n    \"\"\"\n    Apply cluster-based permutation correction for correlation analysis.\n\n    This implements the ACES approach where effect sizes are shuffled to \n    break the association between E-field magnitude and outcome.\n\n    Parameters:\n    -----------\n    subject_data : ndarray (x, y, z, n_subjects)\n        Electric field magnitude data\n    effect_sizes : ndarray (n_subjects,)\n        Continuous outcome measure\n    r_values : ndarray (x, y, z)\n        Correlation coefficients from initial test\n    t_statistics : ndarray (x, y, z)\n        T-statistics from initial test\n    p_values : ndarray (x, y, z)\n        P-values from initial test\n    valid_mask : ndarray (x, y, z)\n        Boolean mask of valid voxels\n    weights : ndarray (n_subjects,), optional\n        Subject weights for weighted correlation\n    correlation_type : str\n        'pearson' or 'spearman'\n    cluster_threshold : float\n        P-value threshold for cluster formation\n    n_permutations : int\n        Number of permutations\n    alpha : float\n        Significance level for cluster-level correction\n    cluster_stat : str\n        'size' or 'mass' (sum of t-values in cluster)\n    alternative : {'two-sided', 'greater', 'less'}, optional\n        Defines the alternative hypothesis (default: 'two-sided'):\n        * 'two-sided': test both positive and negative correlations\n        * 'greater': test positive correlations only (one-tailed, uses full alpha)\n        * 'less': test negative correlations only (one-tailed, uses full alpha)\n    n_jobs : int\n        Number of parallel jobs (-1 = all cores)\n    verbose : bool\n        Print progress information\n    logger : logging.Logger, optional\n        Logger instance\n    save_permutation_log : bool\n        Save permutation details\n    permutation_log_file : str, optional\n        Path for permutation log\n    subject_ids : list, optional\n        Subject IDs for logging\n\n    Returns:\n    --------\n    sig_mask : ndarray (x, y, z)\n        Binary mask of significant voxels\n    cluster_stat_threshold : float\n        Cluster statistic threshold from null distribution\n    sig_clusters : list of dict\n        Information about significant clusters\n    null_distribution : ndarray\n        Maximum cluster statistics from permutations\n    all_clusters : list\n        All observed clusters\n    correlation_data : dict\n        Cluster size and mass data for analysis\n    \"\"\"\n    try:\n        from .io_utils import save_permutation_details\n    except ImportError:\n        from io_utils import save_permutation_details\n\n    effect_sizes = np.asarray(effect_sizes, dtype=np.float64)\n    n_subjects = len(effect_sizes)\n\n    if verbose:\n        header = f\"\\n{'='*70}\\nCORRELATION CLUSTER-BASED PERMUTATION CORRECTION\\n{'='*70}\"\n        if logger:\n            logger.info(header)\n        else:\n            print(header)\n\n        stat_name = \"Cluster Size\" if cluster_stat == 'size' else \"Cluster Mass\"\n        weight_str = \" (weighted)\" if weights is not None else \"\"\n        alt_text = {\n            'two-sided': 'two-sided (tests both positive and negative correlations)',\n            'greater': 'one-sided (tests positive correlations only, full alpha)',\n            'less': 'one-sided (tests negative correlations only, full alpha)'\n        }\n        config_info = (f\"Correlation type: {correlation_type.capitalize()}{weight_str}\\n\"\n                      f\"Alternative hypothesis: {alt_text.get(alternative, alternative)}\\n\"\n                      f\"Cluster statistic: {stat_name}\\n\"\n                      f\"Cluster-forming threshold: p &lt; {cluster_threshold}\\n\"\n                      f\"Number of permutations: {n_permutations}\\n\"\n                      f\"Cluster-level alpha: {alpha}\")\n        if logger:\n            for line in config_info.split('\\n'):\n                logger.info(line)\n        else:\n            print(config_info)\n\n    # Form clusters based on initial threshold and alternative hypothesis\n    if alternative == 'greater':\n        # Test positive correlations only\n        initial_mask = (p_values &lt; cluster_threshold) &amp; valid_mask &amp; (t_statistics &gt; 0)\n        corr_type_str = \"positive correlations\"\n    elif alternative == 'less':\n        # Test negative correlations only\n        initial_mask = (p_values &lt; cluster_threshold) &amp; valid_mask &amp; (t_statistics &lt; 0)\n        corr_type_str = \"negative correlations\"\n    else:\n        # Two-sided: test all significant correlations\n        initial_mask = (p_values &lt; cluster_threshold) &amp; valid_mask\n        corr_type_str = \"correlations (both positive and negative)\"\n\n    labeled_array, n_clusters = label(initial_mask)\n\n    if verbose:\n        msg = f\"\\nFound {n_clusters} clusters at p &lt; {cluster_threshold} ({corr_type_str})\"\n        if logger:\n            logger.info(msg)\n        else:\n            print(msg)\n\n    if n_clusters == 0:\n        if verbose:\n            msg = \"No clusters found. Try increasing cluster_threshold.\"\n            if logger:\n                logger.warning(msg)\n            else:\n                print(msg)\n        empty_data = {'sizes': np.array([]), 'masses': np.array([])}\n        return np.zeros_like(p_values, dtype=int), 0, [], np.array([]), [], empty_data\n\n    # Pre-extract voxel data\n    valid_coords = np.argwhere(valid_mask)\n    n_valid = len(valid_coords)\n\n    voxel_data = np.zeros((n_valid, n_subjects), dtype=np.float64)\n    for idx, coord in enumerate(valid_coords):\n        i, j, k = coord\n        voxel_data[idx, :] = subject_data[i, j, k, :]\n\n    # OPTIMIZATION: For Spearman, pre-rank voxel data ONCE before permutations\n    # This avoids re-ranking the same data in every permutation (huge speedup!)\n    voxel_data_preranked = False\n    if correlation_type == 'spearman':\n        if verbose:\n            print(f\"Pre-ranking voxel data for Spearman correlation (one-time operation)...\")\n        voxel_data = np.apply_along_axis(rankdata, 1, voxel_data)\n        voxel_data_preranked = True\n        if verbose:\n            print(f\"  \u2713 Voxel data pre-ranked ({n_valid} voxels)\")\n\n    if verbose:\n        print(f\"\\nRunning {n_permutations} permutations...\")\n\n    # Determine number of jobs\n    if n_jobs == -1:\n        n_jobs = multiprocessing.cpu_count()\n\n    # Generate seeds for reproducibility\n    seeds = np.random.randint(0, 2**31, size=n_permutations)\n\n    track_indices = save_permutation_log and subject_ids is not None\n\n    if n_jobs == 1:\n        # Sequential execution\n        null_max_cluster_stats = []\n        null_max_cluster_sizes = []\n        null_max_cluster_masses = []\n        permutation_indices = [] if track_indices else None\n\n        iterator = tqdm(range(n_permutations), desc=\"Permutations\") if verbose else range(n_permutations)\n        for perm in iterator:\n            result = _run_single_correlation_permutation(\n                voxel_data, effect_sizes, valid_coords,\n                cluster_threshold, valid_mask, p_values.shape,\n                correlation_type=correlation_type, weights=weights,\n                cluster_stat=cluster_stat, alternative=alternative, seed=seeds[perm],\n                return_indices=track_indices,\n                voxel_data_preranked=voxel_data_preranked\n            )\n            if track_indices:\n                max_stat, perm_idx, max_size, max_mass = result\n                null_max_cluster_stats.append(max_stat)\n                null_max_cluster_sizes.append(max_size)\n                null_max_cluster_masses.append(max_mass)\n                permutation_indices.append(perm_idx)\n            else:\n                max_stat, max_size, max_mass = result\n                null_max_cluster_stats.append(max_stat)\n                null_max_cluster_sizes.append(max_size)\n                null_max_cluster_masses.append(max_mass)\n    else:\n        # Parallel execution\n        if verbose:\n            print(f\"Using {n_jobs} parallel processes...\")\n\n        results = Parallel(n_jobs=n_jobs, verbose=10 if verbose else 0)(\n            delayed(_run_single_correlation_permutation)(\n                voxel_data, effect_sizes, valid_coords,\n                cluster_threshold, valid_mask, p_values.shape,\n                correlation_type=correlation_type, weights=weights,\n                cluster_stat=cluster_stat, alternative=alternative, seed=seeds[perm],\n                return_indices=track_indices,\n                voxel_data_preranked=voxel_data_preranked\n            ) for perm in range(n_permutations)\n        )\n\n        # Clean up parallel processing resources\n        import gc\n        gc.collect()\n\n        if track_indices:\n            null_max_cluster_stats = [r[0] for r in results]\n            permutation_indices = [r[1] for r in results]\n            null_max_cluster_sizes = [r[2] for r in results]\n            null_max_cluster_masses = [r[3] for r in results]\n        else:\n            null_max_cluster_stats = [r[0] for r in results]\n            null_max_cluster_sizes = [r[1] for r in results]\n            null_max_cluster_masses = [r[2] for r in results]\n            permutation_indices = None\n\n    # Clean up\n    del voxel_data\n    gc.collect()\n\n    # Determine threshold using discrete approach (consistent with exact p-values)\n    null_max_cluster_stats = np.array(null_max_cluster_stats)\n\n    # For p &lt; alpha, we need (count &gt;= observed) &lt; alpha * n_permutations\n    # The threshold is the value where exactly alpha * n_permutations values exceed it\n    sorted_null = np.sort(null_max_cluster_stats)[::-1]  # Sort descending\n    threshold_index = int(alpha * n_permutations)\n    if threshold_index == 0:\n        threshold_index = 1  # Need at least 1 for edge case\n    if threshold_index &gt; len(sorted_null):\n        threshold_index = len(sorted_null)\n\n    # Discrete threshold: the value where exactly alpha proportion exceeds it\n    cluster_stat_threshold = sorted_null[threshold_index - 1]  # -1 for 0-indexing\n\n    if verbose:\n        stat_unit = \"voxels\" if cluster_stat == 'size' else \"mass units\"\n        msg = (f\"\\nDiscrete threshold for significance (p &lt; {alpha}): \"\n               f\"{cluster_stat_threshold:.2f} {stat_unit}\\n\"\n               f\"  (This is the {threshold_index}th largest value out of {n_permutations} permutations)\\n\"\n               f\"Null distribution: min={np.min(null_max_cluster_stats):.2f}, \"\n               f\"mean={np.mean(null_max_cluster_stats):.2f}, \"\n               f\"max={np.max(null_max_cluster_stats):.2f}\")\n        if logger:\n            logger.info(msg)\n        else:\n            print(msg)\n\n    # Identify significant clusters using per-cluster p-values (MNE-style)\n    sig_mask = np.zeros_like(p_values, dtype=int)\n    sig_clusters = []\n\n    # Safety limit to prevent memory issues with very large number of clusters\n    max_clusters_to_check = min(n_clusters, 10000)\n\n    if n_clusters &gt; max_clusters_to_check and verbose:\n        print(f\"\\nNote: Checking first {max_clusters_to_check} clusters for significance (out of {n_clusters} total)\")\n\n    # First pass: collect cluster statistics WITHOUT storing full masks (memory efficient)\n    # We only store the cluster_id and compute masks later for significant clusters only\n    cluster_info_list = []\n    all_cluster_stats = []\n\n    # Use scipy.ndimage for efficient cluster statistics\n    from scipy.ndimage import sum as ndimage_sum\n\n    # Get cluster sizes efficiently using bincount\n    cluster_labels_flat = labeled_array.ravel()\n    cluster_sizes_all = np.bincount(cluster_labels_flat, minlength=n_clusters + 1)\n\n    # Get cluster masses using ndimage.sum (vectorized) - only if needed\n    cluster_ids_all = np.arange(1, n_clusters + 1)\n    if cluster_stat == 'mass':\n        cluster_masses_all = ndimage_sum(t_statistics, labeled_array, index=cluster_ids_all)\n\n    for cluster_id in range(1, max_clusters_to_check + 1):\n        size = cluster_sizes_all[cluster_id]\n\n        if size &gt; 1:\n            if cluster_stat == 'size':\n                stat_value = float(size)\n            else:\n                stat_value = float(cluster_masses_all[cluster_id - 1])\n\n            # Don't store masks here - only store ID and stats\n            cluster_info_list.append({\n                'id': cluster_id,\n                'size': int(size),\n                'stat_value': stat_value\n            })\n            all_cluster_stats.append(stat_value)\n\n    # Compute per-cluster p-values using MNE-style approach\n    if all_cluster_stats:\n        all_cluster_stats = np.array(all_cluster_stats)\n\n        # Determine tail based on alternative hypothesis\n        if alternative == 'greater':\n            tail = 1  # Test positive correlations (upper tail)\n        elif alternative == 'less':\n            tail = -1  # Test negative correlations (lower tail)\n        else:\n            tail = 0  # Two-sided test\n\n        cluster_pvalues = pval_from_histogram(all_cluster_stats, null_max_cluster_stats, tail=tail)\n\n        # Log top 10 clusters with their p-values for transparency\n        if verbose and len(cluster_info_list) &gt; 0:\n            msg = f\"\\nTop {min(10, len(cluster_info_list))} observed clusters with p-values:\"\n            if logger:\n                logger.info(msg)\n            else:\n                print(msg)\n\n            for i in range(min(10, len(cluster_info_list))):\n                info = cluster_info_list[i]\n                cluster_pv = cluster_pvalues[i]\n                status = \"SIGNIFICANT\" if cluster_pv &lt; alpha else \"not significant\"\n                msg = (f\"  Cluster {info['id']}: mass={info['stat_value']:.2f}, \"\n                       f\"size={info['size']}, p={cluster_pv:.4f} ({status})\")\n                if logger:\n                    logger.info(msg)\n                else:\n                    print(msg)\n\n        # Collect top observed clusters for visualization (top 10 by cluster mass)\n        all_observed_clusters = []\n        for i, info in enumerate(cluster_info_list[:min(10, len(cluster_info_list))]):\n            all_observed_clusters.append({\n                'id': info['id'],\n                'size': info['size'],\n                'stat_value': info['stat_value'],\n                'p_value': cluster_pvalues[i]\n            })\n\n        # Second pass: identify significant clusters and compute masks ONLY for those\n        for i, info in enumerate(cluster_info_list):\n            cluster_pv = cluster_pvalues[i]\n\n            if cluster_pv &lt; alpha:\n                # Get cluster coordinates and set significant mask (memory efficient)\n                cluster_coords = np.where(labeled_array == info['id'])\n                sig_mask[cluster_coords] = 1\n\n                # Compute additional stats only for significant clusters\n                cluster_r_values = r_values[cluster_coords]\n                peak_r = np.max(cluster_r_values)\n                mean_r = np.mean(cluster_r_values)\n\n                sig_clusters.append({\n                    'id': info['id'],\n                    'size': info['size'],\n                    'stat_value': info['stat_value'],\n                    'peak_r': peak_r,\n                    'mean_r': mean_r,\n                    'p_value': cluster_pv\n                })\n\n    if verbose:\n        msg = (f\"\\nSignificant clusters (p &lt; {alpha}): {len(sig_clusters)}\\n\"\n               f\"Total significant voxels: {np.sum(sig_mask)}\")\n        if logger:\n            logger.info(msg)\n        else:\n            print(msg)\n\n    correlation_data = {\n        'sizes': np.array(null_max_cluster_sizes),\n        'masses': np.array(null_max_cluster_masses)\n    }\n\n    return sig_mask, cluster_stat_threshold, sig_clusters, null_max_cluster_stats, all_observed_clusters, correlation_data\n</code></pre>"},{"location":"reference/stats/#tit.stats.correlation_voxelwise","title":"correlation_voxelwise","text":"<pre><code>correlation_voxelwise(subject_data, effect_sizes, weights=None, correlation_type='pearson', verbose=True)\n</code></pre> <p>Perform vectorized correlation at each voxel between electric field magnitude and continuous outcome measure.</p> <p>This implements the ACES (Automated Correlation of Electric field  strength and Stimulation effect) approach for continuous outcomes.</p> Parameters: <p>subject_data : ndarray (x, y, z, n_subjects)     Electric field magnitude data for all subjects effect_sizes : ndarray (n_subjects,)     Continuous outcome measure for each subject (e.g., effect size,      % improvement, behavioral score) weights : ndarray (n_subjects,), optional     Weights for each subject (e.g., sample size for meta-analysis) correlation_type : {'pearson', 'spearman'}, optional     Type of correlation (default: 'pearson') verbose : bool     Print progress information</p> Returns: <p>r_values : ndarray (x, y, z)     Correlation coefficient at each voxel t_statistics : ndarray (x, y, z)     Studentized correlation (t-statistic) at each voxel p_values : ndarray (x, y, z)     P-value at each voxel valid_mask : ndarray (x, y, z)     Boolean mask of valid voxels</p> Source code in <code>tit/stats/stats_utils.py</code> <pre><code>def correlation_voxelwise(subject_data, effect_sizes, weights=None, \n                          correlation_type='pearson', verbose=True):\n    \"\"\"\n    Perform vectorized correlation at each voxel between electric field\n    magnitude and continuous outcome measure.\n\n    This implements the ACES (Automated Correlation of Electric field \n    strength and Stimulation effect) approach for continuous outcomes.\n\n    Parameters:\n    -----------\n    subject_data : ndarray (x, y, z, n_subjects)\n        Electric field magnitude data for all subjects\n    effect_sizes : ndarray (n_subjects,)\n        Continuous outcome measure for each subject (e.g., effect size, \n        % improvement, behavioral score)\n    weights : ndarray (n_subjects,), optional\n        Weights for each subject (e.g., sample size for meta-analysis)\n    correlation_type : {'pearson', 'spearman'}, optional\n        Type of correlation (default: 'pearson')\n    verbose : bool\n        Print progress information\n\n    Returns:\n    --------\n    r_values : ndarray (x, y, z)\n        Correlation coefficient at each voxel\n    t_statistics : ndarray (x, y, z)\n        Studentized correlation (t-statistic) at each voxel\n    p_values : ndarray (x, y, z)\n        P-value at each voxel\n    valid_mask : ndarray (x, y, z)\n        Boolean mask of valid voxels\n    \"\"\"\n    if verbose:\n        weight_str = \" (weighted)\" if weights is not None else \"\"\n        print(f\"\\nPerforming voxelwise {correlation_type.capitalize()} correlation{weight_str} (vectorized)...\")\n\n    # Validate inputs\n    n_subjects = subject_data.shape[-1]\n    if len(effect_sizes) != n_subjects:\n        raise ValueError(f\"Number of effect sizes ({len(effect_sizes)}) must match \"\n                        f\"number of subjects ({n_subjects})\")\n\n    if weights is not None and len(weights) != n_subjects:\n        raise ValueError(f\"Number of weights ({len(weights)}) must match \"\n                        f\"number of subjects ({n_subjects})\")\n\n    if n_subjects &lt; 3:\n        raise ValueError(f\"Need at least 3 subjects for correlation, got {n_subjects}\")\n\n    effect_sizes = np.asarray(effect_sizes, dtype=np.float64)\n\n    shape = subject_data.shape[:3]\n    r_values = np.zeros(shape)\n    t_statistics = np.zeros(shape)\n    p_values = np.ones(shape)\n\n    # Create mask of valid voxels (non-zero in at least some subjects)\n    valid_mask = np.any(subject_data &gt; 0, axis=-1)\n    total_voxels = np.sum(valid_mask)\n\n    if verbose:\n        print(f\"Computing correlations for {total_voxels} valid voxels...\")\n\n    # Get coordinates of valid voxels\n    valid_coords = np.argwhere(valid_mask)\n    n_valid = len(valid_coords)\n\n    # Pre-extract voxel data for vectorized computation\n    voxel_data = np.zeros((n_valid, n_subjects), dtype=np.float64)\n    for idx, coord in enumerate(valid_coords):\n        i, j, k = coord\n        voxel_data[idx, :] = subject_data[i, j, k, :]\n\n    # Compute correlations for all voxels at once\n    r_1d, t_1d, p_1d = correlation(\n        voxel_data, effect_sizes,\n        correlation_type=correlation_type,\n        weights=weights\n    )\n\n    # Map results back to 3D volume\n    for idx, coord in enumerate(valid_coords):\n        i, j, k = coord\n        r_values[i, j, k] = r_1d[idx]\n        t_statistics[i, j, k] = t_1d[idx]\n        p_values[i, j, k] = p_1d[idx]\n\n    if verbose:\n        print(f\"Correlation range: [{np.min(r_values[valid_mask]):.4f}, {np.max(r_values[valid_mask]):.4f}]\")\n        print(f\"Mean |r|: {np.mean(np.abs(r_values[valid_mask])):.4f}\")\n        sig_05 = np.sum((p_values &lt; 0.05) &amp; valid_mask)\n        sig_01 = np.sum((p_values &lt; 0.01) &amp; valid_mask)\n        print(f\"Voxels with p&lt;0.05 (uncorrected): {sig_05}\")\n        print(f\"Voxels with p&lt;0.01 (uncorrected): {sig_01}\")\n\n    return r_values, t_statistics, p_values, valid_mask\n</code></pre>"},{"location":"reference/stats/#tit.stats.generate_correlation_summary","title":"generate_correlation_summary","text":"<pre><code>generate_correlation_summary(subject_data, effect_sizes, r_values, sig_mask, cluster_threshold, clusters, atlas_results, output_file, params=None, effect_metric='Effect Size', subject_ids=None, weights=None)\n</code></pre> <p>Generate comprehensive summary report for correlation analysis</p> Parameters: <p>subject_data : ndarray (x, y, z, n_subjects)     Electric field magnitude data effect_sizes : ndarray (n_subjects,)     Continuous outcome measures r_values : ndarray (x, y, z)     Correlation map sig_mask : ndarray (x, y, z)     Binary mask of significant voxels cluster_threshold : float     Cluster statistic threshold from permutation clusters : list     List of cluster dictionaries atlas_results : dict     Atlas overlap results output_file : str     Path to output summary file params : dict, optional     Analysis parameters effect_metric : str     Name of the outcome measure subject_ids : list, optional     List of subject IDs weights : ndarray, optional     Subject weights (if used)</p> Source code in <code>tit/stats/reporting.py</code> <pre><code>def generate_correlation_summary(subject_data, effect_sizes, r_values, sig_mask, \n                                 cluster_threshold, clusters, atlas_results, \n                                 output_file, params=None, effect_metric=\"Effect Size\",\n                                 subject_ids=None, weights=None):\n    \"\"\"\n    Generate comprehensive summary report for correlation analysis\n\n    Parameters:\n    -----------\n    subject_data : ndarray (x, y, z, n_subjects)\n        Electric field magnitude data\n    effect_sizes : ndarray (n_subjects,)\n        Continuous outcome measures\n    r_values : ndarray (x, y, z)\n        Correlation map\n    sig_mask : ndarray (x, y, z)\n        Binary mask of significant voxels\n    cluster_threshold : float\n        Cluster statistic threshold from permutation\n    clusters : list\n        List of cluster dictionaries\n    atlas_results : dict\n        Atlas overlap results\n    output_file : str\n        Path to output summary file\n    params : dict, optional\n        Analysis parameters\n    effect_metric : str\n        Name of the outcome measure\n    subject_ids : list, optional\n        List of subject IDs\n    weights : ndarray, optional\n        Subject weights (if used)\n    \"\"\"\n    if params is None:\n        params = {\n            'correlation_type': 'pearson',\n            'cluster_threshold': 0.05,\n            'cluster_stat': 'mass',\n            'n_permutations': 1000,\n            'alpha': 0.05,\n            'use_weights': False\n        }\n\n    n_subjects = len(effect_sizes)\n\n    with open(output_file, 'w') as f:\n        f.write(\"=\"*70 + \"\\n\")\n        f.write(\"CORRELATION-BASED CLUSTER PERMUTATION ANALYSIS SUMMARY\\n\")\n        f.write(\"(ACES-style analysis for continuous outcomes)\\n\")\n        f.write(\"=\"*70 + \"\\n\\n\")\n\n        f.write(\"ANALYSIS DETAILS:\\n\")\n        f.write(\"-\" * 70 + \"\\n\")\n        corr_type = params.get('correlation_type', 'pearson').capitalize()\n        weighted_str = \" (Weighted)\" if params.get('use_weights', False) else \"\"\n        f.write(f\"Correlation Type: {corr_type}{weighted_str}\\n\")\n        f.write(f\"Outcome Measure: {effect_metric}\\n\")\n\n        cluster_stat = params.get('cluster_stat', 'mass')\n        cluster_stat_name = \"Cluster Size\" if cluster_stat == 'size' else \"Cluster Mass\"\n        f.write(f\"Cluster Statistic: {cluster_stat_name}\\n\")\n        f.write(f\"Cluster-forming threshold: p &lt; {params.get('cluster_threshold', 0.05)}\\n\")\n        f.write(f\"Number of permutations: {params.get('n_permutations', 1000)}\\n\")\n        f.write(f\"Cluster-level alpha: {params.get('alpha', 0.05)}\\n\")\n\n        if cluster_stat == 'size':\n            f.write(f\"Cluster size threshold: {cluster_threshold:.1f} voxels\\n\")\n        else:\n            f.write(f\"Cluster mass threshold: {cluster_threshold:.2f}\\n\")\n        f.write(\"\\n\")\n\n        f.write(\"SAMPLE INFORMATION:\\n\")\n        f.write(\"-\" * 70 + \"\\n\")\n        f.write(f\"Number of Subjects: {n_subjects}\\n\")\n        if subject_ids:\n            f.write(f\"Subject IDs: {', '.join(str(s) for s in subject_ids)}\\n\")\n        f.write(\"\\n\")\n\n        f.write(\"EFFECT SIZE DISTRIBUTION:\\n\")\n        f.write(\"-\" * 70 + \"\\n\")\n        f.write(f\"Mean: {np.mean(effect_sizes):.4f}\\n\")\n        f.write(f\"Std:  {np.std(effect_sizes):.4f}\\n\")\n        f.write(f\"Min:  {np.min(effect_sizes):.4f}\\n\")\n        f.write(f\"Max:  {np.max(effect_sizes):.4f}\\n\")\n        f.write(f\"Range: {np.max(effect_sizes) - np.min(effect_sizes):.4f}\\n\")\n\n        if subject_ids:\n            f.write(\"\\nPer-Subject Effect Sizes:\\n\")\n            for sid, es in zip(subject_ids, effect_sizes):\n                weight_str = \"\"\n                if weights is not None:\n                    idx = list(subject_ids).index(sid)\n                    weight_str = f\", weight={weights[idx]:.2f}\"\n                f.write(f\"  {sid}: {es:.4f}{weight_str}\\n\")\n        f.write(\"\\n\")\n\n        if weights is not None:\n            f.write(\"WEIGHT DISTRIBUTION:\\n\")\n            f.write(\"-\" * 70 + \"\\n\")\n            f.write(f\"Mean weight: {np.mean(weights):.4f}\\n\")\n            f.write(f\"Min weight:  {np.min(weights):.4f}\\n\")\n            f.write(f\"Max weight:  {np.max(weights):.4f}\\n\\n\")\n\n        f.write(\"RESULTS:\\n\")\n        f.write(\"-\" * 70 + \"\\n\")\n        n_sig = np.sum(sig_mask)\n        f.write(f\"Number of Significant Voxels: {n_sig}\\n\")\n        f.write(f\"Number of Significant Clusters: {len(clusters)}\\n\\n\")\n\n        if n_sig &gt; 0:\n            sig_bool = sig_mask.astype(bool)\n            mean_r = np.mean(r_values[sig_bool])\n            max_r = np.max(r_values[sig_bool])\n            min_r = np.min(r_values[sig_bool])\n\n            f.write(\"CORRELATION STATISTICS IN SIGNIFICANT VOXELS:\\n\")\n            f.write(\"-\" * 70 + \"\\n\")\n            f.write(f\"Mean r: {mean_r:.4f}\\n\")\n            f.write(f\"Peak r: {max_r:.4f}\\n\")\n            f.write(f\"Min r:  {min_r:.4f}\\n\\n\")\n\n            # E-field statistics in significant voxels\n            mean_efield = np.mean(subject_data[sig_bool, :])\n            max_efield = np.max(subject_data[sig_bool, :])\n            f.write(\"E-FIELD STATISTICS IN SIGNIFICANT VOXELS:\\n\")\n            f.write(\"-\" * 70 + \"\\n\")\n            f.write(f\"Mean E-field: {mean_efield:.6f}\\n\")\n            f.write(f\"Max E-field:  {max_efield:.6f}\\n\\n\")\n\n        if clusters:\n            f.write(\"SIGNIFICANT CLUSTERS:\\n\")\n            f.write(\"-\" * 70 + \"\\n\")\n            for i, c in enumerate(clusters[:20], 1):\n                f.write(f\"\\n{i}. Cluster {c['cluster_id']}: {c['size']} voxels\\n\")\n                f.write(f\"   MNI Center: ({c['center_mni'][0]:.1f}, \"\n                       f\"{c['center_mni'][1]:.1f}, {c['center_mni'][2]:.1f})\\n\")\n                if 'mean_r' in c:\n                    f.write(f\"   Mean r: {c['mean_r']:.4f}\\n\")\n                if 'peak_r' in c:\n                    f.write(f\"   Peak r: {c['peak_r']:.4f}\\n\")\n\n        f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n        f.write(\"ATLAS OVERLAP ANALYSIS\\n\")\n        f.write(\"=\"*70 + \"\\n\")\n\n        for atlas_name, region_counts in atlas_results.items():\n            f.write(f\"\\n{atlas_name}\\n\")\n            f.write(\"-\" * 70 + \"\\n\")\n\n            if region_counts:\n                f.write(f\"Number of regions with significant voxels: {len(region_counts)}\\n\\n\")\n                f.write(\"Top 20 regions:\\n\")\n                for i, r in enumerate(region_counts[:20], 1):\n                    pct = 100 * r['overlap_voxels'] / r['region_size']\n                    f.write(f\"{i:2d}. Region {r['region_id']:3d}: \"\n                           f\"{r['overlap_voxels']:4d} voxels ({pct:5.1f}% of region)\\n\")\n            else:\n                f.write(\"No overlapping regions found.\\n\")\n\n        f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n        f.write(\"INTERPRETATION NOTES\\n\")\n        f.write(\"=\"*70 + \"\\n\\n\")\n        f.write(\"This analysis identifies brain regions where electric field magnitude\\n\")\n        f.write(f\"correlates with {effect_metric}.\\n\\n\")\n        f.write(\"Positive correlations (r &gt; 0):\\n\")\n        f.write(f\"  Higher E-field \u2192 Higher {effect_metric}\\n\\n\")\n        f.write(\"Note: This analysis tests positive correlations only (as per ACES).\\n\")\n        f.write(\"To find regions with negative correlations, invert your effect sizes\\n\")\n        f.write(\"(multiply by -1) and re-run the analysis.\\n\\n\")\n\n        f.write(\"References:\\n\")\n        f.write(\"  - Wischnewski et al. (2021) - ACES approach\\n\")\n        f.write(\"  - Maris &amp; Oostenveld (2007) - Cluster-based permutation\\n\")\n\n    print(f\"\\nSummary written to: {output_file}\")\n</code></pre>"},{"location":"reference/stats/#tit.stats.generate_summary","title":"generate_summary","text":"<pre><code>generate_summary(responders, non_responders, sig_mask, correction_threshold, clusters, atlas_results, output_file, correction_method='cluster', params=None, group1_name='Responders', group2_name='Non-Responders', value_metric='Current Intensity', test_type='unpaired', observed_cluster_sizes=None)\n</code></pre> <p>Generate comprehensive summary report</p> Parameters: <p>responders : ndarray     Responder data (group 1) non_responders : ndarray     Non-responder data (group 2) sig_mask : ndarray     Binary mask of significant voxels correction_threshold : float     Threshold used for multiple comparison correction clusters : list     List of cluster dictionaries atlas_results : dict     Atlas overlap results output_file : str     Path to output summary file correction_method : str     Method used: 'cluster' or 'fdr' params : dict, optional     Dictionary of analysis parameters (cluster_threshold, n_permutations, alpha, etc.)     If None, uses defaults group1_name : str     Name for first group (default: \"Responders\") group2_name : str     Name for second group (default: \"Non-Responders\") value_metric : str     Name of the metric being compared (default: \"Current Intensity\") test_type : str     Type of t-test used: 'paired' or 'unpaired' (default: \"unpaired\") observed_cluster_sizes : list, optional     List of observed cluster sizes (before permutation correction) sorted largest to smallest</p> Source code in <code>tit/stats/reporting.py</code> <pre><code>def generate_summary(responders, non_responders, sig_mask, correction_threshold, \n                    clusters, atlas_results, output_file, \n                    correction_method=\"cluster\",\n                    params=None,\n                    group1_name=\"Responders\",\n                    group2_name=\"Non-Responders\",\n                    value_metric=\"Current Intensity\",\n                    test_type=\"unpaired\",\n                    observed_cluster_sizes=None):\n    \"\"\"\n    Generate comprehensive summary report\n\n    Parameters:\n    -----------\n    responders : ndarray\n        Responder data (group 1)\n    non_responders : ndarray\n        Non-responder data (group 2)\n    sig_mask : ndarray\n        Binary mask of significant voxels\n    correction_threshold : float\n        Threshold used for multiple comparison correction\n    clusters : list\n        List of cluster dictionaries\n    atlas_results : dict\n        Atlas overlap results\n    output_file : str\n        Path to output summary file\n    correction_method : str\n        Method used: 'cluster' or 'fdr'\n    params : dict, optional\n        Dictionary of analysis parameters (cluster_threshold, n_permutations, alpha, etc.)\n        If None, uses defaults\n    group1_name : str\n        Name for first group (default: \"Responders\")\n    group2_name : str\n        Name for second group (default: \"Non-Responders\")\n    value_metric : str\n        Name of the metric being compared (default: \"Current Intensity\")\n    test_type : str\n        Type of t-test used: 'paired' or 'unpaired' (default: \"unpaired\")\n    observed_cluster_sizes : list, optional\n        List of observed cluster sizes (before permutation correction) sorted largest to smallest\n    \"\"\"\n    # Set default parameters if not provided\n    if params is None:\n        params = {\n            'cluster_threshold': 0.01,\n            'n_permutations': 500,\n            'alpha': 0.05\n        }\n\n    # Extract parameters with defaults\n    cluster_threshold_param = params.get('cluster_threshold', 0.01)\n    n_permutations = params.get('n_permutations', 500)\n    alpha = params.get('alpha', 0.05)\n    cluster_stat = params.get('cluster_stat', 'size')\n\n    with open(output_file, 'w') as f:\n        f.write(\"=\"*70 + \"\\n\")\n        f.write(\"VOXELWISE STATISTICAL ANALYSIS SUMMARY\\n\")\n        f.write(\"=\"*70 + \"\\n\\n\")\n\n        f.write(\"ANALYSIS DETAILS:\\n\")\n        f.write(\"-\" * 70 + \"\\n\")\n        test_name = \"Paired t-test\" if test_type == \"paired\" else \"Unpaired (Independent Samples) t-test\"\n        f.write(f\"Statistical Test: {test_name}\\n\")\n\n        if correction_method == \"cluster\":\n            f.write(f\"Multiple Comparison Correction: Cluster-based Permutation\\n\")\n            cluster_stat_name = \"Cluster Size\" if cluster_stat == 'size' else \"Cluster Mass\"\n            f.write(f\"Cluster statistic: {cluster_stat_name}\\n\")\n            f.write(f\"Cluster-forming threshold: p &lt; {cluster_threshold_param} (uncorrected)\\n\")\n            f.write(f\"Number of permutations: {n_permutations}\\n\")\n            f.write(f\"Cluster-level alpha: {alpha}\\n\")\n\n            if cluster_stat == 'size':\n                f.write(f\"Cluster size threshold: {correction_threshold:.1f} voxels\\n\")\n            else:\n                f.write(f\"Cluster mass threshold: {correction_threshold:.2f}\\n\")\n            if 'n_jobs' in params:\n                n_jobs = params['n_jobs']\n                if n_jobs == -1:\n                    import multiprocessing\n                    n_jobs_actual = multiprocessing.cpu_count()\n                    f.write(f\"Parallel processing: {n_jobs_actual} cores\\n\")\n                elif n_jobs == 1:\n                    f.write(f\"Parallel processing: Sequential (1 core)\\n\")\n                else:\n                    f.write(f\"Parallel processing: {n_jobs} cores\\n\")\n            f.write(\"\\n\")\n        else:\n            f.write(f\"Multiple Comparison Correction: FDR (False Discovery Rate)\\n\")\n            f.write(f\"Significance Level: alpha = {alpha}\\n\")\n            f.write(f\"FDR-corrected p-value threshold: {correction_threshold:.6f}\\n\\n\")\n\n        # Add observed clusters information if provided\n        if observed_cluster_sizes is not None and len(observed_cluster_sizes) &gt; 0:\n            f.write(\"OBSERVED CLUSTERS (BEFORE PERMUTATION CORRECTION):\\n\")\n            f.write(\"-\" * 70 + \"\\n\")\n            f.write(f\"Total clusters at p &lt; {cluster_threshold_param}: {len(observed_cluster_sizes)}\\n\")\n            f.write(f\"Largest observed cluster: {observed_cluster_sizes[0]} voxels\\n\")\n            f.write(f\"Total voxels in all clusters: {sum(observed_cluster_sizes)}\\n\\n\")\n\n            f.write(\"Top 10 Largest Observed Clusters:\\n\")\n            for i, size in enumerate(observed_cluster_sizes[:10], 1):\n                f.write(f\"  {i:2d}. {size:6d} voxels\\n\")\n            f.write(\"\\n\")\n\n        f.write(\"SAMPLE INFORMATION:\\n\")\n        f.write(\"-\" * 70 + \"\\n\")\n        f.write(f\"Number of {group1_name}: {responders.shape[-1]}\\n\")\n        f.write(f\"Number of {group2_name}: {non_responders.shape[-1]}\\n\")\n        f.write(f\"Total Subjects: {responders.shape[-1] + non_responders.shape[-1]}\\n\\n\")\n\n        f.write(\"RESULTS:\\n\")\n        f.write(\"-\" * 70 + \"\\n\")\n        n_sig = np.sum(sig_mask)\n        f.write(f\"Number of Significant Voxels: {n_sig}\\n\")\n\n        if n_sig &gt; 0:\n            # Calculate mean values in significant voxels\n            sig_bool = sig_mask.astype(bool)\n            group1_mean = np.mean(responders[sig_bool, :])\n            group2_mean = np.mean(non_responders[sig_bool, :])\n\n            f.write(f\"\\nMean {value_metric} in Significant Voxels:\\n\")\n            f.write(f\"  {group1_name}: {group1_mean:.4f}\\n\")\n            f.write(f\"  {group2_name}: {group2_mean:.4f}\\n\")\n            f.write(f\"  Difference ({group1_name} - {group2_name}): {group1_mean - group2_mean:.4f}\\n\")\n\n        f.write(f\"\\nNumber of Clusters: {len(clusters)}\\n\\n\")\n\n        if clusters:\n            sort_label = \"by statistic\" if cluster_stat == 'mass' else \"by size\"\n            f.write(f\"TOP 10 CLUSTERS ({sort_label}):\\n\")\n            f.write(\"-\" * 70 + \"\\n\")\n            for i, c in enumerate(clusters[:10], 1):\n                f.write(f\"{i}. Cluster {c['cluster_id']}: {c['size']} voxels\")\n                if cluster_stat == 'mass' and 'stat_value' in c:\n                    f.write(f\", mass = {c['stat_value']:.2f}\")\n                f.write(\"\\n\")\n                f.write(f\"   MNI Center: ({c['center_mni'][0]:.1f}, \"\n                       f\"{c['center_mni'][1]:.1f}, {c['center_mni'][2]:.1f})\\n\")\n\n        f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n        f.write(\"ATLAS OVERLAP ANALYSIS\\n\")\n        f.write(\"=\"*70 + \"\\n\\n\")\n\n        for atlas_name, region_counts in atlas_results.items():\n            f.write(f\"\\n{atlas_name}\\n\")\n            f.write(\"-\" * 70 + \"\\n\")\n\n            if region_counts:\n                f.write(f\"Number of regions with significant voxels: {len(region_counts)}\\n\\n\")\n                f.write(\"Top 20 regions:\\n\")\n                for i, r in enumerate(region_counts[:20], 1):\n                    pct = 100 * r['overlap_voxels'] / r['region_size']\n                    f.write(f\"{i:2d}. Region {r['region_id']:3d}: \"\n                           f\"{r['overlap_voxels']:4d} voxels ({pct:5.1f}% of region)\\n\")\n            else:\n                f.write(\"No overlapping regions found.\\n\")\n\n    print(f\"\\nSummary written to: {output_file}\")\n</code></pre>"},{"location":"reference/stats/#tit.stats.get_path_manager","title":"get_path_manager","text":"<pre><code>get_path_manager() -&gt; PathManager\n</code></pre> <p>Get the global PathManager singleton instance.</p> <p>Returns:     The global path manager instance</p> Source code in <code>tit/core/paths.py</code> <pre><code>def get_path_manager() -&gt; PathManager:\n    \"\"\"\n    Get the global PathManager singleton instance.\n\n    Returns:\n        The global path manager instance\n    \"\"\"\n    global _path_manager_instance\n    if _path_manager_instance is None:\n        _path_manager_instance = PathManager()\n    return _path_manager_instance\n</code></pre>"},{"location":"reference/stats/#tit.stats.load_subject_data","title":"load_subject_data","text":"<pre><code>load_subject_data(subject_configs, nifti_file_pattern=None, analysis_type='group_comparison')\n</code></pre> <p>Unified data loading for both group comparison and correlation analysis</p> Parameters: <p>subject_configs : list of dict     Subject configurations (format depends on analysis_type) nifti_file_pattern : str, optional     Pattern for NIfTI files analysis_type : str     Either 'group_comparison' or 'correlation'</p> Returns: <p>For group_comparison:     responders, non_responders, template_img, resp_ids, non_resp_ids For correlation:     subject_data, effect_sizes, weights, template_img, subject_ids</p> Source code in <code>tit/stats/permutation_analysis.py</code> <pre><code>def load_subject_data(subject_configs, nifti_file_pattern=None, analysis_type='group_comparison'):\n    \"\"\"\n    Unified data loading for both group comparison and correlation analysis\n\n    Parameters:\n    -----------\n    subject_configs : list of dict\n        Subject configurations (format depends on analysis_type)\n    nifti_file_pattern : str, optional\n        Pattern for NIfTI files\n    analysis_type : str\n        Either 'group_comparison' or 'correlation'\n\n    Returns:\n    --------\n    For group_comparison:\n        responders, non_responders, template_img, resp_ids, non_resp_ids\n    For correlation:\n        subject_data, effect_sizes, weights, template_img, subject_ids\n    \"\"\"\n    if analysis_type == 'group_comparison':\n        return load_subject_data_group_comparison(subject_configs, nifti_file_pattern)\n    elif analysis_type == 'correlation':\n        return load_subject_data_correlation(subject_configs, nifti_file_pattern)\n    else:\n        raise ValueError(f\"Unknown analysis_type: {analysis_type}\")\n</code></pre>"},{"location":"reference/stats/#tit.stats.load_subject_data_correlation","title":"load_subject_data_correlation","text":"<pre><code>load_subject_data_correlation(subject_configs, nifti_file_pattern=None)\n</code></pre> <p>Load subject data for correlation analysis (continuous outcomes)</p> Source code in <code>tit/stats/permutation_analysis.py</code> <pre><code>def load_subject_data_correlation(subject_configs, nifti_file_pattern=None):\n    \"\"\"\n    Load subject data for correlation analysis (continuous outcomes)\n    \"\"\"\n    if nifti_file_pattern is None:\n        nifti_file_pattern = \"grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz\"\n\n    # Check for required fields\n    required_fields = ['subject_id', 'simulation_name', 'effect_size']\n    for config in subject_configs:\n        for field in required_fields:\n            if field not in config:\n                raise ValueError(f\"Missing required field '{field}' in subject config\")\n\n    # Load all subjects\n    subject_data, template_img, subject_ids = nifti.load_group_data_ti_toolbox(\n        subject_configs,\n        nifti_file_pattern=nifti_file_pattern,\n        dtype=np.float32\n    )\n\n    # Build a lookup from subject_id to config for successfully loaded subjects\n    config_lookup = {c['subject_id']: c for c in subject_configs}\n\n    # Extract effect sizes and weights only for successfully loaded subjects\n    effect_sizes = []\n    weights_list = []\n    has_weights = 'weight' in subject_configs[0]\n\n    for sid in subject_ids:\n        config = config_lookup[sid]\n        effect_sizes.append(config['effect_size'])\n        if has_weights:\n            weights_list.append(config.get('weight', 1.0))\n\n    effect_sizes = np.array(effect_sizes, dtype=np.float64)\n    weights = np.array(weights_list, dtype=np.float64) if has_weights else None\n\n    print(f\"\\nLoaded {len(subject_ids)} subjects: {subject_ids}\")\n    print(f\"Effect sizes: mean={np.mean(effect_sizes):.3f}, std={np.std(effect_sizes):.3f}\")\n    print(f\"Effect size range: [{np.min(effect_sizes):.3f}, {np.max(effect_sizes):.3f}]\")\n    if weights is not None:\n        print(f\"Weights: mean={np.mean(weights):.3f}, range=[{np.min(weights):.3f}, {np.max(weights):.3f}]\")\n    print(f\"Data shape: {subject_data.shape}\")\n\n    return subject_data, effect_sizes, weights, template_img, subject_ids\n</code></pre>"},{"location":"reference/stats/#tit.stats.load_subject_data_group_comparison","title":"load_subject_data_group_comparison","text":"<pre><code>load_subject_data_group_comparison(subject_configs, nifti_file_pattern=None)\n</code></pre> <p>Load subject data for group comparison analysis (binary outcomes)</p> Source code in <code>tit/stats/permutation_analysis.py</code> <pre><code>def load_subject_data_group_comparison(subject_configs, nifti_file_pattern=None):\n    \"\"\"\n    Load subject data for group comparison analysis (binary outcomes)\n    \"\"\"\n    if nifti_file_pattern is None:\n        nifti_file_pattern = \"grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz\"\n\n    # Separate configs by response\n    responder_configs = [c for c in subject_configs if c['response'] == 1]\n    non_responder_configs = [c for c in subject_configs if c['response'] == 0]\n\n    if len(responder_configs) == 0 or len(non_responder_configs) == 0:\n        raise ValueError(\"Need at least one responder and one non-responder\")\n\n    # Load responders\n    responders, template_img, responder_ids = nifti.load_group_data_ti_toolbox(\n        responder_configs,\n        nifti_file_pattern=nifti_file_pattern,\n        dtype=np.float32\n    )\n\n    # Load non-responders\n    non_responders, _, non_responder_ids = nifti.load_group_data_ti_toolbox(\n        non_responder_configs,\n        nifti_file_pattern=nifti_file_pattern,\n        dtype=np.float32\n    )\n\n    print(f\"\\nLoaded {len(responder_ids)} responders: {responder_ids}\")\n    print(f\"Loaded {len(non_responder_ids)} non-responders: {non_responder_ids}\")\n    print(f\"Responders shape: {responders.shape}\")\n    print(f\"Non-responders shape: {non_responders.shape}\")\n\n    return responders, non_responders, template_img, responder_ids, non_responder_ids\n</code></pre>"},{"location":"reference/stats/#tit.stats.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Command-line interface for unified permutation analysis</p> Source code in <code>tit/stats/permutation_analysis.py</code> <pre><code>def main():\n    \"\"\"\n    Command-line interface for unified permutation analysis\n    \"\"\"\n    import argparse\n    import multiprocessing\n\n    # Ensure proper multiprocessing initialization\n    if hasattr(multiprocessing, 'set_start_method'):\n        try:\n            multiprocessing.set_start_method('fork', force=True)\n        except RuntimeError:\n            pass\n\n    parser = argparse.ArgumentParser(\n        description='Unified Cluster-Based Permutation Testing for TI-Toolbox',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  # Group comparison analysis\n  python permutation_analysis.py --csv subjects.csv --name my_analysis \\\\\n      --analysis-type group_comparison\n\n  # Correlation analysis\n  python permutation_analysis.py --csv subjects.csv --name my_analysis \\\\\n      --analysis-type correlation\n\nGroup Comparison CSV Format:\n  subject_id,simulation_name,response\n  070,ICP_RHIPPO,1\n  071,ICP_RHIPPO,0\n\nCorrelation CSV Format:\n  subject_id,simulation_name,effect_size,weight\n  070,ICP_RHIPPO,0.45,25\n  071,ICP_RHIPPO,0.32,30\n\nTissue Types:\n  --tissue-type controls which NIfTI files are loaded:\n    grey  : grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz\n    white : white_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz\n    all   : {simulation_name}_TI_MNI_MNI_TI_max.nii.gz (no prefix)\n        \"\"\"\n    )\n\n    # Required arguments\n    parser.add_argument('--csv', '-c', required=True,\n                        help='Path to CSV file with subject configurations')\n    parser.add_argument('--name', '-n', required=True,\n                        help='Analysis name (used for output directory)')\n    parser.add_argument('--analysis-type', required=True,\n                        choices=['group_comparison', 'correlation'],\n                        help='Type of analysis to perform')\n\n    # Statistical parameters (common)\n    parser.add_argument('--cluster-threshold', '-t', type=float, default=0.05,\n                        help='P-value threshold for cluster formation (default: 0.05)')\n    parser.add_argument('--cluster-stat', choices=['mass', 'size'], default='mass',\n                        help='Cluster statistic: mass (sum of t-values) or size (voxel count) (default: mass)')\n    parser.add_argument('--n-permutations', '-p', type=int, default=1000,\n                        help='Number of permutations (default: 1000)')\n    parser.add_argument('--alpha', '-a', type=float, default=0.05,\n                        help='Cluster-level significance threshold (default: 0.05)')\n    parser.add_argument('--n-jobs', '-j', type=int, default=-1,\n                        help='Number of parallel jobs: -1=all cores, 1=sequential (default: -1)')\n\n    # Group comparison specific parameters\n    parser.add_argument('--test-type', choices=['unpaired', 'paired'], default='unpaired',\n                        help='Statistical test type for group comparison (default: unpaired)')\n    parser.add_argument('--alternative', choices=['two-sided', 'greater', 'less'],\n                        default='two-sided',\n                        help='Alternative hypothesis for group comparison (default: two-sided)')\n\n    # Correlation specific parameters\n    parser.add_argument('--correlation-type', choices=['pearson', 'spearman'],\n                        default='pearson',\n                        help='Type of correlation for correlation analysis (default: pearson)')\n    parser.add_argument('--use-weights', action='store_true',\n                        help='Use weights from CSV if available (correlation analysis)')\n\n    # Optional parameters\n    parser.add_argument('--tissue-type', choices=['grey', 'white', 'all'],\n                        default='grey',\n                        help='Tissue type for NIfTI files: grey (grey matter), '\n                             'white (white matter), or all (all tissues, no prefix). '\n                             '(default: grey)')\n    parser.add_argument('--nifti-pattern', default=None,\n                        help='Custom NIfTI filename pattern (overrides --tissue-type)')\n    parser.add_argument('--quiet', '-q', action='store_true',\n                        help='Suppress progress output')\n\n    args = parser.parse_args()\n\n    # Build configuration\n    config = {\n        'analysis_type': args.analysis_type,\n        'cluster_threshold': args.cluster_threshold,\n        'cluster_stat': args.cluster_stat,\n        'n_permutations': args.n_permutations,\n        'alpha': args.alpha,\n        'n_jobs': args.n_jobs,\n        'tissue_type': args.tissue_type,\n        'nifti_file_pattern': args.nifti_pattern,  # None triggers auto-generation\n    }\n\n    # Add analysis-specific parameters\n    if args.analysis_type == 'group_comparison':\n        config.update({\n            'test_type': args.test_type,\n            'alternative': args.alternative,\n        })\n    else:  # correlation\n        config.update({\n            'correlation_type': args.correlation_type,\n            'use_weights': args.use_weights,\n        })\n\n    # Print header\n    if not args.quiet:\n        print(\"=\"*70)\n        print(\"UNIFIED CLUSTER-BASED PERMUTATION TESTING - TI-TOOLBOX\")\n        print(\"=\"*70)\n        print(f\"Analysis type: {args.analysis_type}\")\n        print(f\"CSV file: {args.csv}\")\n        print(f\"Analysis name: {args.name}\")\n        print(f\"Tissue type: {args.tissue_type}\")\n        print(f\"Permutations: {args.n_permutations}\")\n        print(f\"Parallel jobs: {args.n_jobs if args.n_jobs != -1 else 'all cores'}\")\n        print(f\"Cluster statistic: {args.cluster_stat}\")\n        print(f\"Cluster threshold: p &lt; {args.cluster_threshold}\")\n        print(\"=\"*70)\n        print()\n\n    # Run analysis\n    try:\n        results = run_analysis(\n            subject_configs=args.csv,\n            analysis_name=args.name,\n            config=config,\n            output_callback=None if args.quiet else print\n        )\n\n        # Print summary\n        if not args.quiet:\n            print()\n            print(\"=\"*70)\n            print(\"ANALYSIS COMPLETE!\")\n            print(\"=\"*70)\n            print(f\"Output directory: {results['output_dir']}\")\n            if 'n_significant_clusters' in results:\n                print(f\"Significant clusters: {results['n_significant_clusters']}\")\n            if 'n_significant_voxels' in results:\n                print(f\"Significant voxels: {results['n_significant_voxels']}\")\n            print(f\"Analysis time: {results['analysis_time']:.1f} seconds\")\n            print(\"=\"*70)\n\n        return 0\n\n    except Exception as e:\n        print(f\"\\nERROR: {str(e)}\", file=sys.stderr)\n        if not args.quiet:\n            import traceback\n            traceback.print_exc()\n        return 1\n</code></pre>"},{"location":"reference/stats/#tit.stats.plot_cluster_size_mass_correlation","title":"plot_cluster_size_mass_correlation","text":"<pre><code>plot_cluster_size_mass_correlation(cluster_sizes: np.ndarray, cluster_masses: np.ndarray, output_file: str, *, dpi: int = 300) -&gt; str | None\n</code></pre> <p>Plot correlation between cluster size and cluster mass from permutation null distribution.</p> Source code in <code>tit/plotting/stats.py</code> <pre><code>def plot_cluster_size_mass_correlation(\n    cluster_sizes: np.ndarray,\n    cluster_masses: np.ndarray,\n    output_file: str,\n    *,\n    dpi: int = 300,\n) -&gt; str | None:\n    \"\"\"\n    Plot correlation between cluster size and cluster mass from permutation null distribution.\n    \"\"\"\n    from scipy.stats import pearsonr\n\n    ensure_headless_matplotlib_backend()\n    import matplotlib.pyplot as plt\n\n    import seaborn as sns\n\n    sns.set_style(\"whitegrid\")\n    sns.set_context(\"notebook\", font_scale=1.0)\n\n    # Remove zeros\n    mask = (cluster_sizes &gt; 0) &amp; (cluster_masses &gt; 0)\n    sizes_nonzero = cluster_sizes[mask]\n    masses_nonzero = cluster_masses[mask]\n    if len(sizes_nonzero) &lt; 2:\n        return None\n\n    r_value, p_value = pearsonr(sizes_nonzero, masses_nonzero)\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n\n    if sns is not None:\n        sns.regplot(\n            x=sizes_nonzero,\n            y=masses_nonzero,\n            ax=ax,\n            scatter_kws={\"alpha\": 0.6, \"s\": 50, \"color\": \"steelblue\", \"edgecolors\": \"black\", \"linewidths\": 0.5},\n            line_kws={\"color\": \"red\", \"linewidth\": 2},\n        )\n    else:\n        ax.scatter(sizes_nonzero, masses_nonzero, alpha=0.6, s=50, c=\"steelblue\", edgecolors=\"black\", linewidths=0.5)\n        z = np.polyfit(sizes_nonzero, masses_nonzero, 1)\n        xs = np.linspace(float(np.min(sizes_nonzero)), float(np.max(sizes_nonzero)), 100)\n        ax.plot(xs, z[0] * xs + z[1], color=\"red\", linewidth=2)\n\n    z = np.polyfit(sizes_nonzero, masses_nonzero, 1)\n    ax.set_xlabel(\"Maximum Cluster Size (voxels)\", fontsize=12, fontweight=\"bold\")\n    ax.set_ylabel(\"Maximum Cluster Mass (sum of t-statistics)\", fontsize=12, fontweight=\"bold\")\n    ax.set_title(\n        f\"Cluster Size vs Cluster Mass Correlation\\nPearson r = {r_value:.3f} (p = {p_value:.2e})\",\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n\n    textstr = (\n        f\"n = {len(sizes_nonzero)} permutations\\n\"\n        f\"r = {r_value:.3f}\\n\"\n        f\"p = {p_value:.2e}\\n\"\n        f\"Linear fit: y = {z[0]:.2f}x + {z[1]:.2f}\"\n    )\n    props = dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8)\n    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11, verticalalignment=\"top\", bbox=props)\n\n    ax.grid(True, alpha=0.3)\n    fig.tight_layout()\n\n    return savefig_close(fig, output_file, fmt=\"pdf\", opts=SaveFigOptions(dpi=dpi))\n</code></pre>"},{"location":"reference/stats/#tit.stats.plot_permutation_null_distribution","title":"plot_permutation_null_distribution","text":"<pre><code>plot_permutation_null_distribution(null_distribution: np.ndarray, threshold: float, observed_clusters: Sequence[Mapping[str, float]], output_file: str, *, alpha: float = 0.05, cluster_stat: str = 'size', dpi: int = 300) -&gt; str\n</code></pre> <p>Plot permutation null distribution with threshold and observed clusters.</p> Source code in <code>tit/plotting/stats.py</code> <pre><code>def plot_permutation_null_distribution(\n    null_distribution: np.ndarray,\n    threshold: float,\n    observed_clusters: Sequence[Mapping[str, float]],\n    output_file: str,\n    *,\n    alpha: float = 0.05,\n    cluster_stat: str = \"size\",\n    dpi: int = 300,\n) -&gt; str:\n    \"\"\"\n    Plot permutation null distribution with threshold and observed clusters.\n    \"\"\"\n    ensure_headless_matplotlib_backend()\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    sns.set_style(\"whitegrid\")\n    sns.set_context(\"notebook\", font_scale=1.0)\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Labels based on cluster statistic\n    if cluster_stat == \"size\":\n        x_label = \"Maximum Cluster Size (voxels)\"\n        title = \"Permutation Null Distribution of Maximum Cluster Sizes\"\n        threshold_label = f\"Discrete Threshold (p&lt;{alpha}): {threshold:.1f} voxels\"\n    else:\n        x_label = \"Maximum Cluster Mass (sum of t-statistics)\"\n        title = \"Permutation Null Distribution of Maximum Cluster Mass\"\n        threshold_label = f\"Discrete Threshold (p&lt;{alpha}): {threshold:.2f}\"\n\n    # Histogram\n    if sns is not None:\n        sns.histplot(\n            null_distribution,\n            bins=200,\n            alpha=0.7,\n            color=\"gray\",\n            edgecolor=\"black\",\n            label=\"Null Distribution\",\n            ax=ax,\n        )\n    else:\n        ax.hist(null_distribution, bins=200, alpha=0.7, color=\"gray\", edgecolor=\"black\", label=\"Null Distribution\")\n\n    # Threshold line\n    ax.axvline(threshold, color=\"red\", linestyle=\"--\", linewidth=2, label=threshold_label)\n\n    # Observed clusters\n    sig_plotted = False\n    nonsig_plotted = False\n    for cluster in observed_clusters:\n        stat_value = float(cluster[\"stat_value\"])\n        p_value = cluster.get(\"p_value\", None)\n        if p_value is not None:\n            is_significant = float(p_value) &lt; 0.05\n        else:\n            is_significant = stat_value &gt; threshold\n\n        color = \"green\" if is_significant else \"orange\"\n        label = None\n        if is_significant and not sig_plotted:\n            label = \"Significant Clusters (p&lt;0.05)\"\n            sig_plotted = True\n        elif (not is_significant) and (not nonsig_plotted):\n            label = \"Non-significant Clusters (p\u22650.05)\"\n            nonsig_plotted = True\n\n        ax.axvline(stat_value, color=color, linestyle=\"-\", linewidth=2, alpha=0.7, label=label)\n\n    ax.set_xlabel(x_label, fontsize=12)\n    ax.set_ylabel(\"Frequency\", fontsize=12)\n    ax.set_title(title, fontsize=14, fontweight=\"bold\")\n    ax.legend(loc=\"upper right\", fontsize=10)\n    ax.grid(True, alpha=0.3)\n    fig.tight_layout()\n\n    return savefig_close(fig, output_file, fmt=\"pdf\", opts=SaveFigOptions(dpi=dpi))\n</code></pre>"},{"location":"reference/stats/#tit.stats.prepare_config_from_csv","title":"prepare_config_from_csv","text":"<pre><code>prepare_config_from_csv(csv_file, analysis_type='group_comparison')\n</code></pre> <p>Load subject configurations from CSV file</p> Parameters: <p>csv_file : str     Path to CSV file analysis_type : str     Either 'group_comparison' or 'correlation'</p> Returns: <p>list of dict : Subject configurations</p> Source code in <code>tit/stats/permutation_analysis.py</code> <pre><code>def prepare_config_from_csv(csv_file, analysis_type='group_comparison'):\n    \"\"\"\n    Load subject configurations from CSV file\n\n    Parameters:\n    -----------\n    csv_file : str\n        Path to CSV file\n    analysis_type : str\n        Either 'group_comparison' or 'correlation'\n\n    Returns:\n    --------\n    list of dict : Subject configurations\n    \"\"\"\n    df = pd.read_csv(csv_file)\n\n    if analysis_type == 'group_comparison':\n        # Validate required columns for group comparison\n        required_cols = ['subject_id', 'simulation_name', 'response']\n        for col in required_cols:\n            if col not in df.columns:\n                raise ValueError(f\"CSV file missing required column: '{col}' for group comparison\")\n\n        configs = []\n        for _, row in df.iterrows():\n            # Handle both 'sub-XXX' and 'XXX' formats\n            subject_id = str(row['subject_id']).replace('sub-', '')\n\n            configs.append({\n                'subject_id': subject_id,\n                'simulation_name': row['simulation_name'],\n                'response': int(row['response'])\n            })\n\n    elif analysis_type == 'correlation':\n        # Validate required columns for correlation\n        required_cols = ['subject_id', 'simulation_name', 'effect_size']\n        for col in required_cols:\n            if col not in df.columns:\n                raise ValueError(f\"CSV file missing required column: '{col}' for correlation analysis\")\n\n        configs = []\n        skipped_rows = 0\n        for _, row in df.iterrows():\n            # Skip rows with missing required fields\n            if pd.isna(row['subject_id']) or pd.isna(row['simulation_name']) or pd.isna(row['effect_size']):\n                skipped_rows += 1\n                continue\n\n            # Handle both 'sub-XXX' and 'XXX' formats\n            # Also handle float-to-string conversion (e.g., 101.0 -&gt; \"101\")\n            raw_id = row['subject_id']\n            if isinstance(raw_id, float):\n                # Convert float to int then to string (101.0 -&gt; \"101\")\n                subject_id = str(int(raw_id))\n            else:\n                subject_id = str(raw_id).replace('sub-', '')\n                # Also handle string representations of floats like \"101.0\"\n                if subject_id.endswith('.0'):\n                    subject_id = subject_id[:-2]\n\n            config = {\n                'subject_id': subject_id,\n                'simulation_name': str(row['simulation_name']),\n                'effect_size': float(row['effect_size'])\n            }\n\n            # Add weight if present\n            if 'weight' in df.columns and pd.notna(row.get('weight')):\n                config['weight'] = float(row['weight'])\n\n            configs.append(config)\n\n        if skipped_rows &gt; 0:\n            print(f\"Note: Skipped {skipped_rows} rows with missing required fields\")\n\n        if len(configs) == 0:\n            raise ValueError(\"No valid subject configurations found in CSV file\")\n\n    else:\n        raise ValueError(f\"Unknown analysis_type: {analysis_type}\")\n\n    return configs\n</code></pre>"},{"location":"reference/stats/#tit.stats.run_analysis","title":"run_analysis","text":"<pre><code>run_analysis(subject_configs, analysis_name, config=None, output_callback=None, callback_handler=None, progress_callback=None, stop_callback=None)\n</code></pre> <p>Run unified cluster-based permutation analysis</p> Parameters: <p>subject_configs : list of dict or str     Either a list of subject configurations or path to CSV file analysis_name : str     Name for this analysis (used for output directory) config : dict, optional     Configuration dictionary (merged with defaults based on analysis_type) output_callback : callable, optional     Callback function for status updates (for GUI integration) callback_handler : logging.Handler, optional     Callback handler for GUI console integration progress_callback : callable, optional     Callback function for progress updates stop_callback : callable, optional     Callback function to check if analysis should be stopped</p> Returns: <p>dict : Results dictionary (structure depends on analysis_type)</p> Source code in <code>tit/stats/permutation_analysis.py</code> <pre><code>def run_analysis(subject_configs, analysis_name, config=None, output_callback=None,\n                 callback_handler=None, progress_callback=None, stop_callback=None):\n    \"\"\"\n    Run unified cluster-based permutation analysis\n\n    Parameters:\n    -----------\n    subject_configs : list of dict or str\n        Either a list of subject configurations or path to CSV file\n    analysis_name : str\n        Name for this analysis (used for output directory)\n    config : dict, optional\n        Configuration dictionary (merged with defaults based on analysis_type)\n    output_callback : callable, optional\n        Callback function for status updates (for GUI integration)\n    callback_handler : logging.Handler, optional\n        Callback handler for GUI console integration\n    progress_callback : callable, optional\n        Callback function for progress updates\n    stop_callback : callable, optional\n        Callback function to check if analysis should be stopped\n\n    Returns:\n    --------\n    dict : Results dictionary (structure depends on analysis_type)\n    \"\"\"\n    # Determine analysis type from config or default\n    analysis_type = 'group_comparison'  # default\n    if config and 'analysis_type' in config:\n        analysis_type = config['analysis_type']\n\n    # Merge config with appropriate defaults\n    if analysis_type == 'group_comparison':\n        CONFIG = DEFAULT_CONFIG_GROUP_COMPARISON.copy()\n    elif analysis_type == 'correlation':\n        CONFIG = DEFAULT_CONFIG_CORRELATION.copy()\n    else:\n        raise ValueError(f\"Unknown analysis_type: {analysis_type}\")\n\n    if config:\n        CONFIG.update(config)\n\n    # Ensure analysis_type is set in CONFIG\n    CONFIG['analysis_type'] = analysis_type\n\n    # Callback helper\n    def log_callback(msg):\n        if output_callback:\n            output_callback(msg)\n\n    # Start timing\n    analysis_start_time = time.time()\n\n    # Set up output directory\n    pm = get_path_manager() if get_path_manager else None\n    if pm:\n        project_dir = pm.project_dir\n        derivatives_dir = pm.get_derivatives_dir()\n        output_dir = os.path.join(\n            derivatives_dir,\n            const.DIR_TI_TOOLBOX,\n            'stats',\n            analysis_type,\n            analysis_name\n        )\n    else:\n        project_dir = os.environ.get('PROJECT_DIR', '/mnt')\n        output_dir = os.path.join(\n            project_dir,\n            'derivatives',\n            'tit',\n            'stats',\n            analysis_type,\n            analysis_name\n        )\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Set up logging\n    if callback_handler:\n        logger, log_file = setup_logging(output_dir, analysis_type, callback_handler)\n    else:\n        logger, log_file = setup_logging(output_dir, analysis_type)\n\n    # Log header\n    analysis_title = \"CLUSTER-BASED PERMUTATION TESTING\" if analysis_type == 'group_comparison' else \"CORRELATION-BASED CLUSTER PERMUTATION TESTING\"\n    subtitle = \"\" if analysis_type == 'group_comparison' else \"(ACES-style analysis for continuous outcomes)\"\n\n    logger.info(\"=\"*70)\n    logger.info(analysis_title)\n    if subtitle:\n        logger.info(subtitle)\n    logger.info(\"=\"*70)\n    logger.info(f\"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    logger.info(f\"Analysis name: {analysis_name}\")\n    logger.info(f\"Analysis type: {analysis_type}\")\n    logger.info(f\"Output directory: {output_dir}\")\n    logger.info(f\"Log file: {log_file}\")\n    logger.info(\"\")\n\n    log_callback(f\"Starting {analysis_type} analysis: {analysis_name}\")\n\n    # Construct nifti_file_pattern from tissue_type\n    if CONFIG.get('nifti_file_pattern') is None:\n        tissue_type = CONFIG.get('tissue_type', 'grey')\n        if tissue_type == 'grey':\n            CONFIG['nifti_file_pattern'] = 'grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz'\n        elif tissue_type == 'white':\n            CONFIG['nifti_file_pattern'] = 'white_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz'\n        elif tissue_type == 'all':\n            CONFIG['nifti_file_pattern'] = '{simulation_name}_TI_MNI_MNI_TI_max.nii.gz'\n        else:\n            raise ValueError(f\"Invalid tissue_type: '{tissue_type}'. Must be 'grey', 'white', or 'all'\")\n\n    # Log configuration\n    logger.info(\"CONFIGURATION:\")\n    if analysis_type == 'group_comparison':\n        logger.info(f\"  Statistical test: {CONFIG['test_type'].capitalize()} t-test\")\n        alt_text = {\n            'two-sided': 'two-sided (\u2260)',\n            'greater': f\"one-sided ({CONFIG['group1_name']} &gt; {CONFIG['group2_name']})\",\n            'less': f\"one-sided ({CONFIG['group1_name']} &lt; {CONFIG['group2_name']})\"\n        }\n        logger.info(f\"  Alternative hypothesis: {alt_text.get(CONFIG['alternative'], CONFIG['alternative'])}\")\n    else:\n        logger.info(f\"  Correlation type: {CONFIG['correlation_type'].capitalize()}\")\n\n    cluster_stat_name = \"Cluster Size\" if CONFIG['cluster_stat'] == 'size' else \"Cluster Mass\"\n    logger.info(f\"  Cluster statistic: {cluster_stat_name}\")\n    logger.info(f\"  Cluster threshold: {CONFIG['cluster_threshold']}\")\n    logger.info(f\"  Number of permutations: {CONFIG['n_permutations']}\")\n    logger.info(f\"  Alpha level: {CONFIG['alpha']}\")\n    logger.info(f\"  Parallel jobs: {CONFIG['n_jobs']}\")\n    logger.info(f\"  Tissue type: {CONFIG.get('tissue_type', 'grey')}\")\n    logger.info(f\"  NIfTI pattern: {CONFIG['nifti_file_pattern']}\")\n    logger.info(\"\")\n\n    # Load subject configurations\n    if isinstance(subject_configs, str):\n        logger.info(f\"Loading subject configurations from: {subject_configs}\")\n        subject_configs = prepare_config_from_csv(subject_configs, analysis_type)\n\n    # Branch based on analysis type\n    if analysis_type == 'group_comparison':\n        return _run_group_comparison_analysis(\n            subject_configs, CONFIG, output_dir, logger, log_callback,\n            analysis_start_time, log_file, stop_callback\n        )\n    else:  # correlation\n        return _run_correlation_analysis(\n            subject_configs, CONFIG, output_dir, logger, log_callback,\n            analysis_start_time, log_file, stop_callback\n        )\n</code></pre>"},{"location":"reference/stats/#tit.stats.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(output_dir, analysis_type='group_comparison', callback_handler=None)\n</code></pre> <p>Set up logging for unified analysis</p> Parameters: <p>output_dir : str     Directory where log file will be saved analysis_type : str     Type of analysis for log naming callback_handler : logging.Handler, optional     Callback handler for GUI integration</p> Returns: <p>logger : logging.Logger     Configured logger instance log_file : str     Path to log file</p> Source code in <code>tit/stats/permutation_analysis.py</code> <pre><code>def setup_logging(output_dir, analysis_type='group_comparison', callback_handler=None):\n    \"\"\"\n    Set up logging for unified analysis\n\n    Parameters:\n    -----------\n    output_dir : str\n        Directory where log file will be saved\n    analysis_type : str\n        Type of analysis for log naming\n    callback_handler : logging.Handler, optional\n        Callback handler for GUI integration\n\n    Returns:\n    --------\n    logger : logging.Logger\n        Configured logger instance\n    log_file : str\n        Path to log file\n    \"\"\"\n    if logging_util is None:\n        raise ImportError(\"logging_util module not available\")\n\n    # Create timestamp for log file\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = os.path.join(output_dir, f\"{analysis_type}_analysis_{timestamp}.log\")\n\n    # Create logger\n    logger_name = f\"{'GroupComparison' if analysis_type == 'group_comparison' else 'Correlation'}Analysis\"\n    logger = logging_util.get_logger(logger_name, log_file, overwrite=True)\n\n    # If callback handler provided (for GUI), suppress console output and add callback\n    if callback_handler:\n        logging_util.suppress_console_output(logger)\n        logger.addHandler(callback_handler)\n\n    # Configure external loggers\n    external_loggers = ['scipy', 'numpy', 'nibabel', 'pandas', 'matplotlib']\n    logging_util.configure_external_loggers(external_loggers, logger)\n\n    return logger, log_file\n</code></pre>"},{"location":"reference/stats/#tit.stats.ttest_voxelwise","title":"ttest_voxelwise","text":"<pre><code>ttest_voxelwise(responders, non_responders, test_type='unpaired', alternative='two-sided', verbose=True)\n</code></pre> <p>Perform vectorized t-test (paired or unpaired) at each voxel.</p> <p>Uses vectorized computation for optimal performance.</p> Parameters: <p>responders : ndarray (x, y, z, n_subjects)     Responder data (group 1) non_responders : ndarray (x, y, z, n_subjects)     Non-responder data (group 2) test_type : str     Either 'paired' or 'unpaired' t-test alternative : {'two-sided', 'greater', 'less'}, optional     Defines the alternative hypothesis (default: 'two-sided'):     * 'two-sided': means are different (responders \u2260 non-responders)     * 'greater': responders have higher values (responders &gt; non-responders)     * 'less': responders have lower values (responders &lt; non-responders) verbose : bool     Print progress information</p> Returns: <p>p_values : ndarray (x, y, z)     P-value at each voxel t_statistics : ndarray (x, y, z)     T-statistic at each voxel valid_mask : ndarray (x, y, z)     Boolean mask of valid voxels</p> Source code in <code>tit/stats/stats_utils.py</code> <pre><code>def ttest_voxelwise(responders, non_responders, test_type='unpaired', alternative='two-sided', verbose=True):\n    \"\"\"\n    Perform vectorized t-test (paired or unpaired) at each voxel.\n\n    Uses vectorized computation for optimal performance.\n\n    Parameters:\n    -----------\n    responders : ndarray (x, y, z, n_subjects)\n        Responder data (group 1)\n    non_responders : ndarray (x, y, z, n_subjects)\n        Non-responder data (group 2)\n    test_type : str\n        Either 'paired' or 'unpaired' t-test\n    alternative : {'two-sided', 'greater', 'less'}, optional\n        Defines the alternative hypothesis (default: 'two-sided'):\n        * 'two-sided': means are different (responders \u2260 non-responders)\n        * 'greater': responders have higher values (responders &gt; non-responders)\n        * 'less': responders have lower values (responders &lt; non-responders)\n    verbose : bool\n        Print progress information\n\n    Returns:\n    --------\n    p_values : ndarray (x, y, z)\n        P-value at each voxel\n    t_statistics : ndarray (x, y, z)\n        T-statistic at each voxel\n    valid_mask : ndarray (x, y, z)\n        Boolean mask of valid voxels\n    \"\"\"\n    if verbose:\n        test_name = \"Paired\" if test_type == 'paired' else \"Unpaired (Independent Samples)\"\n        alt_text = \"\"\n        if alternative == 'greater':\n            alt_text = \" (one-sided: responders &gt; non-responders)\"\n        elif alternative == 'less':\n            alt_text = \" (one-sided: responders &lt; non-responders)\"\n        print(f\"\\nPerforming voxelwise {test_name} t-tests{alt_text} (vectorized)...\")\n\n    # Validate test type\n    if test_type not in ['paired', 'unpaired']:\n        raise ValueError(\"test_type must be 'paired' or 'unpaired'\")\n\n    # For paired test, check that sample sizes match\n    if test_type == 'paired':\n        if responders.shape[-1] != non_responders.shape[-1]:\n            raise ValueError(f\"Paired t-test requires equal sample sizes. \"\n                           f\"Got {responders.shape[-1]} vs {non_responders.shape[-1]} subjects\")\n\n    shape = responders.shape[:3]\n    p_values = np.ones(shape)\n    t_statistics = np.zeros(shape)\n\n    # Create mask of valid voxels (non-zero in at least some subjects)\n    responder_mask = np.any(responders &gt; 0, axis=-1)\n    non_responder_mask = np.any(non_responders &gt; 0, axis=-1)\n    valid_mask = responder_mask | non_responder_mask\n\n    total_voxels = np.sum(valid_mask)\n    if verbose:\n        print(f\"Testing {total_voxels} valid voxels using vectorized approach...\")\n\n    # Get coordinates of valid voxels\n    valid_coords = np.argwhere(valid_mask)\n\n    # Vectorized approach: compute all tests at once\n    n_valid = len(valid_coords)\n    n_resp = responders.shape[-1]\n    n_non_resp = non_responders.shape[-1]\n\n    # Pre-extract voxel data using advanced indexing (faster than loop)\n    # This avoids Python loop overhead by using NumPy's optimized indexing\n    idx_i, idx_j, idx_k = valid_coords[:, 0], valid_coords[:, 1], valid_coords[:, 2]\n\n    # Extract all valid voxels at once using fancy indexing\n    resp_extracted = responders[idx_i, idx_j, idx_k, :].astype(np.float32)  # (n_valid, n_resp)\n    non_resp_extracted = non_responders[idx_i, idx_j, idx_k, :].astype(np.float32)  # (n_valid, n_non_resp)\n\n    # Concatenate horizontally\n    voxel_data = np.concatenate([resp_extracted, non_resp_extracted], axis=1)\n\n    # Use vectorized t-test functions\n    if test_type == 'paired':\n        t_stats_1d, p_values_1d = ttest_rel(\n            voxel_data, n_resp, alternative=alternative\n        )\n    else:\n        t_stats_1d, p_values_1d = ttest_ind(\n            voxel_data, n_resp, n_non_resp, alternative=alternative\n        )\n\n    # Map results back to 3D volume coordinates using advanced indexing (faster than loop)\n    t_statistics[idx_i, idx_j, idx_k] = t_stats_1d\n    p_values[idx_i, idx_j, idx_k] = p_values_1d\n\n    return p_values, t_statistics, valid_mask\n</code></pre>"},{"location":"reference/stats/#permutation-analysis-titstatspermutation_analysis","title":"Permutation analysis (<code>tit.stats.permutation_analysis</code>)","text":"<p>Unified Cluster-Based Permutation Testing for TI-Toolbox</p> <p>This script provides unified cluster-based permutation testing for both: 1. Group comparison analysis (binary responder/non-responder classification) 2. Correlation analysis (continuous outcome measures)</p> <p>Supports both t-test and correlation-based statistical approaches with cluster-based permutation correction for multiple comparisons.</p> <p>Usage:     from tit.stats import permutation_analysis     # Group comparison     results = permutation_analysis.run_analysis(         subject_configs, analysis_name, analysis_type='group_comparison'     )     # Correlation analysis     results = permutation_analysis.run_analysis(         subject_configs, analysis_name, analysis_type='correlation'     )</p> <p>For GUI usage, see gui/extensions/permutation_analysis.py</p>"},{"location":"reference/stats/#tit.stats.permutation_analysis.load_subject_data","title":"load_subject_data","text":"<pre><code>load_subject_data(subject_configs, nifti_file_pattern=None, analysis_type='group_comparison')\n</code></pre> <p>Unified data loading for both group comparison and correlation analysis</p> Parameters: <p>subject_configs : list of dict     Subject configurations (format depends on analysis_type) nifti_file_pattern : str, optional     Pattern for NIfTI files analysis_type : str     Either 'group_comparison' or 'correlation'</p> Returns: <p>For group_comparison:     responders, non_responders, template_img, resp_ids, non_resp_ids For correlation:     subject_data, effect_sizes, weights, template_img, subject_ids</p> Source code in <code>tit/stats/permutation_analysis.py</code> <pre><code>def load_subject_data(subject_configs, nifti_file_pattern=None, analysis_type='group_comparison'):\n    \"\"\"\n    Unified data loading for both group comparison and correlation analysis\n\n    Parameters:\n    -----------\n    subject_configs : list of dict\n        Subject configurations (format depends on analysis_type)\n    nifti_file_pattern : str, optional\n        Pattern for NIfTI files\n    analysis_type : str\n        Either 'group_comparison' or 'correlation'\n\n    Returns:\n    --------\n    For group_comparison:\n        responders, non_responders, template_img, resp_ids, non_resp_ids\n    For correlation:\n        subject_data, effect_sizes, weights, template_img, subject_ids\n    \"\"\"\n    if analysis_type == 'group_comparison':\n        return load_subject_data_group_comparison(subject_configs, nifti_file_pattern)\n    elif analysis_type == 'correlation':\n        return load_subject_data_correlation(subject_configs, nifti_file_pattern)\n    else:\n        raise ValueError(f\"Unknown analysis_type: {analysis_type}\")\n</code></pre>"},{"location":"reference/stats/#tit.stats.permutation_analysis.load_subject_data_correlation","title":"load_subject_data_correlation","text":"<pre><code>load_subject_data_correlation(subject_configs, nifti_file_pattern=None)\n</code></pre> <p>Load subject data for correlation analysis (continuous outcomes)</p> Source code in <code>tit/stats/permutation_analysis.py</code> <pre><code>def load_subject_data_correlation(subject_configs, nifti_file_pattern=None):\n    \"\"\"\n    Load subject data for correlation analysis (continuous outcomes)\n    \"\"\"\n    if nifti_file_pattern is None:\n        nifti_file_pattern = \"grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz\"\n\n    # Check for required fields\n    required_fields = ['subject_id', 'simulation_name', 'effect_size']\n    for config in subject_configs:\n        for field in required_fields:\n            if field not in config:\n                raise ValueError(f\"Missing required field '{field}' in subject config\")\n\n    # Load all subjects\n    subject_data, template_img, subject_ids = nifti.load_group_data_ti_toolbox(\n        subject_configs,\n        nifti_file_pattern=nifti_file_pattern,\n        dtype=np.float32\n    )\n\n    # Build a lookup from subject_id to config for successfully loaded subjects\n    config_lookup = {c['subject_id']: c for c in subject_configs}\n\n    # Extract effect sizes and weights only for successfully loaded subjects\n    effect_sizes = []\n    weights_list = []\n    has_weights = 'weight' in subject_configs[0]\n\n    for sid in subject_ids:\n        config = config_lookup[sid]\n        effect_sizes.append(config['effect_size'])\n        if has_weights:\n            weights_list.append(config.get('weight', 1.0))\n\n    effect_sizes = np.array(effect_sizes, dtype=np.float64)\n    weights = np.array(weights_list, dtype=np.float64) if has_weights else None\n\n    print(f\"\\nLoaded {len(subject_ids)} subjects: {subject_ids}\")\n    print(f\"Effect sizes: mean={np.mean(effect_sizes):.3f}, std={np.std(effect_sizes):.3f}\")\n    print(f\"Effect size range: [{np.min(effect_sizes):.3f}, {np.max(effect_sizes):.3f}]\")\n    if weights is not None:\n        print(f\"Weights: mean={np.mean(weights):.3f}, range=[{np.min(weights):.3f}, {np.max(weights):.3f}]\")\n    print(f\"Data shape: {subject_data.shape}\")\n\n    return subject_data, effect_sizes, weights, template_img, subject_ids\n</code></pre>"},{"location":"reference/stats/#tit.stats.permutation_analysis.load_subject_data_group_comparison","title":"load_subject_data_group_comparison","text":"<pre><code>load_subject_data_group_comparison(subject_configs, nifti_file_pattern=None)\n</code></pre> <p>Load subject data for group comparison analysis (binary outcomes)</p> Source code in <code>tit/stats/permutation_analysis.py</code> <pre><code>def load_subject_data_group_comparison(subject_configs, nifti_file_pattern=None):\n    \"\"\"\n    Load subject data for group comparison analysis (binary outcomes)\n    \"\"\"\n    if nifti_file_pattern is None:\n        nifti_file_pattern = \"grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz\"\n\n    # Separate configs by response\n    responder_configs = [c for c in subject_configs if c['response'] == 1]\n    non_responder_configs = [c for c in subject_configs if c['response'] == 0]\n\n    if len(responder_configs) == 0 or len(non_responder_configs) == 0:\n        raise ValueError(\"Need at least one responder and one non-responder\")\n\n    # Load responders\n    responders, template_img, responder_ids = nifti.load_group_data_ti_toolbox(\n        responder_configs,\n        nifti_file_pattern=nifti_file_pattern,\n        dtype=np.float32\n    )\n\n    # Load non-responders\n    non_responders, _, non_responder_ids = nifti.load_group_data_ti_toolbox(\n        non_responder_configs,\n        nifti_file_pattern=nifti_file_pattern,\n        dtype=np.float32\n    )\n\n    print(f\"\\nLoaded {len(responder_ids)} responders: {responder_ids}\")\n    print(f\"Loaded {len(non_responder_ids)} non-responders: {non_responder_ids}\")\n    print(f\"Responders shape: {responders.shape}\")\n    print(f\"Non-responders shape: {non_responders.shape}\")\n\n    return responders, non_responders, template_img, responder_ids, non_responder_ids\n</code></pre>"},{"location":"reference/stats/#tit.stats.permutation_analysis.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Command-line interface for unified permutation analysis</p> Source code in <code>tit/stats/permutation_analysis.py</code> <pre><code>def main():\n    \"\"\"\n    Command-line interface for unified permutation analysis\n    \"\"\"\n    import argparse\n    import multiprocessing\n\n    # Ensure proper multiprocessing initialization\n    if hasattr(multiprocessing, 'set_start_method'):\n        try:\n            multiprocessing.set_start_method('fork', force=True)\n        except RuntimeError:\n            pass\n\n    parser = argparse.ArgumentParser(\n        description='Unified Cluster-Based Permutation Testing for TI-Toolbox',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  # Group comparison analysis\n  python permutation_analysis.py --csv subjects.csv --name my_analysis \\\\\n      --analysis-type group_comparison\n\n  # Correlation analysis\n  python permutation_analysis.py --csv subjects.csv --name my_analysis \\\\\n      --analysis-type correlation\n\nGroup Comparison CSV Format:\n  subject_id,simulation_name,response\n  070,ICP_RHIPPO,1\n  071,ICP_RHIPPO,0\n\nCorrelation CSV Format:\n  subject_id,simulation_name,effect_size,weight\n  070,ICP_RHIPPO,0.45,25\n  071,ICP_RHIPPO,0.32,30\n\nTissue Types:\n  --tissue-type controls which NIfTI files are loaded:\n    grey  : grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz\n    white : white_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz\n    all   : {simulation_name}_TI_MNI_MNI_TI_max.nii.gz (no prefix)\n        \"\"\"\n    )\n\n    # Required arguments\n    parser.add_argument('--csv', '-c', required=True,\n                        help='Path to CSV file with subject configurations')\n    parser.add_argument('--name', '-n', required=True,\n                        help='Analysis name (used for output directory)')\n    parser.add_argument('--analysis-type', required=True,\n                        choices=['group_comparison', 'correlation'],\n                        help='Type of analysis to perform')\n\n    # Statistical parameters (common)\n    parser.add_argument('--cluster-threshold', '-t', type=float, default=0.05,\n                        help='P-value threshold for cluster formation (default: 0.05)')\n    parser.add_argument('--cluster-stat', choices=['mass', 'size'], default='mass',\n                        help='Cluster statistic: mass (sum of t-values) or size (voxel count) (default: mass)')\n    parser.add_argument('--n-permutations', '-p', type=int, default=1000,\n                        help='Number of permutations (default: 1000)')\n    parser.add_argument('--alpha', '-a', type=float, default=0.05,\n                        help='Cluster-level significance threshold (default: 0.05)')\n    parser.add_argument('--n-jobs', '-j', type=int, default=-1,\n                        help='Number of parallel jobs: -1=all cores, 1=sequential (default: -1)')\n\n    # Group comparison specific parameters\n    parser.add_argument('--test-type', choices=['unpaired', 'paired'], default='unpaired',\n                        help='Statistical test type for group comparison (default: unpaired)')\n    parser.add_argument('--alternative', choices=['two-sided', 'greater', 'less'],\n                        default='two-sided',\n                        help='Alternative hypothesis for group comparison (default: two-sided)')\n\n    # Correlation specific parameters\n    parser.add_argument('--correlation-type', choices=['pearson', 'spearman'],\n                        default='pearson',\n                        help='Type of correlation for correlation analysis (default: pearson)')\n    parser.add_argument('--use-weights', action='store_true',\n                        help='Use weights from CSV if available (correlation analysis)')\n\n    # Optional parameters\n    parser.add_argument('--tissue-type', choices=['grey', 'white', 'all'],\n                        default='grey',\n                        help='Tissue type for NIfTI files: grey (grey matter), '\n                             'white (white matter), or all (all tissues, no prefix). '\n                             '(default: grey)')\n    parser.add_argument('--nifti-pattern', default=None,\n                        help='Custom NIfTI filename pattern (overrides --tissue-type)')\n    parser.add_argument('--quiet', '-q', action='store_true',\n                        help='Suppress progress output')\n\n    args = parser.parse_args()\n\n    # Build configuration\n    config = {\n        'analysis_type': args.analysis_type,\n        'cluster_threshold': args.cluster_threshold,\n        'cluster_stat': args.cluster_stat,\n        'n_permutations': args.n_permutations,\n        'alpha': args.alpha,\n        'n_jobs': args.n_jobs,\n        'tissue_type': args.tissue_type,\n        'nifti_file_pattern': args.nifti_pattern,  # None triggers auto-generation\n    }\n\n    # Add analysis-specific parameters\n    if args.analysis_type == 'group_comparison':\n        config.update({\n            'test_type': args.test_type,\n            'alternative': args.alternative,\n        })\n    else:  # correlation\n        config.update({\n            'correlation_type': args.correlation_type,\n            'use_weights': args.use_weights,\n        })\n\n    # Print header\n    if not args.quiet:\n        print(\"=\"*70)\n        print(\"UNIFIED CLUSTER-BASED PERMUTATION TESTING - TI-TOOLBOX\")\n        print(\"=\"*70)\n        print(f\"Analysis type: {args.analysis_type}\")\n        print(f\"CSV file: {args.csv}\")\n        print(f\"Analysis name: {args.name}\")\n        print(f\"Tissue type: {args.tissue_type}\")\n        print(f\"Permutations: {args.n_permutations}\")\n        print(f\"Parallel jobs: {args.n_jobs if args.n_jobs != -1 else 'all cores'}\")\n        print(f\"Cluster statistic: {args.cluster_stat}\")\n        print(f\"Cluster threshold: p &lt; {args.cluster_threshold}\")\n        print(\"=\"*70)\n        print()\n\n    # Run analysis\n    try:\n        results = run_analysis(\n            subject_configs=args.csv,\n            analysis_name=args.name,\n            config=config,\n            output_callback=None if args.quiet else print\n        )\n\n        # Print summary\n        if not args.quiet:\n            print()\n            print(\"=\"*70)\n            print(\"ANALYSIS COMPLETE!\")\n            print(\"=\"*70)\n            print(f\"Output directory: {results['output_dir']}\")\n            if 'n_significant_clusters' in results:\n                print(f\"Significant clusters: {results['n_significant_clusters']}\")\n            if 'n_significant_voxels' in results:\n                print(f\"Significant voxels: {results['n_significant_voxels']}\")\n            print(f\"Analysis time: {results['analysis_time']:.1f} seconds\")\n            print(\"=\"*70)\n\n        return 0\n\n    except Exception as e:\n        print(f\"\\nERROR: {str(e)}\", file=sys.stderr)\n        if not args.quiet:\n            import traceback\n            traceback.print_exc()\n        return 1\n</code></pre>"},{"location":"reference/stats/#tit.stats.permutation_analysis.prepare_config_from_csv","title":"prepare_config_from_csv","text":"<pre><code>prepare_config_from_csv(csv_file, analysis_type='group_comparison')\n</code></pre> <p>Load subject configurations from CSV file</p> Parameters: <p>csv_file : str     Path to CSV file analysis_type : str     Either 'group_comparison' or 'correlation'</p> Returns: <p>list of dict : Subject configurations</p> Source code in <code>tit/stats/permutation_analysis.py</code> <pre><code>def prepare_config_from_csv(csv_file, analysis_type='group_comparison'):\n    \"\"\"\n    Load subject configurations from CSV file\n\n    Parameters:\n    -----------\n    csv_file : str\n        Path to CSV file\n    analysis_type : str\n        Either 'group_comparison' or 'correlation'\n\n    Returns:\n    --------\n    list of dict : Subject configurations\n    \"\"\"\n    df = pd.read_csv(csv_file)\n\n    if analysis_type == 'group_comparison':\n        # Validate required columns for group comparison\n        required_cols = ['subject_id', 'simulation_name', 'response']\n        for col in required_cols:\n            if col not in df.columns:\n                raise ValueError(f\"CSV file missing required column: '{col}' for group comparison\")\n\n        configs = []\n        for _, row in df.iterrows():\n            # Handle both 'sub-XXX' and 'XXX' formats\n            subject_id = str(row['subject_id']).replace('sub-', '')\n\n            configs.append({\n                'subject_id': subject_id,\n                'simulation_name': row['simulation_name'],\n                'response': int(row['response'])\n            })\n\n    elif analysis_type == 'correlation':\n        # Validate required columns for correlation\n        required_cols = ['subject_id', 'simulation_name', 'effect_size']\n        for col in required_cols:\n            if col not in df.columns:\n                raise ValueError(f\"CSV file missing required column: '{col}' for correlation analysis\")\n\n        configs = []\n        skipped_rows = 0\n        for _, row in df.iterrows():\n            # Skip rows with missing required fields\n            if pd.isna(row['subject_id']) or pd.isna(row['simulation_name']) or pd.isna(row['effect_size']):\n                skipped_rows += 1\n                continue\n\n            # Handle both 'sub-XXX' and 'XXX' formats\n            # Also handle float-to-string conversion (e.g., 101.0 -&gt; \"101\")\n            raw_id = row['subject_id']\n            if isinstance(raw_id, float):\n                # Convert float to int then to string (101.0 -&gt; \"101\")\n                subject_id = str(int(raw_id))\n            else:\n                subject_id = str(raw_id).replace('sub-', '')\n                # Also handle string representations of floats like \"101.0\"\n                if subject_id.endswith('.0'):\n                    subject_id = subject_id[:-2]\n\n            config = {\n                'subject_id': subject_id,\n                'simulation_name': str(row['simulation_name']),\n                'effect_size': float(row['effect_size'])\n            }\n\n            # Add weight if present\n            if 'weight' in df.columns and pd.notna(row.get('weight')):\n                config['weight'] = float(row['weight'])\n\n            configs.append(config)\n\n        if skipped_rows &gt; 0:\n            print(f\"Note: Skipped {skipped_rows} rows with missing required fields\")\n\n        if len(configs) == 0:\n            raise ValueError(\"No valid subject configurations found in CSV file\")\n\n    else:\n        raise ValueError(f\"Unknown analysis_type: {analysis_type}\")\n\n    return configs\n</code></pre>"},{"location":"reference/stats/#tit.stats.permutation_analysis.run_analysis","title":"run_analysis","text":"<pre><code>run_analysis(subject_configs, analysis_name, config=None, output_callback=None, callback_handler=None, progress_callback=None, stop_callback=None)\n</code></pre> <p>Run unified cluster-based permutation analysis</p> Parameters: <p>subject_configs : list of dict or str     Either a list of subject configurations or path to CSV file analysis_name : str     Name for this analysis (used for output directory) config : dict, optional     Configuration dictionary (merged with defaults based on analysis_type) output_callback : callable, optional     Callback function for status updates (for GUI integration) callback_handler : logging.Handler, optional     Callback handler for GUI console integration progress_callback : callable, optional     Callback function for progress updates stop_callback : callable, optional     Callback function to check if analysis should be stopped</p> Returns: <p>dict : Results dictionary (structure depends on analysis_type)</p> Source code in <code>tit/stats/permutation_analysis.py</code> <pre><code>def run_analysis(subject_configs, analysis_name, config=None, output_callback=None,\n                 callback_handler=None, progress_callback=None, stop_callback=None):\n    \"\"\"\n    Run unified cluster-based permutation analysis\n\n    Parameters:\n    -----------\n    subject_configs : list of dict or str\n        Either a list of subject configurations or path to CSV file\n    analysis_name : str\n        Name for this analysis (used for output directory)\n    config : dict, optional\n        Configuration dictionary (merged with defaults based on analysis_type)\n    output_callback : callable, optional\n        Callback function for status updates (for GUI integration)\n    callback_handler : logging.Handler, optional\n        Callback handler for GUI console integration\n    progress_callback : callable, optional\n        Callback function for progress updates\n    stop_callback : callable, optional\n        Callback function to check if analysis should be stopped\n\n    Returns:\n    --------\n    dict : Results dictionary (structure depends on analysis_type)\n    \"\"\"\n    # Determine analysis type from config or default\n    analysis_type = 'group_comparison'  # default\n    if config and 'analysis_type' in config:\n        analysis_type = config['analysis_type']\n\n    # Merge config with appropriate defaults\n    if analysis_type == 'group_comparison':\n        CONFIG = DEFAULT_CONFIG_GROUP_COMPARISON.copy()\n    elif analysis_type == 'correlation':\n        CONFIG = DEFAULT_CONFIG_CORRELATION.copy()\n    else:\n        raise ValueError(f\"Unknown analysis_type: {analysis_type}\")\n\n    if config:\n        CONFIG.update(config)\n\n    # Ensure analysis_type is set in CONFIG\n    CONFIG['analysis_type'] = analysis_type\n\n    # Callback helper\n    def log_callback(msg):\n        if output_callback:\n            output_callback(msg)\n\n    # Start timing\n    analysis_start_time = time.time()\n\n    # Set up output directory\n    pm = get_path_manager() if get_path_manager else None\n    if pm:\n        project_dir = pm.project_dir\n        derivatives_dir = pm.get_derivatives_dir()\n        output_dir = os.path.join(\n            derivatives_dir,\n            const.DIR_TI_TOOLBOX,\n            'stats',\n            analysis_type,\n            analysis_name\n        )\n    else:\n        project_dir = os.environ.get('PROJECT_DIR', '/mnt')\n        output_dir = os.path.join(\n            project_dir,\n            'derivatives',\n            'tit',\n            'stats',\n            analysis_type,\n            analysis_name\n        )\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Set up logging\n    if callback_handler:\n        logger, log_file = setup_logging(output_dir, analysis_type, callback_handler)\n    else:\n        logger, log_file = setup_logging(output_dir, analysis_type)\n\n    # Log header\n    analysis_title = \"CLUSTER-BASED PERMUTATION TESTING\" if analysis_type == 'group_comparison' else \"CORRELATION-BASED CLUSTER PERMUTATION TESTING\"\n    subtitle = \"\" if analysis_type == 'group_comparison' else \"(ACES-style analysis for continuous outcomes)\"\n\n    logger.info(\"=\"*70)\n    logger.info(analysis_title)\n    if subtitle:\n        logger.info(subtitle)\n    logger.info(\"=\"*70)\n    logger.info(f\"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    logger.info(f\"Analysis name: {analysis_name}\")\n    logger.info(f\"Analysis type: {analysis_type}\")\n    logger.info(f\"Output directory: {output_dir}\")\n    logger.info(f\"Log file: {log_file}\")\n    logger.info(\"\")\n\n    log_callback(f\"Starting {analysis_type} analysis: {analysis_name}\")\n\n    # Construct nifti_file_pattern from tissue_type\n    if CONFIG.get('nifti_file_pattern') is None:\n        tissue_type = CONFIG.get('tissue_type', 'grey')\n        if tissue_type == 'grey':\n            CONFIG['nifti_file_pattern'] = 'grey_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz'\n        elif tissue_type == 'white':\n            CONFIG['nifti_file_pattern'] = 'white_{simulation_name}_TI_MNI_MNI_TI_max.nii.gz'\n        elif tissue_type == 'all':\n            CONFIG['nifti_file_pattern'] = '{simulation_name}_TI_MNI_MNI_TI_max.nii.gz'\n        else:\n            raise ValueError(f\"Invalid tissue_type: '{tissue_type}'. Must be 'grey', 'white', or 'all'\")\n\n    # Log configuration\n    logger.info(\"CONFIGURATION:\")\n    if analysis_type == 'group_comparison':\n        logger.info(f\"  Statistical test: {CONFIG['test_type'].capitalize()} t-test\")\n        alt_text = {\n            'two-sided': 'two-sided (\u2260)',\n            'greater': f\"one-sided ({CONFIG['group1_name']} &gt; {CONFIG['group2_name']})\",\n            'less': f\"one-sided ({CONFIG['group1_name']} &lt; {CONFIG['group2_name']})\"\n        }\n        logger.info(f\"  Alternative hypothesis: {alt_text.get(CONFIG['alternative'], CONFIG['alternative'])}\")\n    else:\n        logger.info(f\"  Correlation type: {CONFIG['correlation_type'].capitalize()}\")\n\n    cluster_stat_name = \"Cluster Size\" if CONFIG['cluster_stat'] == 'size' else \"Cluster Mass\"\n    logger.info(f\"  Cluster statistic: {cluster_stat_name}\")\n    logger.info(f\"  Cluster threshold: {CONFIG['cluster_threshold']}\")\n    logger.info(f\"  Number of permutations: {CONFIG['n_permutations']}\")\n    logger.info(f\"  Alpha level: {CONFIG['alpha']}\")\n    logger.info(f\"  Parallel jobs: {CONFIG['n_jobs']}\")\n    logger.info(f\"  Tissue type: {CONFIG.get('tissue_type', 'grey')}\")\n    logger.info(f\"  NIfTI pattern: {CONFIG['nifti_file_pattern']}\")\n    logger.info(\"\")\n\n    # Load subject configurations\n    if isinstance(subject_configs, str):\n        logger.info(f\"Loading subject configurations from: {subject_configs}\")\n        subject_configs = prepare_config_from_csv(subject_configs, analysis_type)\n\n    # Branch based on analysis type\n    if analysis_type == 'group_comparison':\n        return _run_group_comparison_analysis(\n            subject_configs, CONFIG, output_dir, logger, log_callback,\n            analysis_start_time, log_file, stop_callback\n        )\n    else:  # correlation\n        return _run_correlation_analysis(\n            subject_configs, CONFIG, output_dir, logger, log_callback,\n            analysis_start_time, log_file, stop_callback\n        )\n</code></pre>"},{"location":"reference/stats/#tit.stats.permutation_analysis.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(output_dir, analysis_type='group_comparison', callback_handler=None)\n</code></pre> <p>Set up logging for unified analysis</p> Parameters: <p>output_dir : str     Directory where log file will be saved analysis_type : str     Type of analysis for log naming callback_handler : logging.Handler, optional     Callback handler for GUI integration</p> Returns: <p>logger : logging.Logger     Configured logger instance log_file : str     Path to log file</p> Source code in <code>tit/stats/permutation_analysis.py</code> <pre><code>def setup_logging(output_dir, analysis_type='group_comparison', callback_handler=None):\n    \"\"\"\n    Set up logging for unified analysis\n\n    Parameters:\n    -----------\n    output_dir : str\n        Directory where log file will be saved\n    analysis_type : str\n        Type of analysis for log naming\n    callback_handler : logging.Handler, optional\n        Callback handler for GUI integration\n\n    Returns:\n    --------\n    logger : logging.Logger\n        Configured logger instance\n    log_file : str\n        Path to log file\n    \"\"\"\n    if logging_util is None:\n        raise ImportError(\"logging_util module not available\")\n\n    # Create timestamp for log file\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = os.path.join(output_dir, f\"{analysis_type}_analysis_{timestamp}.log\")\n\n    # Create logger\n    logger_name = f\"{'GroupComparison' if analysis_type == 'group_comparison' else 'Correlation'}Analysis\"\n    logger = logging_util.get_logger(logger_name, log_file, overwrite=True)\n\n    # If callback handler provided (for GUI), suppress console output and add callback\n    if callback_handler:\n        logging_util.suppress_console_output(logger)\n        logger.addHandler(callback_handler)\n\n    # Configure external loggers\n    external_loggers = ['scipy', 'numpy', 'nibabel', 'pandas', 'matplotlib']\n    logging_util.configure_external_loggers(external_loggers, logger)\n\n    return logger, log_file\n</code></pre>"}]}